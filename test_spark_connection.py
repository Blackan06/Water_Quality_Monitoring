#!/usr/bin/env python3
"""
Test Spark connection and processing
"""

import sys
import os
import logging
from datetime import datetime, timezone

# Add the project root to Python path
sys.path.append('/Users/kiethuynhanh/Documents/Water_Quality_Monitoring')

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def test_spark_connection():
    """Test Spark connection"""
    try:
        logger.info("ğŸš€ Testing Spark connection...")
        
        # Import Spark consumer
        from include.iot_streaming.spark_consumer import SparkKafkaConsumer
        
        # Create consumer instance
        consumer = SparkKafkaConsumer()
        
        if consumer.spark is None:
            logger.error("âŒ Spark session is None")
            return False
            
        # Test basic Spark operations
        logger.info("ğŸ“Š Testing Spark operations...")
        
        # Create test DataFrame
        test_data = [
            {"station_id": 1, "ph": 7.2, "temperature": 25.5, "do": 8.2, "measurement_time": datetime.now(timezone.utc)},
            {"station_id": 2, "ph": 6.8, "temperature": 24.0, "do": 7.5, "measurement_time": datetime.now(timezone.utc)},
        ]
        
        df = consumer.spark.createDataFrame(test_data)
        logger.info(f"âœ… Created DataFrame with {df.count()} rows")
        
        # Test data cleaning
        cleaned_df = consumer._clean_data(df)
        logger.info(f"âœ… Data cleaning successful: {cleaned_df.count()} valid records")
        
        # Test feature engineering
        features_df = consumer._add_features(cleaned_df)
        logger.info(f"âœ… Feature engineering successful: {features_df.count()} records with features")
        
        # Test WQI calculation
        wqi_df = consumer._calculate_wqi_spark(features_df)
        logger.info(f"âœ… WQI calculation successful: {wqi_df.count()} records with WQI")
        
        # Show sample results
        logger.info("ğŸ“‹ Sample results:")
        wqi_df.select("station_id", "ph", "temperature", "do", "wqi", "quality_status").show(5)
        
        logger.info("âœ… All Spark tests passed!")
        return True
        
    except Exception as e:
        logger.error(f"âŒ Spark test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_kafka_message_processing():
    """Test processing a Kafka message"""
    try:
        logger.info("ğŸ“¨ Testing Kafka message processing...")
        
        from include.iot_streaming.spark_consumer import SparkKafkaConsumer
        
        consumer = SparkKafkaConsumer()
        
        # Create test message
        test_message = {
            "station_id": 1,
            "ph": 7.2,
            "temperature": 25.5,
            "do": 8.2,
            "measurement_time": datetime.now(timezone.utc).isoformat()
        }
        
        import json
        message_str = json.dumps(test_message)
        
        # Process message
        result = consumer.process_kafka_message(message_str)
        
        if result.get("success"):
            logger.info("âœ… Kafka message processing successful")
            logger.info(f"ğŸ“Š Processed data: {result.get('data', {})}")
            return True
        else:
            logger.error(f"âŒ Kafka message processing failed: {result.get('error')}")
            return False
            
    except Exception as e:
        logger.error(f"âŒ Kafka message test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    logger.info("ğŸ§ª Starting Spark connection tests...")
    
    # Test 1: Basic Spark connection
    spark_ok = test_spark_connection()
    
    if spark_ok:
        # Test 2: Kafka message processing
        kafka_ok = test_kafka_message_processing()
        
        if kafka_ok:
            logger.info("ğŸ‰ All tests passed! Spark is working correctly.")
        else:
            logger.error("âŒ Kafka message processing failed")
    else:
        logger.error("âŒ Spark connection failed")
    
    logger.info("ğŸ Test completed")
