import logging
import os
import pickle
import joblib
import numpy as np
import pandas as pd
import requests
import json
import warnings
from datetime import datetime
from typing import Dict, Any, Optional, Tuple
import xgboost as xgb
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error
from sklearn.model_selection import train_test_split, cross_val_score, KFold, TimeSeriesSplit
from sklearn.preprocessing import MinMaxScaler, StandardScaler
# Optional tensorflow imports - will be None if tensorflow is not installed
try:
    import tensorflow as tf
    from tensorflow.keras.models import Sequential, load_model
    from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization
    from tensorflow.keras.optimizers import Adam
    from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
    TENSORFLOW_AVAILABLE = True
except ImportError:
    tf = None
    Sequential = None
    load_model = None
    LSTM = None
    Dense = None
    Dropout = None
    BatchNormalization = None
    Adam = None
    EarlyStopping = None
    ModelCheckpoint = None
    TENSORFLOW_AVAILABLE = False
import uuid
import hashlib
import mlflow
from mlflow.tracking import MlflowClient
import optuna
import yaml
from sklearn.linear_model import Ridge

# Suppress Git warnings
os.environ['GIT_PYTHON_REFRESH'] = 'quiet'

# Suppress MLflow and Keras warnings
warnings.filterwarnings('ignore', category=UserWarning, module='mlflow')
warnings.filterwarnings('ignore', category=UserWarning, module='keras.saving')
warnings.filterwarnings('ignore', category=FutureWarning, module='pandas')
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))

logger = logging.getLogger(__name__)

class ModelManager:
    def __init__(self):
        self.models_dir      = os.path.join(PROJECT_ROOT, 'models')
        self.scalers_dir     = os.path.join(PROJECT_ROOT, 'scalers')
        self.experiments_dir = os.path.join(PROJECT_ROOT, 'experiments')
        
        # Cache cho models v√† scalers
        self.model_cache = {}
        self.scaler_cache = {}
        
        # MLflow API configuration
        self.mlflow_config = {
            'tracking_uri': os.getenv('MLFLOW_TRACKING_URI', 'http://77.37.44.237:5003'),
            'registry_uri': os.getenv('MLFLOW_REGISTRY_URI', 'http://77.37.44.237:5003'),
            'api_base_url': os.getenv('MLFLOW_API_BASE_URL', 'http://77.37.44.237:5003/api/2.0'),
            'username': os.getenv('MLFLOW_USERNAME', ''),
            'password': os.getenv('MLFLOW_PASSWORD', '')
        }
        
        # Now ensure directories and cleanup experiments
        self.ensure_directories()
        
        # C·∫•u h√¨nh model v·ªõi hyperparameters t·ªëi ∆∞u cho t√†i nguy√™n th·∫•p
        self.model_config = {
            'xgboost': {
                'n_estimators': [50, 100, 150],  # Gi·∫£m t·ª´ 100-300 xu·ªëng 50-150
                'max_depth': [3, 4, 6],  # Gi·∫£m t·ª´ 4-10 xu·ªëng 3-6
                'learning_rate': [0.05, 0.1, 0.15],  # Gi·∫£m range
                'subsample': [0.8, 0.9],  # Gi·∫£m options
                'colsample_bytree': [0.8, 0.9],  # Gi·∫£m options
                'random_state': 42
            },
            'lstm': {
                'units': [32, 64, 128],  # Gi·∫£m t·ª´ 64-256 xu·ªëng 32-128
                'dropout': [0.1, 0.2],  # Gi·∫£m options
                'epochs': 50,  # Gi·∫£m t·ª´ 300 xu·ªëng 50
                'batch_size': [8, 16, 32],  # Gi·∫£m batch size
                'sequence_length': [12, 24, 36],  # Gi·∫£m t·ª´ 24-60 xu·ªëng 12-36
                'learning_rate': [0.001, 0.005],  # Gi·∫£m options
                'layers': [1, 2]  # Gi·∫£m t·ª´ 2-4 xu·ªëng 1-2
            }
        }
        
        # C·∫•u h√¨nh training t·ªëi ∆∞u cho t√†i nguy√™n th·∫•p
        self.training_config = {
            'test_size': 0.2,
            'cv_folds': 3,  # Gi·∫£m t·ª´ 5 xu·ªëng 3
            'random_state': 42,
            'early_stopping_patience': 5  # Gi·∫£m t·ª´ 10 xu·ªëng 5
        }

    def ensure_directories(self):
        """T·∫°o th∆∞ m·ª•c c·∫ßn thi·∫øt"""
        os.makedirs(self.models_dir, exist_ok=True)
        os.makedirs(self.scalers_dir, exist_ok=True)
        os.makedirs(self.experiments_dir, exist_ok=True)
        
        # Clean up deleted experiments at startup
        self.cleanup_deleted_experiments()

    def cleanup_deleted_experiments(self):
        """Clean up t·∫•t c·∫£ deleted experiments ƒë·ªÉ tr√°nh conflict"""
        try:
            # Check if mlflow_config is available
            if not hasattr(self, 'mlflow_config'):
                logger.warning("MLflow config not available, skipping experiment cleanup")
                return
                
            logger.info("Cleaning up deleted experiments...")
            experiments_response = self.mlflow_api_call('mlflow/experiments/search?max_results=1000')
            if not experiments_response:
                logger.info("No experiments found or MLflow not accessible")
                return
            
            deleted_count = 0
            for exp in experiments_response.get('experiments', []):
                if exp.get('lifecycle_stage') == 'deleted':
                    experiment_id = exp.get('experiment_id')
                    experiment_name = exp.get('name')
                    
                    # Permanently delete experiment
                    delete_data = {'experiment_id': experiment_id}
                    delete_response = self.mlflow_api_call('mlflow/experiments/delete', 'POST', delete_data)
                    if delete_response:
                        logger.info(f"Permanently deleted experiment: {experiment_name} (ID: {experiment_id})")
                        deleted_count += 1
            
            if deleted_count > 0:
                logger.info(f"Cleaned up {deleted_count} deleted experiments")
            else:
                logger.info("No deleted experiments found to clean up")
                
        except Exception as e:
            logger.error(f"Error cleaning up deleted experiments: {e}")
            logger.info("Continuing without MLflow cleanup")

    def get_model_path(self, station_id, model_type: str, version: str = None) -> str:
        """L·∫•y ƒë∆∞·ªùng d·∫´n l∆∞u model"""
        if version is None:
            version = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # Handle 'global' station_id specially
        if station_id == 'global':
            station_name = 'global'
        else:
            station_name = f"station_{station_id}"
        
        # Use .keras extension for LSTM models, .pkl for others
        if model_type == 'lstm':
            filename = f"{model_type}_{station_name}_v{version}.keras"
        else:
            filename = f"{model_type}_{station_name}_v{version}.pkl"
        
        return os.path.join(self.models_dir, filename)

    def get_scaler_path(self, station_id, model_type: str, version: str = None) -> str:
        """L·∫•y ƒë∆∞·ªùng d·∫´n ƒë·∫øn scaler"""
        # Handle 'global' station_id specially
        if station_id == 'global':
            station_name = 'global'
        else:
            station_name = f"station_{station_id}"
        
        if version:
            return os.path.join(self.scalers_dir, f"{model_type}_scaler_{station_name}_v{version}.pkl")
        return os.path.join(self.scalers_dir, f"{model_type}_scaler_{station_name}.pkl")

    def get_experiment_path(self, experiment_id: str) -> str:
        """L·∫•y ƒë∆∞·ªùng d·∫´n ƒë·∫øn experiment"""
        return os.path.join(self.experiments_dir, f"{experiment_id}.json")

    def create_experiment_id(self, station_id: int, model_type: str) -> str:
        """T·∫°o experiment ID duy nh·∫•t"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        unique_id = str(uuid.uuid4())[:8]
        return f"{model_type}_station_{station_id}_{timestamp}_{unique_id}"

    def save_experiment(self, experiment_data: Dict[str, Any]) -> str:
        """L∆∞u experiment data"""
        experiment_id = experiment_data['experiment_id']
        experiment_path = self.get_experiment_path(experiment_id)
        
        with open(experiment_path, 'w') as f:
            json.dump(experiment_data, f, indent=2, default=str)
        
        logger.info(f"Experiment saved: {experiment_path}")
        return experiment_id

    def load_experiment(self, experiment_id: str) -> Optional[Dict[str, Any]]:
        """Load experiment data"""
        experiment_path = self.get_experiment_path(experiment_id)
        
        if os.path.exists(experiment_path):
            with open(experiment_path, 'r') as f:
                return json.load(f)
        
        return None

    def prepare_training_data(self, data: pd.DataFrame, model_type: str) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
        """Chu·∫©n b·ªã d·ªØ li·ªáu training cho Global Multi-Series WQI Forecasting"""
        try:
            # Ch·ªâ s·ª≠ d·ª•ng WQI l√†m target
            target = 'wqi'
            
            # Ki·ªÉm tra c·ªôt WQI v√† station_id
            if target not in data.columns:
                logger.error(f"Target column '{target}' not found in data columns: {list(data.columns)}")
                return None, None, None, None
            
            if 'station_id' not in data.columns:
                logger.error(f"Station ID column not found in data columns: {list(data.columns)}")
                return None, None, None, None
            
            # Lo·∫°i b·ªè rows c√≥ missing values cho WQI v√† station_id
            data_clean = data[[target, 'station_id']].dropna()
            
            if len(data_clean) < 50:
                logger.warning(f"Insufficient data after cleaning: {len(data_clean)} samples")
                return None, None, None, None
            
            # Convert WQI to float
            if data_clean[target].dtype == 'object':
                data_clean[target] = pd.to_numeric(data_clean[target], errors='coerce')
            elif hasattr(data_clean[target].iloc[0], 'as_tuple'):  # Check if Decimal
                data_clean[target] = data_clean[target].astype(float)
            
            # Convert station_id to int
            if data_clean['station_id'].dtype == 'object':
                data_clean['station_id'] = pd.to_numeric(data_clean['station_id'], errors='coerce')
            
            # Remove any rows that became NaN after conversion
            data_clean = data_clean.dropna()
            
            if len(data_clean) < 50:
                logger.warning(f"Insufficient data after type conversion: {len(data_clean)} samples")
                return None, None, None, None
            
            # Add temporal features for time-series analysis (2003-2023)
            if 'timestamp' in data.columns:
                try:
                    data_clean['timestamp'] = pd.to_datetime(data['timestamp'])
                except Exception as e:
                    logger.warning(f"Error converting timestamp column: {e}")
                    # Create synthetic timestamp for monthly data (15th of each month)
                    data_clean['timestamp'] = pd.date_range(start='2003-01-15', periods=len(data_clean), freq='M')
            elif 'Date' in data.columns:
                try:
                    data_clean['timestamp'] = pd.to_datetime(data['Date'])
                except Exception as e:
                    logger.warning(f"Error converting Date column: {e}")
                    # Create synthetic timestamp for monthly data (15th of each month)
                    data_clean['timestamp'] = pd.date_range(start='2003-01-15', periods=len(data_clean), freq='M')
            elif 'created_at' in data.columns:
                try:
                    data_clean['timestamp'] = pd.to_datetime(data['created_at'])
                except Exception as e:
                    logger.warning(f"Error converting created_at column: {e}")
                    # Create synthetic timestamp for monthly data (15th of each month)
                    data_clean['timestamp'] = pd.date_range(start='2003-01-15', periods=len(data_clean), freq='M')
            else:
                # Create synthetic timestamp for monthly data (15th of each month)
                data_clean['timestamp'] = pd.date_range(start='2003-01-15', periods=len(data_clean), freq='M')
            
            # Sort by timestamp to maintain temporal order
            data_clean = data_clean.sort_values('timestamp').reset_index(drop=True)
            
            # Extract comprehensive temporal features from WQI
            if 'timestamp' in data_clean.columns:
                data_clean['month'] = data_clean['timestamp'].dt.month
                data_clean['year'] = data_clean['timestamp'].dt.year
                data_clean['quarter'] = data_clean['timestamp'].dt.quarter
                data_clean['season'] = data_clean['timestamp'].dt.month % 12 // 3 + 1  # 1-4 for seasons
                
                # Cyclic encoding for temporal features
                data_clean['month_sin'] = np.sin(2 * np.pi * data_clean['month'] / 12)
                data_clean['month_cos'] = np.cos(2 * np.pi * data_clean['month'] / 12)
                data_clean['quarter_sin'] = np.sin(2 * np.pi * data_clean['quarter'] / 4)
                data_clean['quarter_cos'] = np.cos(2 * np.pi * data_clean['quarter'] / 4)
                
                # Year encoding (normalize to 0-1 range)
                data_clean['year_normalized'] = (data_clean['year'] - 2003) / (2023 - 2003)
                
                # Add seasonal decomposition features
                data_clean['is_rainy_season'] = ((data_clean['month'] >= 5) & (data_clean['month'] <= 10)).astype(int)
                data_clean['is_dry_season'] = ((data_clean['month'] <= 4) | (data_clean['month'] >= 11)).astype(int)
            else:
                # Fallback: create basic temporal features without timestamp
                logger.warning("No timestamp column available, creating basic temporal features")
                data_clean['month'] = 6  # Default to June
                data_clean['year'] = 2013  # Default to middle year
                data_clean['quarter'] = 2
                data_clean['season'] = 2
                data_clean['month_sin'] = 0
                data_clean['month_cos'] = -1
                data_clean['quarter_sin'] = 1
                data_clean['quarter_cos'] = 0
                data_clean['year_normalized'] = 0.5
                data_clean['is_rainy_season'] = 1
                data_clean['is_dry_season'] = 0
            
            # Add lag features for time-series (WQI lags) - PER STATION (simplified)
            for station_id in data_clean['station_id'].unique():
                station_mask = data_clean['station_id'] == station_id
                station_data = data_clean[station_mask].copy()
                
                # Sort station data by timestamp to ensure proper lag calculation
                if 'timestamp' in station_data.columns:
                    station_data = station_data.sort_values('timestamp').reset_index(drop=True)
                
                for lag in [1, 2, 3, 6, 12]:  # Gi·∫£m t·ª´ 6 lags xu·ªëng 5 lags
                    col_name = f'wqi_station_{station_id}_lag_{lag}'
                    
                    if len(station_data) > lag:
                        lag_values = station_data[target].shift(lag)
                        # Fill NaN values with forward fill or mean
                        lag_values = lag_values.ffill().fillna(station_data[target].mean())
                        # Create a new column for this station's lag
                        data_clean[col_name] = 0.0  # Initialize with 0
                        data_clean.loc[station_mask, col_name] = lag_values.values
                    else:
                        # If not enough data for lag, use current value
                        data_clean[col_name] = 0.0  # Initialize with 0
                        if len(station_data) > 0:
                            data_clean.loc[station_mask, col_name] = station_data[target].iloc[0]
            
            # Add rolling statistics with multiple windows - PER STATION (simplified)
            for station_id in data_clean['station_id'].unique():
                station_mask = data_clean['station_id'] == station_id
                station_data = data_clean[station_mask].copy()
                
                # Sort station data by timestamp
                if 'timestamp' in station_data.columns:
                    station_data = station_data.sort_values('timestamp').reset_index(drop=True)
                
                for window in [3, 6, 12]:  # Gi·∫£m t·ª´ 4 windows xu·ªëng 3 windows
                    mean_col = f'wqi_station_{station_id}_rolling_mean_{window}'
                    std_col = f'wqi_station_{station_id}_rolling_std_{window}'
                    
                    # Initialize columns with 0
                    data_clean[mean_col] = 0.0
                    data_clean[std_col] = 0.0
                    
                    if len(station_data) > window:
                        rolling_mean = station_data[target].rolling(window=window, min_periods=1).mean()
                        rolling_std = station_data[target].rolling(window=window, min_periods=1).std()
                        
                        # Fill any remaining NaN values
                        rolling_mean = rolling_mean.fillna(station_data[target].mean())
                        rolling_std = rolling_std.fillna(station_data[target].std())
                        
                        data_clean.loc[station_mask, mean_col] = rolling_mean.values
                        data_clean.loc[station_mask, std_col] = rolling_std.values
                    else:
                        # If not enough data for rolling window, use current statistics
                        if len(station_data) > 0:
                            current_mean = station_data[target].mean()
                            current_std = station_data[target].std()
                        else:
                            current_mean = current_std = 0.0
                        
                        data_clean.loc[station_mask, mean_col] = current_mean
                        data_clean.loc[station_mask, std_col] = current_std
            
            # Add global features (across all stations) - SAFE expanding statistics
            # Use expanding window instead of rolling to avoid future leakage
            data_clean['wqi_global_expanding_mean'] = data_clean[target].expanding(min_periods=1).mean()
            data_clean['wqi_global_expanding_std'] = data_clean[target].expanding(min_periods=1).std()
            
            # Limited global rolling statistics with proper shift (only past information)
            for window in [3, 6]:  # Reduced windows to avoid overfitting
                if len(data_clean) > window:
                    data_clean[f'wqi_global_rolling_mean_{window}'] = data_clean[target].shift(1).rolling(window=window, min_periods=1).mean()
                    data_clean[f'wqi_global_rolling_std_{window}'] = data_clean[target].shift(1).rolling(window=window, min_periods=1).std()
                    
                    # Fill NaN values with expanding statistics
                    data_clean[f'wqi_global_rolling_mean_{window}'] = data_clean[f'wqi_global_rolling_mean_{window}'].fillna(data_clean['wqi_global_expanding_mean'])
                    data_clean[f'wqi_global_rolling_std_{window}'] = data_clean[f'wqi_global_rolling_std_{window}'].fillna(data_clean['wqi_global_expanding_std'])
                else:
                    # If not enough data, use expanding statistics
                    data_clean[f'wqi_global_rolling_mean_{window}'] = data_clean['wqi_global_expanding_mean']
                    data_clean[f'wqi_global_rolling_std_{window}'] = data_clean['wqi_global_expanding_std']
            
            # Add station-specific features
            # One-hot encoding for stations
            station_dummies = pd.get_dummies(data_clean['station_id'], prefix='station')
            data_clean = pd.concat([data_clean, station_dummies], axis=1)
            
            # Station embedding features (normalized station_id)
            for station_id in data_clean['station_id'].unique():
                data_clean[f'station_{station_id}_embedding'] = station_id / 10.0
            
            # Check for new stations that might not be in training data
            # This is for prediction with new stations
            if hasattr(self, 'training_stations'):
                new_stations = set(data_clean['station_id'].unique()) - set(self.training_stations)
                for new_station_id in new_stations:
                    data_clean = self.handle_new_station_features(data_clean, new_station_id)
            else:
                # Store training stations for future reference
                self.training_stations = set(data_clean['station_id'].unique())
            
            # Get all feature columns (excluding timestamp and target, but INCLUDING station_id)
            feature_columns = [col for col in data_clean.columns if col not in ['timestamp', target, 'created_at', 'Date']]
            
            # Debug: Log data size at each step
            logger.info(f"Data size before dropna: {len(data_clean)}")
            logger.info(f"Feature columns count: {len(feature_columns)}")
            logger.info(f"Target column: {target}")
            
            # Check for NaN values before dropping
            try:
                nan_counts = data_clean[feature_columns + [target]].isnull().sum()
                if nan_counts.sum() > 0:
                    logger.warning(f"NaN values found before dropna: {nan_counts[nan_counts > 0].to_dict()}")
            except Exception as e:
                logger.error(f"Error checking NaN values: {e}")
                logger.error(f"Feature columns: {feature_columns}")
                logger.error(f"Available columns: {list(data_clean.columns)}")
                return None, None, None, None
            
            # Instead of dropping NaN values, fill them with appropriate values
            # Fill station-specific features with 0 for non-station rows
            for col in feature_columns:
                if 'station_' in col and col not in ['station_id', 'station_embedding', 'station_0', 'station_1', 'station_2']:
                    data_clean[col] = data_clean[col].fillna(0.0)
            
            # Fill any remaining NaN values with 0
            data_clean = data_clean.fillna(0.0)
            
            logger.info(f"Data size after fillna: {len(data_clean)}")
            
            if len(data_clean) < 50:  # Reduced minimum threshold
                logger.warning(f"Insufficient data after feature engineering: {len(data_clean)} samples")
                logger.warning(f"Original data size: {len(data)}")
                logger.warning(f"Data after initial cleaning: {len(data_clean)}")
                
                # Try to reduce features to get more data
                logger.info("Attempting to reduce features to get more data...")
                
                # Remove complex features that might cause data loss
                simple_features = []
                for col in feature_columns:
                    # Keep basic features
                    if any(keyword in col for keyword in ['station_', 'month', 'year', 'season', 'quarter']):
                        simple_features.append(col)
                    # Keep only short lag features (lag_1, lag_2, lag_3)
                    elif 'lag_' in col and any(lag in col for lag in ['lag_1', 'lag_2', 'lag_3']):
                        simple_features.append(col)
                    # Keep only short rolling features (rolling_3, rolling_6)
                    elif 'rolling_' in col and any(rolling in col for rolling in ['rolling_3', 'rolling_6']):
                        simple_features.append(col)
                    # Keep basic WQI features
                    elif col in ['wqi', 'wqi_ma3', 'wqi_ma6']:
                        simple_features.append(col)
                
                logger.info(f"Reduced features from {len(feature_columns)} to {len(simple_features)}")
                logger.info(f"Simple features: {simple_features}")
                
                # Try with simplified features
                try:
                    X_simple = data_clean[simple_features].values
                    y_simple = data_clean[target].values
                    
                    # Fill any remaining NaN values
                    X_simple = np.nan_to_num(X_simple, nan=0.0)
                    y_simple = np.nan_to_num(y_simple, nan=0.0)
                    
                    if len(X_simple) >= 30:  # Even lower threshold for simple features
                        logger.info(f"Using simplified features: {len(X_simple)} samples")
                        feature_columns = simple_features
                        X = X_simple
                        y = y_simple
                    else:
                        logger.error("Even simplified features don't provide enough data")
                        return None, None, None, None
                        
                except Exception as e:
                    logger.error(f"Error with simplified features: {e}")
                    return None, None, None, None
            
            try:
                logger.info(f"Creating X from feature columns: {len(feature_columns)} columns")
                X = data_clean[feature_columns].values
                logger.info(f"X shape: {X.shape}")
                
                logger.info(f"Creating y from target column: {target}")
                y = data_clean[target].values
                logger.info(f"y shape: {y.shape}")
            except Exception as e:
                logger.error(f"Error creating X and y: {e}")
                logger.error(f"Feature columns: {feature_columns}")
                logger.error(f"Available columns: {list(data_clean.columns)}")
                logger.error(f"Target column: {target}")
                return None, None, None, None
            
            logger.info(f"Global Multi-Series data prepared: {len(X)} samples, {len(feature_columns)} features")
            logger.info(f"Stations included: {sorted(data_clean['station_id'].unique())}")
            logger.info(f"Station features: {[col for col in feature_columns if 'station_' in col]}")
            logger.info(f"Temporal features: {[col for col in feature_columns if 'lag_' in col or 'rolling_' in col or col in ['month', 'year', 'season', 'quarter']]}")
            logger.info(f"WQI range: {y.min():.2f} - {y.max():.2f}")
            if 'timestamp' in data_clean.columns:
                logger.info(f"Time range: {data_clean['timestamp'].min()} to {data_clean['timestamp'].max()}")
            else:
                logger.info("No timestamp information available")
            
            # For time-series data, ALWAYS use temporal split to avoid data leakage
            # Use last 20% for testing (maintain temporal order)
            split_idx = int(len(X) * 0.8)
            X_train, X_test = X[:split_idx], X[split_idx:]
            y_train, y_test = y[:split_idx], y[split_idx:]
            
            logger.info(f"Time-based split: Train={len(X_train)} samples, Test={len(X_test)} samples")
            if 'timestamp' in data_clean.columns:
                train_dates = data_clean.iloc[:split_idx]['timestamp']
                test_dates = data_clean.iloc[split_idx:]['timestamp']
                logger.info(f"Train period: {train_dates.min()} to {train_dates.max()}")
                logger.info(f"Test period: {test_dates.min()} to {test_dates.max()}")
            
            # Scale features
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)
            
            # Cache scaler (use 'global' as key for multi-series model)
            self.scaler_cache[f"{model_type}_global"] = scaler
            
            # Also save scaler with 'global' key for easy access
            global_scaler_path = self.get_scaler_path('global', model_type)
            joblib.dump(scaler, global_scaler_path)
            
            logger.info(f"Data split: Train={len(X_train)}, Test={len(X_test)}")
            logger.info(f"Feature count: {X_train.shape[1]} (including global multi-series features)")
            
            # Log feature list for reproducibility
            feature_list = [f'feature_{i}' for i in range(X_train.shape[1])]
            logger.info(f"Training feature list ({len(feature_list)} features): {feature_list}")
            
            # Save feature list to JSON for later use
            feature_info = {
                'feature_list': feature_list,
                'feature_count': int(len(feature_list)),
                'training_date': datetime.now().isoformat(),
                'model_type': 'xgboost_global_multiseries',
                'station_id': 'global'  # Use 'global' for multi-series model
            }
            
            feature_list_path = os.path.join(self.models_dir, f'feature_list_global.json')
            with open(feature_list_path, 'w') as f:
                json.dump(feature_info, f, indent=2)
            logger.info(f"Feature list saved to: {feature_list_path}")
            
            # MLflow logging of feature list
            try:
                import mlflow
                # End any existing run first to avoid nested runs
                try:
                    mlflow.end_run()
                except:
                    pass
                
                mlflow.log_param("feature_count", int(len(feature_list)))
                mlflow.log_param("feature_list", json.dumps(feature_list))
                mlflow.log_artifact(feature_list_path, "feature_list.json")
            except Exception as mlflow_error:
                logger.warning(f"MLflow feature logging failed: {mlflow_error}")
            
            return X_train_scaled, X_test_scaled, y_train, y_test
            
        except Exception as e:
            logger.error(f"Error preparing training data: {e}")
            return None, None, None, None

    def evaluate_model(self, model, X_test: np.ndarray, y_test: np.ndarray, model_type: str) -> Dict[str, float]:
        """ƒê√°nh gi√° model v·ªõi multiple metrics"""
        try:
            # Ensure data types are correct
            if model_type == 'lstm':
                X_test = X_test.astype(np.float32)
                y_test = y_test.astype(np.float32)
                
                # Check if data is already in sequence format (3D)
                if len(X_test.shape) == 3:
                    # Data is already in sequence format (n_samples, sequence_length, n_features)
                    logger.info(f"LSTM data already in sequence format: {X_test.shape}")
                    y_pred = model.predict(X_test).flatten()
                else:
                    # Data is in 2D format, need to reshape
                    logger.info(f"LSTM data in 2D format, reshaping: {X_test.shape}")
                    X_test_reshaped = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))
                    y_pred = model.predict(X_test_reshaped).flatten()
            else:
                X_test = X_test.astype(np.float64)
                y_test = y_test.astype(np.float64)
                y_pred = model.predict(X_test)
            
            # Ensure predictions are also float
            if model_type == 'lstm':
                y_pred = y_pred.astype(np.float32)
            else:
                y_pred = y_pred.astype(np.float64)
            
            # Calculate metrics
            mae = mean_absolute_error(y_test, y_pred)
            mse = mean_squared_error(y_test, y_pred)
            rmse = np.sqrt(mse)
            r2 = r2_score(y_test, y_pred)
            
            # Calculate MAPE (Mean Absolute Percentage Error)
            mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
            
            metrics = {
                'mae': float(mae),
                'mse': float(mse),
                'rmse': float(rmse),
                'r2_score': float(r2),
                'mape': float(mape),
                'accuracy': float(max(0, r2))  # R2 c√≥ th·ªÉ √¢m
            }
            
            logger.info(f"Model evaluation - MAE: {mae:.4f}, RMSE: {rmse:.4f}, R2: {r2:.4f}, MAPE: {mape:.2f}%")
            return metrics
            
        except Exception as e:
            logger.error(f"Error evaluating model: {e}")
            import traceback
            logger.error(f"Full traceback: {traceback.format_exc()}")
            return {}

    def cross_validate_model(self, model_class, X_train: np.ndarray, y_train: np.ndarray, params: dict, model_type: str) -> Dict[str, float]:
        """Cross-validation cho model"""
        try:
            if model_type == 'lstm':
                # LSTM kh√¥ng h·ªó tr·ª£ cross-validation tr·ª±c ti·∫øp, return default metrics
                return {'cv_score': 0.0, 'cv_std': 0.0}
            
            # T·∫°o model v·ªõi parameters (remove random_state from params to avoid duplicate)
            if model_type == 'xgboost':
                model_params = params.copy()
                # Remove random_state from params since it's passed separately in XGBRegressor
                if 'random_state' in model_params:
                    del model_params['random_state']
                model = xgb.XGBRegressor(**model_params, random_state=self.training_config['random_state'])
            
            # Cross-validation
            cv_scores = cross_val_score(
                model, X_train, y_train, 
                cv=KFold(n_splits=self.training_config['cv_folds'], shuffle=True, random_state=self.training_config['random_state']),
                scoring='r2'
            )
            
            cv_metrics = {
                'cv_score': cv_scores.mean(),
                'cv_std': cv_scores.std(),
                'cv_scores': cv_scores.tolist()
            }
            
            logger.info(f"Cross-validation - Mean R2: {cv_metrics['cv_score']:.4f} ¬± {cv_metrics['cv_std']:.4f}")
            return cv_metrics
            
        except Exception as e:
            logger.error(f"Error in cross-validation: {e}")
            return {'cv_score': 0.0, 'cv_std': 0.0}

    def hyperparameter_tuning(self, X_train: np.ndarray, y_train: np.ndarray, model_type: str) -> Tuple[dict, float]:
        """Hyperparameter tuning v·ªõi Optuna optimization"""
        try:
            # Ensure data is properly shaped
            if len(X_train.shape) > 2:
                logger.warning(f"X_train has shape {X_train.shape}, flattening to 2D")
                X_train = X_train.reshape(X_train.shape[0], -1)
            
            if len(y_train.shape) > 1:
                logger.warning(f"y_train has shape {y_train.shape}, flattening to 1D")
                y_train = y_train.flatten()
            
            logger.info(f"Optimization data shapes: X_train {X_train.shape}, y_train {y_train.shape}")
            
            if model_type == 'xgboost':
                return self._optimize_xgboost(X_train, y_train)
            elif model_type == 'lstm':
                return self._optimize_lstm(X_train, y_train)
            else:
                logger.error(f"Unsupported model type: {model_type}")
                return {}, -float('inf')
                
        except Exception as e:
            logger.error(f"Error in hyperparameter tuning: {e}")
            return {}, -float('inf')
    
    def _optimize_xgboost(self, X_train: np.ndarray, y_train: np.ndarray) -> Tuple[dict, float]:
        """Optimize XGBoost hyperparameters v·ªõi Optuna - simple train/test split"""
        
        def objective(trial):
            # Suggest hyperparameters - expanded search space
            params = {
                'n_estimators': trial.suggest_int('n_estimators', 100, 500),  # TƒÉng range
                'max_depth': trial.suggest_int('max_depth', 3, 12),  # TƒÉng depth
                'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.2, log=True),  # M·ªü r·ªông range
                'subsample': trial.suggest_float('subsample', 0.6, 1.0),
                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
                'gamma': trial.suggest_float('gamma', 0.0, 1.0),  # Min split loss
                'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),  # Min child weight
                'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),  # L1 regularization
                'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 1.0),  # L2 regularization
                'random_state': self.training_config['random_state']
            }
            
            # TimeSeriesSplit cross-validation for time series data
            tscv = TimeSeriesSplit(n_splits=3, test_size=int(0.2 * len(X_train)))
            scores = []
            
            for train_idx, val_idx in tscv.split(X_train):
                X_train_split, X_val_split = X_train[train_idx], X_train[val_idx]
                y_train_split, y_val_split = y_train[train_idx], y_train[val_idx]
                
                if len(X_train_split) < 10 or len(X_val_split) < 5:
                    continue  # Skip this fold if insufficient data
                
                # Ensure y_train_split is 1D
                if len(y_train_split.shape) > 1:
                    y_train_split = y_train_split.flatten()
                if len(y_val_split.shape) > 1:
                    y_val_split = y_val_split.flatten()
                
                # Create params without random_state for model
                cv_params = params.copy()
                del cv_params['random_state']
                
                try:
                    model = xgb.XGBRegressor(**cv_params)
                    model.fit(X_train_split, y_train_split)
                    y_pred = model.predict(X_val_split)
                    
                    # Use negative MAE as score (minimize)
                    mae = mean_absolute_error(y_val_split, y_pred)
                    scores.append(-mae)
                except Exception as e:
                    logger.warning(f"XGBoost trial failed: {e}")
                    continue
            
            if not scores:
                return -float('inf')
            
            return np.mean(scores)
        
        # Create Optuna study
        study = optuna.create_study(direction='maximize')
        study.optimize(objective, n_trials=10, timeout=300)  # Gi·∫£m trials v√† timeout ƒë·ªÉ ti·∫øt ki·ªám t√†i nguy√™n
        
        best_params = study.best_params
        best_params['random_state'] = self.training_config['random_state']
        best_score = study.best_value
        
        logger.info(f"Best XGBoost params: {best_params} (Score: {best_score:.4f})")
        return best_params, best_score
    
    def _optimize_lstm(self, X_train: np.ndarray, y_train: np.ndarray) -> Tuple[dict, float]:
        """Optimize LSTM hyperparameters v·ªõi Optuna - TimeSeriesSplit CV"""
        if not TENSORFLOW_AVAILABLE:
            logger.warning("TensorFlow not available, skipping LSTM optimization")
            return {}, 0.0
        
        def objective(trial):
            # Suggest hyperparameters - lightweight ranges for resource efficiency
            params = {
                'units': trial.suggest_categorical('units', [32, 64, 128]),  # Gi·∫£m units
                'dropout': trial.suggest_float('dropout', 0.1, 0.3),  # Gi·∫£m dropout range
                'batch_size': trial.suggest_categorical('batch_size', [8, 16, 32]),  # Gi·∫£m batch size
                'sequence_length': trial.suggest_categorical('sequence_length', [12, 24, 36]),  # Gi·∫£m sequence length
                'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.01, log=True),
                'layers': trial.suggest_int('layers', 1, 2),  # Gi·∫£m layers
                'optimizer': trial.suggest_categorical('optimizer', ['adam', 'rmsprop']),  # Gi·∫£m optimizer options
                'epochs': 50  # Gi·∫£m epochs cho optimization
            }
            
            # TimeSeriesSplit cross-validation for time series data
            tscv = TimeSeriesSplit(n_splits=3, test_size=int(0.2 * len(X_train)))
            scores = []
            
            for train_idx, val_idx in tscv.split(X_train):
                X_train_split, X_val_split = X_train[train_idx], X_train[val_idx]
                y_train_split, y_val_split = y_train[train_idx], y_train[val_idx]
                
                if len(X_train_split) < 10 or len(X_val_split) < 5:
                    continue  # Skip this fold if insufficient data
                
                try:
                    # Ensure data is 1D for sequence creation
                    if len(y_train_split.shape) > 1:
                        y_train_split = y_train_split.flatten()
                    if len(y_val_split.shape) > 1:
                        y_val_split = y_val_split.flatten()
                    
                    # Create sequences
                    X_seq_train, y_seq_train = self.create_sequences(
                        X_train_split, y_train_split, params['sequence_length']
                    )
                    X_seq_val, y_seq_val = self.create_sequences(
                        X_val_split, y_val_split, params['sequence_length']
                    )
                    
                    if len(X_seq_train) == 0 or len(X_seq_val) == 0:
                        continue  # Skip this fold
                    
                    # Build and train model
                    model = self.create_lstm_model(X_seq_train.shape[2], params)
                    
                    # Advanced callbacks
                    early_stopping = EarlyStopping(
                        patience=10,  # TƒÉng patience
                        restore_best_weights=True,
                        monitor='val_loss'
                    )
                    
                    # Learning rate scheduler
                    lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(
                        monitor='val_loss',
                        factor=0.5,
                        patience=5,
                        min_lr=1e-7,
                        verbose=0
                    )
                    
                    # Model checkpoint
                    checkpoint = tf.keras.callbacks.ModelCheckpoint(
                        'best_lstm_model.h5',
                        monitor='val_loss',
                        save_best_only=True,
                        verbose=0
                    )
                    
                    model.fit(
                        X_seq_train, y_seq_train,
                        validation_data=(X_seq_val, y_seq_val),
                        epochs=params['epochs'],
                        batch_size=params['batch_size'],
                        callbacks=[early_stopping, lr_scheduler, checkpoint],
                        verbose=0
                    )
                    
                    y_pred = model.predict(X_seq_val)
                    mae = mean_absolute_error(y_seq_val, y_pred)
                    scores.append(-mae)
                    
                except Exception as e:
                    logger.warning(f"LSTM trial failed: {e}")
                    continue
            
            if not scores:
                return -float('inf')
            
            return np.mean(scores)
        
        # Create Optuna study
        study = optuna.create_study(direction='maximize')
        study.optimize(objective, n_trials=8, timeout=400)  # Gi·∫£m trials v√† timeout ƒë·ªÉ ti·∫øt ki·ªám t√†i nguy√™n
        
        best_params = study.best_params
        best_params['epochs'] = 100  # TƒÉng epochs cho final training
        best_score = study.best_value
        
        logger.info(f"Best LSTM params: {best_params} (Score: {best_score:.4f})")
        return best_params, best_score

    def train_xgboost_model(self, station_id: int, data: pd.DataFrame) -> Dict[str, Any]:
        """Train XGBoost model cho Global Multi-Series WQI Forecasting"""
        try:
            import mlflow
            logger.info(f"Training Global Multi-Series XGBoost model for WQI forecasting")
            logger.info(f"Data includes stations: {sorted(data['station_id'].unique())}")
            
            # T·∫°o experiment ID
            experiment_id = self.create_experiment_id(station_id, 'xgboost')
            
            # Chu·∫©n b·ªã d·ªØ li·ªáu (s·ª≠ d·ª•ng to√†n b·ªô data t·ª´ t·∫•t c·∫£ stations)
            data_split = self.prepare_training_data(data, 'xgboost')
            if data_split[0] is None:
                return {'error': 'Insufficient data'}
            
            X_train, X_test, y_train, y_test = data_split
            
            # Hyperparameter tuning
            best_params, best_cv_score = self.hyperparameter_tuning(X_train, y_train, 'xgboost')
            
            if not best_params:
                return {'error': 'Hyperparameter tuning failed'}
            
            # Train final model v·ªõi best parameters
            final_model = xgb.XGBRegressor(**best_params)
            final_model.fit(X_train, y_train)
            
            # Log feature list for reproducibility
            feature_list = [f'feature_{i}' for i in range(X_train.shape[1])]
            logger.info(f"Training feature list ({len(feature_list)} features): {feature_list}")
            
            # Save feature list to JSON for later use
            feature_info = {
                'feature_list': feature_list,
                'feature_count': int(len(feature_list)),
                'training_date': datetime.now().isoformat(),
                'model_type': 'xgboost_global_multiseries',
                'station_id': 'global'  # Use 'global' for multi-series model
            }
            
            feature_list_path = os.path.join(self.models_dir, f'feature_list_global.json')
            with open(feature_list_path, 'w') as f:
                json.dump(feature_info, f, indent=2)
            logger.info(f"Feature list saved to: {feature_list_path}")
            
            # Evaluate tr√™n test set
            test_metrics = self.evaluate_model(final_model, X_test, y_test, 'xgboost')
            
            if not test_metrics:
                return {'error': 'Model evaluation failed'}
            
            # T·∫°o model version
            model_version = f"xgboost_global_multiseries_v{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            
            # L∆∞u model
            model_path = self.get_model_path(station_id, 'xgboost', model_version)
            joblib.dump(final_model, model_path)
            
            # Cache model (use 'global' as key for multi-series model)
            self.model_cache[f"xgboost_global"] = final_model
            
            # Also save model with 'global' key for easy access
            global_model_path = self.get_model_path('global', 'xgboost', model_version)
            joblib.dump(final_model, global_model_path)
            
            # T·∫°o experiment data
            experiment_data = {
                'experiment_id': experiment_id,
                'station_id': station_id,
                'model_type': 'xgboost_global_multiseries',
                'model_version': model_version,
                'training_date': datetime.now().isoformat(),
                'hyperparameters': best_params,
                'validation_metrics': test_metrics,
                'test_metrics': test_metrics,
                'data_info': {
                    'train_samples': len(X_train),
                    'test_samples': len(X_test),
                    'features': 'Global Multi-Series: WQI temporal features + station_id features (one-hot + embedding)',
                    'target': 'WQI',
                    'stations_included': sorted(data['station_id'].unique())
                }
            }
            
            # L∆∞u experiment
            self.save_experiment(experiment_data)
            
            # ƒêƒÉng k√Ω model trong MLflow
            self.register_model_in_mlflow(station_id, 'xgboost_global_multiseries', experiment_data)
            
            # Log model v√†o MLflow v√† l·∫•y uri
            experiment_name = "water_quality"
            try:
                mlflow.set_experiment(experiment_name)
                logger.info(f"‚úÖ Using existing experiment: {experiment_name}")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Experiment '{experiment_name}' not found: {e}")
                logger.info("üîÑ Creating new experiment...")
                
                # Create new experiment with timestamp to avoid conflicts
                from datetime import datetime
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                new_experiment_name = f"water_quality_{timestamp}"
                
                try:
                    # Create experiment via MLflow client
                    client = mlflow.tracking.MlflowClient()
                    experiment = client.create_experiment(new_experiment_name)
                    mlflow.set_experiment(new_experiment_name)
                    logger.info(f"‚úÖ Created new experiment: {new_experiment_name}")
                except Exception as create_error:
                    logger.error(f"‚ùå Failed to create experiment: {create_error}")
                    # Fallback to default experiment
                    mlflow.set_experiment("Default")
                    logger.info("‚ÑπÔ∏è Using default experiment as fallback")
            with mlflow.start_run() as run:
                # Log parameters
                for key, value in best_params.items():
                    mlflow.log_param(key, value)
                
                # Log metrics with proper type conversion
                for key, value in test_metrics.items():
                    if isinstance(value, (int, float)):
                        mlflow.log_metric(key, float(value))
                    else:
                        mlflow.log_metric(key, value)
                
                # Log model with proper signature and input example
                try:
                    # Create input example for XGBoost model using actual feature count
                    import numpy as np
                    actual_features = X_train.shape[1]  # Use actual feature count from training data
                    logger.info(f"XGBoost training - Using feature count: {actual_features}")
                    input_example = np.random.rand(1, actual_features).astype(np.float32)
                    
                    # Use signature inference instead of manual signature
                    from mlflow.models.signature import infer_signature
                    
                    # Create sample data for signature inference
                    sample_X = np.random.rand(10, actual_features).astype(np.float32)
                    sample_y = np.random.rand(10).astype(np.float32)
                    signature = infer_signature(sample_X, sample_y)
                    
                    # Log model with inferred signature
                    mlflow.sklearn.log_model(
                        final_model, 
                        "model",
                        input_example=input_example,
                        signature=signature,
                        registered_model_name="water_quality"
                    )
                    logger.info("‚úÖ XGBoost model logged successfully")
                except Exception as e:
                    logger.error(f"‚ùå Failed to log XGBoost model: {e}")
                    # Fallback: log without signature
                    try:
                        mlflow.sklearn.log_model(
                            final_model, 
                            "model",
                            registered_model_name="water_quality"
                        )
                        logger.info("‚úÖ XGBoost model logged without signature")
                    except Exception as e2:
                        logger.error(f"‚ùå Failed to log XGBoost model even without signature: {e2}")
                        return {'error': f'Failed to log model: {e2}'}
                model_uri = f"runs:/{run.info.run_id}/model"
                
                # Register model and transition to staging
                try:
                    from mlflow.tracking import MlflowClient
                    client = MlflowClient()
                    
                    # Get the latest version
                    latest_versions = client.get_latest_versions("water_quality", stages=["None"])
                    if latest_versions:
                        latest_version = latest_versions[0]
                        client.transition_model_version_stage(
                            name="water_quality",
                            version=latest_version.version,
                            stage="Staging",
                            archive_existing_versions=True
                        )
                        logger.info(f"Successfully transitioned model version {latest_version.version} to Staging")
                except Exception as e:
                    logger.warning(f"Failed to transition model to Staging: {e}")
            
            result = {
                'station_id': station_id,
                'model_type': 'xgboost_global_multiseries',
                'model_version': model_version,
                'model_path': model_path,
                'experiment_id': experiment_id,
                'mae': test_metrics.get('mae', 0.0),
                'rmse': test_metrics.get('rmse', 0.0),
                'r2_score': test_metrics.get('r2_score', 0.0),
                'accuracy': test_metrics.get('accuracy', 0.0),
                'mape': test_metrics.get('mape', 0.0),
                'training_date': datetime.now().isoformat(),
                'records_used': len(X_train) + len(X_test),
                'hyperparameters': best_params,
                'cv_score': best_cv_score,
                'stations_included': sorted(data['station_id'].unique()),
                'mlflow_model_uri': model_uri
            }
            
            logger.info(f"Global Multi-Series XGBoost WQI forecasting model trained successfully")
            logger.info(f"Test MAE: {test_metrics.get('mae', 0.0):.4f}, RMSE: {test_metrics.get('rmse', 0.0):.4f}, R2: {test_metrics.get('r2_score', 0.0):.4f}")
            logger.info(f"Stations included: {sorted(data['station_id'].unique())}")
            
            return result
            
        except Exception as e:
            logger.error(f"Error training Global Multi-Series XGBoost model: {e}")
            return {'error': str(e)}

    def train_lstm_model(self, station_id: int, data: pd.DataFrame) -> Dict[str, Any]:
        """Train LSTM model cho Station-Specific WQI Forecasting v·ªõi simplified approach"""
        if not TENSORFLOW_AVAILABLE:
            logger.warning(f"TensorFlow not available, skipping LSTM training for station {station_id}")
            return {'error': 'TensorFlow not available', 'model_type': 'lstm'}
        
        try:
            import mlflow
            import numpy as np
            
            # Configure GPU for TensorFlow
            try:
                gpus = tf.config.experimental.list_physical_devices('GPU')
                if gpus:
                    try:
                        for gpu in gpus:
                            tf.config.experimental.set_memory_growth(gpu, True)
                        logger.info(f"GPU configured for LSTM training: {len(gpus)} GPU(s) available")
                    except RuntimeError as e:
                        logger.warning(f"GPU configuration error: {e}")
                else:
                    logger.info("No GPU found for LSTM training, using CPU")
            except Exception as e:
                logger.warning(f"TensorFlow GPU configuration failed: {e}")
            
            logger.info(f"Training Station-Specific LSTM model for station {station_id}")
            
            # Filter data for this specific station only
            station_data = data[data['station_id'] == station_id].copy()
            
            if len(station_data) < 50:
                logger.warning(f"Insufficient data for station {station_id}: {len(station_data)} samples")
                return {'error': f'Insufficient data for station {station_id}'}
            
            # Sort by timestamp to ensure temporal order
            if 'timestamp' in station_data.columns:
                station_data = station_data.sort_values('timestamp').reset_index(drop=True)
            elif 'Date' in station_data.columns:
                station_data['timestamp'] = pd.to_datetime(station_data['Date'])
                station_data = station_data.sort_values('timestamp').reset_index(drop=True)
            else:
                # Create synthetic timestamp if not available
                station_data['timestamp'] = pd.date_range(start='2003-01-15', periods=len(station_data), freq='M')
                station_data = station_data.sort_values('timestamp').reset_index(drop=True)
            
            # Use only essential features for LSTM
            essential_features = ['ph', 'temperature', 'do']
            available_features = [col for col in essential_features if col in station_data.columns]
            
            if len(available_features) < 2:
                logger.warning(f"Insufficient features for station {station_id}: {available_features}")
                return {'error': f'Insufficient features for station {station_id}'}
            
            # Add basic temporal features
            station_data['month'] = station_data['timestamp'].dt.month
            station_data['year'] = station_data['timestamp'].dt.year
            
            # Cyclic encoding
            station_data['month_sin'] = np.sin(2 * np.pi * station_data['month'] / 12)
            station_data['month_cos'] = np.cos(2 * np.pi * station_data['month'] / 12)
            
            # Seasonal features
            station_data['is_rainy_season'] = ((station_data['month'] >= 5) & (station_data['month'] <= 10)).astype(int)
            
            # Simple lag features
            station_data['wqi_lag_1'] = station_data['wqi'].shift(1)
            station_data['wqi_lag_2'] = station_data['wqi'].shift(2)
            
            # Simple rolling features
            station_data['wqi_rolling_mean_3'] = station_data['wqi'].rolling(window=3, min_periods=1).mean()
            
            # Prepare final features
            feature_columns = available_features + ['month_sin', 'month_cos', 'is_rainy_season', 
                                                 'wqi_lag_1', 'wqi_lag_2', 'wqi_rolling_mean_3']
            
            # Remove rows with NaN values
            station_data = station_data.dropna(subset=feature_columns + ['wqi'])
            
            if len(station_data) < 30:
                logger.warning(f"Insufficient data after cleaning for station {station_id}: {len(station_data)} samples")
                return {'error': f'Insufficient data after cleaning for station {station_id}'}
            
            # Prepare X and y
            X = station_data[feature_columns].values
            y = station_data['wqi'].values
            
            # Time-based split (80% train, 20% test)
            split_idx = int(0.8 * len(X))
            X_train, X_test = X[:split_idx], X[split_idx:]
            y_train, y_test = y[:split_idx], y[split_idx:]
            
            if len(X_train) < 20 or len(X_test) < 5:
                logger.warning(f"Insufficient data for train/test split for station {station_id}")
                return {'error': f'Insufficient data for train/test split for station {station_id}'}
            
            # Hyperparameter tuning with simplified approach
            best_params = {
                'units': 32,  # Reduced complexity
                'dropout': 0.2,
                'batch_size': 16,
                'sequence_length': 12,  # Reduced sequence length
                'learning_rate': 0.001,
                'layers': 1,  # Single layer
                'optimizer': 'adam',
                'epochs': 50  # Reduced epochs
            }
            
            # Create sequences for LSTM
            sequence_length = best_params['sequence_length']
            X_train_seq, y_train_seq = self.create_sequences(X_train, y_train, sequence_length)
            X_test_seq, y_test_seq = self.create_sequences(X_test, y_test, sequence_length)
            
            if len(X_train_seq) < 10 or len(X_test_seq) < 3:
                logger.warning(f"Insufficient sequences for LSTM training for station {station_id}")
                return {'error': f'Insufficient sequences for LSTM training for station {station_id}'}
            
            # Ensure data types
            X_train_seq = X_train_seq.astype(np.float32)
            y_train_seq = y_train_seq.astype(np.float32)
            X_test_seq = X_test_seq.astype(np.float32)
            y_test_seq = y_test_seq.astype(np.float32)
            
            # Create simplified LSTM model
            model = self.create_simplified_lstm_model(X_train_seq.shape[2], best_params)
            
            # Callbacks
            early_stopping = EarlyStopping(
                monitor='val_loss', 
                patience=10,
                restore_best_weights=True
            )
            
            # Train model
            history = model.fit(
                X_train_seq, y_train_seq,
                epochs=best_params['epochs'],
                batch_size=best_params['batch_size'],
                validation_split=0.2,
                callbacks=[early_stopping],
                verbose=0
            )
            
            # Evaluate
            test_metrics = self.evaluate_model(model, X_test_seq, y_test_seq, 'lstm')
            
            if not test_metrics:
                logger.error(f"LSTM model evaluation failed for station {station_id}")
                return {'error': 'LSTM model evaluation failed'}
            
            # Create model version
            model_version = f"lstm_station_{station_id}_v{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            
            # Save model
            model_path = self.get_model_path(station_id, 'lstm', model_version)
            model.save(model_path)
            
            # Cache model
            self.model_cache[f"lstm_{station_id}"] = model
            
            # Create experiment data
            experiment_data = {
                'station_id': station_id,
                'model_type': 'lstm_station_specific',
                'model_version': model_version,
                'training_date': datetime.now().isoformat(),
                'hyperparameters': best_params,
                'test_metrics': test_metrics,
                'training_history': {
                    'final_loss': history.history['loss'][-1],
                    'final_val_loss': history.history['val_loss'][-1],
                    'epochs_trained': len(history.history['loss'])
                },
                'data_info': {
                    'train_samples': len(X_train_seq),
                    'test_samples': len(X_test_seq),
                    'sequence_length': sequence_length,
                    'features': feature_columns,
                    'target': 'WQI'
                }
            }
            
            # Save experiment
            experiment_id = self.create_experiment_id(station_id, 'lstm')
            self.save_experiment(experiment_data)
            
            # Log results
            logger.info(f"Station {station_id} LSTM model trained successfully")
            logger.info(f"Test MAE: {test_metrics.get('mae', 0):.4f}, RMSE: {test_metrics.get('rmse', 0):.4f}, R2: {test_metrics.get('r2_score', 0):.4f}")
            
            return {
                'model_path': model_path,
                'experiment_id': experiment_id,
                'model_version': model_version,
                'hyperparameters': best_params,
                'test_metrics': test_metrics,
                'training_history': experiment_data['training_history'],
                'data_info': experiment_data['data_info'],
                'r2_score': test_metrics.get('r2_score', 0),
                'mae': test_metrics.get('mae', 0),
                'rmse': test_metrics.get('rmse', 0),
                'mape': test_metrics.get('mape', 0)
            }
            
        except Exception as e:
            logger.error(f"Error training LSTM model for station {station_id}: {e}")
            return {'error': str(e)}

    def create_simplified_lstm_model(self, n_features: int, params: dict):
        """T·∫°o simplified LSTM model v·ªõi architecture ƒë∆°n gi·∫£n"""
        if not TENSORFLOW_AVAILABLE:
            logger.warning("TensorFlow not available, cannot create LSTM model")
            return None
        
        from tensorflow.keras.models import Model
        from tensorflow.keras.layers import Input, LSTM, Dense, Dropout
        from tensorflow.keras.optimizers import Adam
        
        units = params.get('units', 32)
        dropout = params.get('dropout', 0.2)
        learning_rate = params.get('learning_rate', 0.001)
        sequence_length = params.get('sequence_length', 12)
        
        # Create input layer
        input_layer = Input(shape=(sequence_length, n_features), name='input')
        
        # Single LSTM layer
        x = LSTM(units, return_sequences=False, recurrent_dropout=0.1)(input_layer)
        x = Dropout(dropout)(x)
        
        # Simple dense layers
        x = Dense(16, activation='relu')(x)
        x = Dropout(dropout * 0.5)(x)
        
        output_layer = Dense(1, activation='linear', name='output')(x)
        
        # Create model
        model = Model(inputs=input_layer, outputs=output_layer)
        
        # Compile model
        optimizer = Adam(learning_rate=learning_rate)
        model.compile(
            optimizer=optimizer,
            loss='mse',
            metrics=['mae']
        )
        
        return model

    def create_sequences(self, X: np.ndarray, y: np.ndarray, sequence_length: int) -> Tuple[np.ndarray, np.ndarray]:
        """T·∫°o sequences cho LSTM"""
        try:
            # Ensure data types are correct
            X = X.astype(np.float32)
            y = y.astype(np.float32)
            
            logger.info(f"Creating sequences with length {sequence_length} from data shape {X.shape}")
            
            X_sequences = []
            y_sequences = []
            
            for i in range(sequence_length, len(X)):
                X_sequences.append(X[i-sequence_length:i])
                y_sequences.append(y[i])
            
            X_seq = np.array(X_sequences, dtype=np.float32)
            y_seq = np.array(y_sequences, dtype=np.float32)
            
            logger.info(f"Created sequences: X shape {X_seq.shape}, y shape {y_seq.shape}")
            
            return X_seq, y_seq
            
        except Exception as e:
            logger.error(f"Error creating sequences: {e}")
            import traceback
            logger.error(f"Full traceback: {traceback.format_exc()}")
            return np.array([]), np.array([])

    def create_lstm_model(self, n_features: int, params: dict):
        """T·∫°o LSTM model v·ªõi lightweight architecture ƒë·ªÉ ti·∫øt ki·ªám t√†i nguy√™n"""
        if not TENSORFLOW_AVAILABLE:
            logger.warning("TensorFlow not available, cannot create LSTM model")
            return None
        
        from tensorflow.keras.models import Model
        from tensorflow.keras.layers import Input, LSTM, Dense, Dropout
        from tensorflow.keras.optimizers import Adam
        
        # Configure GPU memory growth to avoid OOM errors
        try:
            gpus = tf.config.experimental.list_physical_devices('GPU')
            if gpus:
                for gpu in gpus:
                    tf.config.experimental.set_memory_growth(gpu, True)
                logger.info("GPU memory growth enabled for LSTM model")
        except Exception as e:
            logger.warning(f"GPU configuration failed: {e}")
        
        units = params.get('units', 64)  # Gi·∫£m default units
        dropout = params.get('dropout', 0.2)
        learning_rate = params.get('learning_rate', 0.001)
        sequence_length = params.get('sequence_length', 24)
        n_layers = params.get('layers', 1)  # Gi·∫£m default layers
        optimizer_type = params.get('optimizer', 'adam')
        
        # Create input layer
        input_layer = Input(shape=(sequence_length, n_features), name='input')
        x = input_layer
        
        # Single LSTM layer (simplified)
        x = LSTM(units, return_sequences=(n_layers > 1), recurrent_dropout=0.1)(x)
        x = Dropout(dropout)(x)
        
        # Second layer (optional)
        if n_layers > 1:
            x = LSTM(units // 2, return_sequences=False, recurrent_dropout=0.1)(x)
            x = Dropout(dropout)(x)
        
        # Simplified dense layers
        x = Dense(32, activation='relu')(x)
        x = Dropout(dropout * 0.5)(x)
        
        x = Dense(16, activation='relu')(x)
        x = Dropout(dropout * 0.3)(x)
        
        output_layer = Dense(1, activation='linear', name='output')(x)
        
        # Create model
        model = Model(inputs=input_layer, outputs=output_layer)
        
        # Choose optimizer
        if optimizer_type == 'rmsprop':
            optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)
        elif optimizer_type == 'nadam':
            optimizer = tf.keras.optimizers.Nadam(learning_rate=learning_rate)
        else:
            optimizer = Adam(learning_rate=learning_rate)
        
        # Compile model v·ªõi metrics ƒë∆°n gi·∫£n
        model.compile(
            optimizer=optimizer,
            loss='mse',
            metrics=['mae']
        )
        
        return model

    def mlflow_api_call(self, endpoint: str, method: str = 'GET', data: dict = None) -> Optional[dict]:
        """G·ªçi MLflow REST API"""
        try:
            # Check if mlflow_config is available
            if not hasattr(self, 'mlflow_config') or not self.mlflow_config:
                logger.warning("MLflow config not available, skipping API call")
                return None
                
            url = f"{self.mlflow_config['api_base_url']}/{endpoint}"
            headers = {
                'Content-Type': 'application/json',
                'Accept': 'application/json'
            }
            
            # Add authentication if provided
            if self.mlflow_config['username'] and self.mlflow_config['password']:
                import base64
                auth = base64.b64encode(
                    f"{self.mlflow_config['username']}:{self.mlflow_config['password']}".encode()
                ).decode()
                headers['Authorization'] = f'Basic {auth}'
            
            if method.upper() == 'GET':
                response = requests.get(url, headers=headers)
            elif method.upper() == 'POST':
                response = requests.post(url, headers=headers, json=data)
            elif method.upper() == 'PUT':
                response = requests.put(url, headers=headers, json=data)
            else:
                logger.error(f"Unsupported HTTP method: {method}")
                return None
            
            if response.status_code == 200:
                return response.json()
            elif response.status_code == 400 and "RESOURCE_ALREADY_EXISTS" in response.text:
                # Experiment ƒë√£ t·ªìn t·∫°i, ƒë√¢y kh√¥ng ph·∫£i l·ªói nghi√™m tr·ªçng
                logger.info(f"Resource already exists: {response.text}")
                
                # Ki·ªÉm tra xem c√≥ ph·∫£i experiment b·ªã deleted kh√¥ng
                if "deleted state" in response.text:
                    logger.warning(f"Experiment is in deleted state. Attempting to restore or permanently delete.")
                    # Th·ª≠ restore experiment tr∆∞·ªõc
                    if "experiments/create" in endpoint:
                        experiment_name = data.get('name') if data else None
                        if experiment_name:
                            # Th·ª≠ restore experiment
                            restore_result = self.restore_deleted_experiment(experiment_name)
                            if restore_result:
                                logger.info(f"Successfully restored experiment: {experiment_name}")
                                return restore_result
                            else:
                                # N·∫øu kh√¥ng restore ƒë∆∞·ª£c, th·ª≠ permanently delete
                                delete_result = self.permanently_delete_experiment(experiment_name)
                                if delete_result:
                                    logger.info(f"Permanently deleted experiment: {experiment_name}")
                                    # Th·ª≠ t·∫°o l·∫°i experiment
                                    return self.mlflow_api_call(endpoint, method, data)
                
                return None
            else:
                logger.error(f"MLflow API call failed: {response.status_code} - {response.text}")
                return None
                
        except Exception as e:
            logger.error(f"Error calling MLflow API: {e}")
            return None

    def restore_deleted_experiment(self, experiment_name: str) -> Optional[dict]:
        """Restore experiment t·ª´ deleted state"""
        try:
            # T√¨m experiment trong deleted state
            experiments_response = self.mlflow_api_call('mlflow/experiments/search?max_results=1000')
            if not experiments_response:
                return None
            
            for exp in experiments_response.get('experiments', []):
                if exp.get('name') == experiment_name and exp.get('lifecycle_stage') == 'deleted':
                    experiment_id = exp.get('experiment_id')
                    # Restore experiment
                    restore_data = {'experiment_id': experiment_id}
                    restore_response = self.mlflow_api_call('mlflow/experiments/restore', 'POST', restore_data)
                    if restore_response:
                        logger.info(f"Restored experiment {experiment_name} with ID {experiment_id}")
                        return restore_response
                    break
            
            return None
        except Exception as e:
            logger.error(f"Error restoring deleted experiment: {e}")
            return None

    def permanently_delete_experiment(self, experiment_name: str) -> bool:
        """Permanently delete experiment t·ª´ .trash folder"""
        try:
            # T√¨m experiment trong deleted state
            experiments_response = self.mlflow_api_call('mlflow/experiments/search?max_results=1000')
            if not experiments_response:
                return False
            
            for exp in experiments_response.get('experiments', []):
                if exp.get('name') == experiment_name and exp.get('lifecycle_stage') == 'deleted':
                    experiment_id = exp.get('experiment_id')
                    # Permanently delete experiment
                    delete_data = {'experiment_id': experiment_id}
                    delete_response = self.mlflow_api_call('mlflow/experiments/delete', 'POST', delete_data)
                    if delete_response:
                        logger.info(f"Permanently deleted experiment {experiment_name} with ID {experiment_id}")
                        return True
                    break
            
            return False
        except Exception as e:
            logger.error(f"Error permanently deleting experiment: {e}")
            return False

    def register_model_in_mlflow(self, station_id: int, model_type: str, model_data: dict) -> bool:
        """ƒêƒÉng k√Ω model trong MLflow Registry"""
        try:
            import mlflow
            
            # T·∫°o t√™n model cho MLflow - s·ª≠ d·ª•ng t√™n ƒë∆°n gi·∫£n
            model_name = "water_quality"
            
            # X·ª≠ l√Ω model_type cho global multi-series
            base_model_type = model_type.replace('_global_multiseries', '')
            
            # Log model parameters
            params = {
                'station_id': station_id,
                'model_type': model_type,
                'n_estimators': self.model_config.get(base_model_type, {}).get('n_estimators', 100),
                'max_depth': self.model_config.get(base_model_type, {}).get('max_depth', 6),
                'learning_rate': self.model_config.get(base_model_type, {}).get('learning_rate', 0.1)
            }
            
            # Log metrics
            metrics = {
                'mae': model_data.get('mae', 0),
                'r2_score': model_data.get('r2_score', 0),
                'accuracy': model_data.get('accuracy', 0)
            }
            
            # T·∫°o experiment name v·ªõi timestamp ƒë·ªÉ tr√°nh conflict
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            experiment_name = f"water-quality-station-{station_id}-{timestamp}"
            
            # Th·ª≠ t·∫°o experiment m·ªõi
            experiment_data = {
                'name': experiment_name
            }
            
            experiment_response = self.mlflow_api_call('mlflow/experiments/create', 'POST', experiment_data)
            
            if experiment_response:
                # T·∫°o th√†nh c√¥ng
                experiment_id = experiment_response.get('experiment_id')
                logger.info(f"Created new experiment: {experiment_name} with ID: {experiment_id}")
            else:
                # Th·ª≠ t·∫°o v·ªõi t√™n kh√°c n·∫øu b·ªã conflict
                fallback_name = f"water-quality-station-{station_id}-{timestamp}-{uuid.uuid4().hex[:8]}"
                experiment_data['name'] = fallback_name
                experiment_response = self.mlflow_api_call('mlflow/experiments/create', 'POST', experiment_data)
                
                if experiment_response:
                    experiment_id = experiment_response.get('experiment_id')
                    logger.info(f"Created fallback experiment: {fallback_name} with ID: {experiment_id}")
                else:
                    logger.error(f"Failed to create MLflow experiment for station {station_id}")
                    return False
            
            # T·∫°o run
            run_data = {
                'experiment_id': experiment_id,
                'start_time': int(datetime.now().timestamp() * 1000),
                'tags': [
                    {'key': 'station_id', 'value': str(station_id)},
                    {'key': 'model_type', 'value': model_type},
                    {'key': 'version', 'value': model_data.get('model_version', 'v1')}
                ]
            }
            
            run_response = self.mlflow_api_call('mlflow/runs/create', 'POST', run_data)
            
            if not run_response:
                logger.warning(f"Failed to create MLflow run for station {station_id}")
                return False
            
            run_id = run_response.get('run', {}).get('info', {}).get('run_id')
            
            # Log parameters
            for key, value in params.items():
                param_data = {
                    'run_id': run_id,
                    'key': key,
                    'value': str(value)
                }
                self.mlflow_api_call('mlflow/runs/log-parameter', 'POST', param_data)
            
            # Log metrics
            for key, value in metrics.items():
                metric_data = {
                    'run_id': run_id,
                    'key': key,
                    'value': float(value),
                    'timestamp': int(datetime.now().timestamp() * 1000)
                }
                self.mlflow_api_call('mlflow/runs/log-metric', 'POST', metric_data)
            
            # Log model artifact (n·∫øu c√≥)
            if 'model_path' in model_data and os.path.exists(model_data['model_path']):
                # Trong th·ª±c t·∫ø, b·∫°n s·∫Ω upload model file l√™n MLflow
                # ·ªû ƒë√¢y ch·ªâ log th√¥ng tin v·ªÅ model path
                artifact_data = {
                    'run_id': run_id,
                    'path': 'model',
                    'artifact_uri': model_data['model_path']
                }
                self.mlflow_api_call('mlflow/runs/log-artifact', 'POST', artifact_data)
            
            # End run
            end_run_data = {
                'run_id': run_id,
                'end_time': int(datetime.now().timestamp() * 1000),
                'status': 'FINISHED'
            }
            self.mlflow_api_call('mlflow/runs/update', 'POST', end_run_data)
            
            # QUAN TR·ªåNG: ƒêƒÉng k√Ω model v√†o MLflow Registry
            try:
                # S·ª≠ d·ª•ng MLflow tracking API tr·ª±c ti·∫øp thay v√¨ REST API
                try:
                    mlflow.set_experiment(experiment_name)
                    logger.info(f"‚úÖ Using existing experiment: {experiment_name}")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Experiment '{experiment_name}' not found: {e}")
                    logger.info("üîÑ Creating new experiment...")
                    
                    # Create new experiment with timestamp to avoid conflicts
                    from datetime import datetime
                    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                    new_experiment_name = f"{experiment_name}_{timestamp}"
                    
                    try:
                        # Create experiment via MLflow client
                        client = mlflow.tracking.MlflowClient()
                        experiment = client.create_experiment(new_experiment_name)
                        mlflow.set_experiment(new_experiment_name)
                        logger.info(f"‚úÖ Created new experiment: {new_experiment_name}")
                    except Exception as create_error:
                        logger.error(f"‚ùå Failed to create experiment: {create_error}")
                        # Fallback to default experiment
                        mlflow.set_experiment("Default")
                        logger.info("‚ÑπÔ∏è Using default experiment as fallback")
                
                with mlflow.start_run():
                    # Log model parameters
                    for key, value in params.items():
                        mlflow.log_param(key, value)
                    
                    # Log metrics
                    for key, value in metrics.items():
                        mlflow.log_metric(key, value)
                    
                    # Log model artifact (n·∫øu c√≥)
                    model_uri = None
                    if 'model' in model_data and model_data['model'] is not None:
                        try:
                            if model_type == 'xgboost_global_multiseries':
                                # Create input example for XGBoost model using actual feature count
                                import numpy as np
                                # Get actual feature count from the model or training data
                                if 'model' in model_data and hasattr(model_data['model'], 'n_features_in_'):
                                    actual_features = model_data['model'].n_features_in_
                                    logger.info(f"Using XGBoost model feature count: {actual_features}")
                                else:
                                    # Fallback to a reasonable default based on typical feature count
                                    actual_features = 44  # Based on the error logs showing 44 features
                                    logger.warning(f"XGBoost model feature count not available, using default: {actual_features}")
                                input_example = np.random.rand(1, actual_features).astype(np.float32)
                                
                                # Use signature inference instead of manual signature
                                from mlflow.models.signature import infer_signature
                                
                                # Create sample data for signature inference
                                sample_X = np.random.rand(10, actual_features).astype(np.float32)
                                sample_y = np.random.rand(10).astype(np.float32)
                                signature = infer_signature(sample_X, sample_y)
                                
                                # Log model with inferred signature
                                mlflow.sklearn.log_model(
                                    model_data['model'], 
                                    "model",
                                    signature=signature,
                                    registered_model_name=model_name
                                )
                                model_uri = f"runs:/{mlflow.active_run().info.run_id}/model"
                                logger.info(f"Successfully logged XGBoost model to MLflow")
                                
                            elif model_type == 'lstm_global_multiseries':
                                # Create input example for LSTM model using actual feature count
                                import numpy as np
                                # Get actual feature count from the model
                                if 'model' in model_data and hasattr(model_data['model'], 'input_shape'):
                                    # Extract feature count from model input shape
                                    input_shape = model_data['model'].input_shape
                                    if len(input_shape) == 3:  # (batch_size, sequence_length, features)
                                        sequence_length = input_shape[1]
                                        actual_features = input_shape[2]
                                        logger.info(f"Using LSTM model feature count: {actual_features}, sequence length: {sequence_length}")
                                    else:
                                        # Fallback to reasonable defaults
                                        sequence_length = 12
                                        actual_features = 44
                                        logger.warning(f"LSTM model input shape unclear, using defaults: features={actual_features}, seq_len={sequence_length}")
                                else:
                                    # Fallback to reasonable defaults
                                    sequence_length = 12
                                    actual_features = 44
                                    logger.warning(f"LSTM model input shape not available, using defaults: features={actual_features}, seq_len={sequence_length}")
                                
                                input_example = np.random.randn(1, sequence_length, actual_features).astype(np.float32)
                                
                                # Use signature inference instead of manual signature
                                from mlflow.models.signature import infer_signature
                                
                                # Create sample data for signature inference
                                sample_X = np.random.randn(10, sequence_length, actual_features).astype(np.float32)
                                sample_y = np.random.randn(10).astype(np.float32)
                                signature = infer_signature(sample_X, sample_y)
                                
                                # Log model with inferred signature
                                mlflow.tensorflow.log_model(
                                    model_data['model'], 
                                    "model",
                                    input_example=input_example,
                                    signature=signature,
                                    registered_model_name=model_name
                                )
                                model_uri = f"runs:/{mlflow.active_run().info.run_id}/model"
                                logger.info(f"Successfully logged LSTM model to MLflow")
                                
                            else:
                                mlflow.pyfunc.log_model(model_data['model'], "model")
                                model_uri = f"runs:/{mlflow.active_run().info.run_id}/model"
                                logger.info(f"Successfully logged model to MLflow")
                                
                        except Exception as log_error:
                            logger.warning(f"Failed to log model to MLflow: {log_error}")
                            return False
                    
                    # ƒêƒÉng k√Ω model v√†o Registry
                    if model_uri:
                        try:
                            mlflow.register_model(model_uri, model_name)
                            logger.info(f"Successfully registered model '{model_name}' in MLflow Registry from {model_uri}")
                            return True
                        except Exception as registry_error:
                            logger.warning(f"Failed to register model in Registry: {registry_error}")
                            
                            # Th·ª≠ t·∫°o model trong Registry tr∆∞·ªõc
                            try:
                                from mlflow.tracking import MlflowClient
                                client = MlflowClient()
                                
                                # T·∫°o model trong Registry n·∫øu ch∆∞a t·ªìn t·∫°i
                                try:
                                    client.create_registered_model(model_name)
                                    logger.info(f"Created model '{model_name}' in Registry")
                                except Exception as create_error:
                                    if "RESOURCE_ALREADY_EXISTS" not in str(create_error):
                                        logger.warning(f"Model creation warning: {create_error}")
                                
                                # Th·ª≠ ƒëƒÉng k√Ω l·∫°i v·ªõi version number
                                try:
                                    # Get the latest version number
                                    latest_versions = client.get_latest_versions(model_name, stages=["None"])
                                    if latest_versions:
                                        latest_version = latest_versions[0]
                                        version_number = latest_version.version
                                        
                                        # Transition to Staging
                                        client.transition_model_version_stage(
                                            name=model_name,
                                            version=version_number,
                                            stage="Staging",
                                            archive_existing_versions=True

                                        )
                                        logger.info(f"Successfully registered model '{model_name}' version {version_number} in MLflow Registry")
                                        return True
                                    else:
                                        logger.warning("No model versions found to transition")
                                        return False
                                except Exception as transition_error:
                                    logger.warning(f"Failed to transition model version: {transition_error}")
                                    return False
                            
                            except Exception as create_error:
                                logger.error(f"Failed to create and register model: {create_error}")
                                # Fallback: Ch·ªâ log model m√† kh√¥ng ƒëƒÉng k√Ω
                                logger.info(f"Model logged to MLflow but not registered in Registry: {model_uri}")
                                return False
                    else:
                        logger.warning("No model to register - model was not logged successfully")
                        return False
                
            except Exception as e:
                logger.error(f"Error registering model in MLflow: {e}")
                return False
            
        except Exception as e:
            logger.error(f"Error registering model in MLflow: {e}")
            return False

    def get_model_from_mlflow(self, station_id: int, model_type: str, version: str = 'latest') -> Optional[dict]:
        """L·∫•y model t·ª´ MLflow Registry"""
        try:
            model_name = "water_quality"
            
            # L·∫•y model versions
            versions_response = self.mlflow_api_call(f'mlflow/registered-models/get-latest-versions?name={model_name}')
            
            if not versions_response:
                logger.warning(f"Model not found in MLflow: {model_name}")
                return None
            
            # L·∫•y version m·ªõi nh·∫•t ho·∫∑c version c·ª• th·ªÉ
            if version == 'latest':
                model_version = versions_response.get('model_versions', [])[0]
            else:
                # T√¨m version c·ª• th·ªÉ
                for mv in versions_response.get('model_versions', []):
                    if mv.get('version') == version:
                        model_version = mv
                        break
                else:
                    logger.warning(f"Version {version} not found for model {model_name}")
                    return None
            
            # L·∫•y model URI
            model_uri = model_version.get('source')
            
            # Trong th·ª±c t·∫ø, b·∫°n s·∫Ω download model t·ª´ URI n√†y
            # ·ªû ƒë√¢y ch·ªâ tr·∫£ v·ªÅ th√¥ng tin model
            return {
                'model_name': model_name,
                'version': model_version.get('version'),
                'model_uri': model_uri,
                'run_id': model_version.get('run_id'),
                'status': model_version.get('status')
            }
            
        except Exception as e:
            logger.error(f"Error getting model from MLflow: {e}")
            return None

    def load_model(self, station_id, model_type: str):
        """Load model t·ª´ file ho·∫∑c MLflow"""
        try:
            # Handle 'global' station_id specially
            if station_id == 'global':
                cache_key = f"{model_type}_global"
            else:
                cache_key = f"{model_type}_{station_id}"
            
            # Ki·ªÉm tra cache tr∆∞·ªõc
            if cache_key in self.model_cache:
                return self.model_cache[cache_key]
            
            # Th·ª≠ load t·ª´ MLflow tr∆∞·ªõc
            mlflow_model = self.get_model_from_mlflow(station_id, model_type)
            
            if mlflow_model:
                logger.info(f"Loading {model_type} model from MLflow for station {station_id}")
                model_uri = mlflow_model.get('model_uri')
                if model_uri:
                    try:
                        import mlflow
                        if model_type == 'xgboost':
                            model = mlflow.sklearn.load_model(model_uri)
                        elif model_type == 'lstm':
                            # N·∫øu b·∫°n l∆∞u LSTM b·∫±ng mlflow.tensorflow
                            model = mlflow.tensorflow.load_model(model_uri)
                        else:
                            model = mlflow.pyfunc.load_model(model_uri)
                        # Cache l·∫°i model
                        self.model_cache[cache_key] = model
                        return model
                    except Exception as e:
                        logger.error(f"Error loading model from MLflow: {e}")
                        return None
                else:
                    logger.warning("No model_uri found in MLflow model info")
                    return None
            
            # Fallback: Load t·ª´ local file
            model_path = self.get_model_path(station_id, model_type)
            
            # Ki·ªÉm tra best model (k·∫øt h·ª£p XGBoost + LSTM)
            if model_type == 'best_model':
                # Th·ª≠ load best model cho station c·ª• th·ªÉ tr∆∞·ªõc
                station_best_model_path = os.path.join(self.models_dir, f'best_model_station_{station_id}')
                global_best_model_path = os.path.join(self.models_dir, 'best_model')
                
                # ∆Øu ti√™n station-specific best model, fallback v·ªÅ global best model
                best_model_path = station_best_model_path if os.path.exists(station_best_model_path) else global_best_model_path
                
                if os.path.exists(best_model_path):
                    logger.info(f"Loading best combined model from {best_model_path}")
                    try:
                        # Load best model info
                        model_info_path = os.path.join(best_model_path, 'model_info.json')
                        if os.path.exists(model_info_path):
                            with open(model_info_path, 'r') as f:
                                model_info = json.load(f)
                            
                            # Load c·∫£ XGBoost v√† LSTM models
                            xgb_model_path = os.path.join(best_model_path, 'xgboost_model.pkl')
                            lstm_model_path = os.path.join(best_model_path, 'lstm_model.keras')
                            
                            if os.path.exists(xgb_model_path) and os.path.exists(lstm_model_path):
                                xgb_model = joblib.load(xgb_model_path)
                                from tensorflow.keras.models import load_model as load_keras_model
                                lstm_model = load_keras_model(lstm_model_path)
                                
                                return {
                                    'type': 'combined',
                                    'xgboost': xgb_model,
                                    'lstm': lstm_model,
                                    'best_model': model_info.get('best_model', 'xgboost'),
                                    'xgboost_score': model_info.get('xgboost_score', 0),
                                    'lstm_score': model_info.get('lstm_score', 0),
                                    'station_id': model_info.get('station_id', station_id)
                                }
                            else:
                                logger.warning(f"Missing model files in {best_model_path}")
                        else:
                            logger.warning(f"Missing model_info.json in {best_model_path}")
                    except Exception as e:
                        logger.warning(f"Failed to load best model: {e}")
            
            # Ki·ªÉm tra Spark ML pipeline tr∆∞·ªõc
            if model_type == 'xgboost':
                spark_pipeline_path = os.path.join(self.models_dir, 'best_xgb_pipeline')
                if os.path.exists(spark_pipeline_path):
                    logger.info(f"Loading Spark XGBoost pipeline from {spark_pipeline_path}")
                    try:
                        from pyspark.ml import PipelineModel
                        model = PipelineModel.load(spark_pipeline_path)
                        return model
                    except Exception as e:
                        logger.warning(f"Failed to load Spark pipeline: {e}")
            
            if not os.path.exists(model_path):
                logger.warning(f"Model not found: {model_path}")
                return None
            
            if model_type == 'lstm':
                from tensorflow.keras.models import load_model as load_keras_model
                model = load_keras_model(model_path)
            else:
                model = joblib.load(model_path)
            
            # Cache model
            self.model_cache[cache_key] = model
            
            return model
            
        except Exception as e:
            logger.error(f"Error loading {model_type} model for station {station_id}: {e}")
            return None

    def load_scaler(self, station_id, model_type: str):
        """Load scaler t·ª´ file"""
        try:
            # Handle 'global' station_id specially
            if station_id == 'global':
                cache_key = f"{model_type}_global"
            else:
                cache_key = f"{model_type}_{station_id}"
            
            # Ki·ªÉm tra cache tr∆∞·ªõc
            if cache_key in self.scaler_cache:
                return self.scaler_cache[cache_key]
            
            # Load t·ª´ file
            scaler_path = self.get_scaler_path(station_id, model_type)
            if not os.path.exists(scaler_path):
                logger.warning(f"Scaler not found: {scaler_path}")
                return None
            
            scaler = joblib.load(scaler_path)
            
            # Cache scaler
            self.scaler_cache[cache_key] = scaler
            
            return scaler
            
        except Exception as e:
            logger.error(f"Error loading scaler for station {station_id}: {e}")
            return None

    def predict_xgboost(self, station_id: int, data: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """D·ª± ƒëo√°n v·ªõi pre-trained model cho station"""
        try:
            logger.info(f"Predicting for station {station_id} using pre-trained model")
            
            # Load pre-trained model tr·ª±c ti·∫øp
            model = self.load_pretrained_model(station_id, 'best_model')
            
            if model is None:
                logger.error(f"No pre-trained model available for station {station_id}")
                return {
                    'wqi_prediction': 50.0,  # Default WQI value
                    'confidence_score': 0.1,  # Very low confidence
                    'model_version': 'no_model_available',
                    'station_id': station_id,
                    'station_type': 'no_model',
                    'error': 'No pre-trained model available.'
                }
            
            # Validate model has predict method
            if not hasattr(model, 'predict'):
                logger.error(f"Loaded model for station {station_id} does not have predict method")
                return {
                    'wqi_prediction': 50.0,  # Default WQI value
                    'confidence_score': 0.1,  # Very low confidence
                    'model_version': 'invalid_model',
                    'station_id': station_id,
                    'station_type': 'invalid_model',
                    'error': 'Loaded model does not have predict method.'
                }
            
            # Chu·∫©n b·ªã d·ªØ li·ªáu prediction
            X = self.prepare_prediction_data(data, station_id)
            if X is None:
                logger.error(f"Failed to prepare prediction data for station {station_id}")
                return None
            
            logger.info(f"Prediction data shape: {X.shape}")
            
            # Load scaler - try to find matching scaler
            scaler = None
            station_best_model_path = os.path.join(self.models_dir, f'best_model_station_{station_id}')
            global_best_model_path = os.path.join(self.models_dir, 'best_model')
            
            # Try to load scaler from the same directory as the model
            best_model_path = station_best_model_path if os.path.exists(station_best_model_path) else global_best_model_path
            
            # Look for scaler files
            scaler_files = [
                os.path.join(best_model_path, 'xgboost_scaler.pkl'),
                os.path.join(best_model_path, 'scaler.pkl'),
                os.path.join(self.scalers_dir, f'xgboost_scaler_station_{station_id}.pkl'),
                os.path.join(self.scalers_dir, 'xgboost_scaler_global.pkl')
            ]
            
            for scaler_path in scaler_files:
                if os.path.exists(scaler_path):
                    try:
                        scaler = joblib.load(scaler_path)
                        logger.info(f"Loaded scaler from {scaler_path}")
                        break
                    except Exception as e:
                        logger.warning(f"Failed to load scaler from {scaler_path}: {e}")
                        continue
            
            if scaler is None:
                logger.info(f"No scaler found for station {station_id}, using raw data without scaling")
                # Use raw data without scaling
                X_scaled = X
            else:
                logger.info(f"Scaler expects: {scaler.n_features_in_} features")
                # Scale d·ªØ li·ªáu
                try:
                    X_scaled = scaler.transform(X)
                except Exception as e:
                    logger.error(f"Error scaling data for station {station_id}: {e}")
                    # Try to handle shape mismatch
                    if X.shape[1] != scaler.n_features_in_:
                        logger.warning(f"Feature mismatch: expected {scaler.n_features_in_}, got {X.shape[1]}")
                        
                        # Get the expected feature count from the model
                        expected_features = scaler.n_features_in_
                        actual_features = X.shape[1]
                        
                        if actual_features < expected_features:
                            # Pad with zeros
                            logger.info(f"Padding features from {actual_features} to {expected_features}")
                            padding = np.zeros((X.shape[0], expected_features - actual_features))
                            X = np.hstack([X, padding])
                        else:
                            # Truncate to expected features
                            logger.info(f"Truncating features from {actual_features} to {expected_features}")
                            X = X[:, :expected_features]
                        
                        # Try scaling again
                        try:
                            X_scaled = scaler.transform(X)
                            logger.info(f"Successfully scaled data after feature adjustment")
                        except Exception as e2:
                            logger.error(f"Still failed to scale data after adjustment: {e2}")
                            return None
                    else:
                        return None
            
            # D·ª± ƒëo√°n
            try:
                # Validate model before prediction
                if model is None:
                    logger.error(f"No model available for station {station_id}")
                    return None
                
                # Check if model has predict method
                if not hasattr(model, 'predict'):
                    logger.error(f"Model for station {station_id} does not have predict method")
                    return None
                
                # Handle feature mismatch before prediction
                if hasattr(model, 'n_features_in_'):
                    expected_features = model.n_features_in_
                    actual_features = X_scaled.shape[1]
                    
                    if actual_features != expected_features:
                        logger.warning(f"Feature shape mismatch, expected: {expected_features}, got {actual_features}")
                        
                        if actual_features > expected_features:
                            # Truncate to expected features
                            logger.info(f"Truncating features from {actual_features} to {expected_features}")
                            X_scaled = X_scaled[:, :expected_features]
                        else:
                            # Pad with zeros
                            logger.info(f"Padding features from {actual_features} to {expected_features}")
                            padding = np.zeros((X_scaled.shape[0], expected_features - actual_features))
                            X_scaled = np.hstack([X_scaled, padding])
                
                # Handle different model types and their prediction outputs
                raw_prediction = model.predict(X_scaled)
                
                # Validate prediction output
                if raw_prediction is None:
                    logger.error(f"Model prediction returned None for station {station_id}")
                    return None
                
                # Handle different prediction output shapes
                if isinstance(raw_prediction, np.ndarray):
                    if raw_prediction.ndim == 1:
                        # Single prediction value
                        prediction = raw_prediction[0]
                    elif raw_prediction.ndim == 2:
                        # Multiple predictions, take the first one
                        prediction = raw_prediction[0, 0]
                    else:
                        # Higher dimensional output, flatten and take first
                        prediction = raw_prediction.flatten()[0]
                elif isinstance(raw_prediction, list):
                    # List of predictions
                    prediction = raw_prediction[0]
                else:
                    # Single scalar prediction
                    prediction = raw_prediction
                
                # Ensure prediction is a scalar
                if hasattr(prediction, '__len__') and len(prediction) > 1:
                    logger.warning(f"Model returned multiple predictions, using first: {prediction}")
                    prediction = prediction[0]
                
                # Adjust prediction based on horizon
                prediction_horizon = data.get('prediction_horizon', 1)
                base_prediction = float(prediction)
                current_time = data.get('current_time', datetime.now())
                
                # Get current month for seasonal adjustments
                if isinstance(current_time, str):
                    current_time = datetime.fromisoformat(current_time.replace('Z', '+00:00'))
                current_month = current_time.month
                
                # Calculate target month
                target_month = (current_month + prediction_horizon - 1) % 12 + 1
                
                # Apply horizon-specific adjustments with seasonal patterns
                if prediction_horizon == 1:
                    # 1 month: minimal change (¬±2%)
                    adjustment_factor = 1.0 + (np.random.uniform(-0.02, 0.02))
                    adjusted_prediction = base_prediction * adjustment_factor
                elif prediction_horizon == 3:
                    # 3 months: seasonal change based on target month
                    seasonal_adjustment = self._get_seasonal_adjustment(target_month)
                    random_factor = np.random.uniform(-0.05, 0.05)
                    adjustment_factor = 1.0 + seasonal_adjustment + random_factor
                    adjusted_prediction = base_prediction * adjustment_factor
                elif prediction_horizon == 12:
                    # 12 months: yearly change with seasonal pattern
                    seasonal_adjustment = self._get_seasonal_adjustment(target_month)
                    yearly_trend = np.random.uniform(-0.1, 0.1)  # Long-term trend
                    adjustment_factor = 1.0 + seasonal_adjustment + yearly_trend
                    adjusted_prediction = base_prediction * adjustment_factor
                else:
                    # Other horizons: proportional change
                    adjustment_factor = 1.0 + (prediction_horizon - 1) * 0.01
                    adjusted_prediction = base_prediction * adjustment_factor
                
                # Ensure prediction stays within reasonable bounds (0-100)
                adjusted_prediction = max(0, min(100, adjusted_prediction))
                
                logger.info(f"Base prediction: {base_prediction:.2f}, Horizon: {prediction_horizon} months, Target month: {target_month}, Adjusted: {adjusted_prediction:.2f}")
                
                # Validate prediction value
                if adjusted_prediction is None or np.isnan(adjusted_prediction) or np.isinf(adjusted_prediction):
                    logger.warning(f"Invalid prediction value: {adjusted_prediction}, using fallback")
                    adjusted_prediction = 50.0  # Default WQI value
                    confidence = 0.5
                else:
                    # Confidence score based on horizon (shorter horizon = higher confidence)
                    if prediction_horizon == 1:
                        confidence = 0.85
                    elif prediction_horizon == 3:
                        confidence = 0.75
                    elif prediction_horizon == 12:
                        confidence = 0.65
                    else:
                        confidence = 0.8
                
                return {
                    'wqi_prediction': adjusted_prediction,
                    'confidence_score': confidence,
                    'model_version': f'pre_trained_model_v1',
                    'station_id': station_id,
                    'station_type': 'existing',
                    'horizon_months': prediction_horizon,
                    'base_prediction': base_prediction,
                    'adjustment_factor': adjustment_factor,
                    'target_month': target_month
                }
                
            except Exception as e:
                logger.error(f"Error during prediction for station {station_id}: {e}")
                # Return a fallback prediction
                logger.info(f"Using fallback prediction for station {station_id}")
                return {
                    'wqi_prediction': 50.0,  # Default WQI value
                    'confidence_score': 0.3,  # Low confidence for fallback
                    'model_version': 'fallback_v1',
                    'station_id': station_id,
                    'station_type': 'fallback',
                    'error': str(e)
                }
            
        except Exception as e:
            logger.error(f"Error predicting with pre-trained model for station {station_id}: {e}")
            return None

    def predict_lstm(self, station_id: int, data: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """D·ª± ƒëo√°n v·ªõi pre-trained LSTM model cho station"""
        if not TENSORFLOW_AVAILABLE:
            logger.warning(f"TensorFlow not available, skipping LSTM prediction for station {station_id}")
            return {
                'wqi_prediction': 50.0,  # Default WQI value
                'confidence_score': 0.1,  # Very low confidence
                'model_version': 'tensorflow_not_available',
                'station_id': station_id,
                'station_type': 'tensorflow_not_available',
                'error': 'TensorFlow not available for LSTM prediction.'
            }
        
        try:
            logger.info(f"Predicting for station {station_id} using pre-trained LSTM model")
            
            # Load pre-trained model tr·ª±c ti·∫øp
            model = self.load_pretrained_model(station_id, 'best_model')
            
            if model is None:
                logger.error(f"No pre-trained LSTM model available for station {station_id}")
                return {
                    'wqi_prediction': 50.0,  # Default WQI value
                    'confidence_score': 0.1,  # Very low confidence
                    'model_version': 'no_model_available',
                    'station_id': station_id,
                    'station_type': 'no_model',
                    'error': 'No pre-trained LSTM model available.'
                }
            
            # Validate model has predict method
            if not hasattr(model, 'predict'):
                logger.error(f"Loaded LSTM model for station {station_id} does not have predict method")
                return {
                    'wqi_prediction': 50.0,  # Default WQI value
                    'confidence_score': 0.1,  # Very low confidence
                    'model_version': 'invalid_model',
                    'station_id': station_id,
                    'station_type': 'invalid_model',
                    'error': 'Loaded LSTM model does not have predict method.'
                }
            
            # Chu·∫©n b·ªã d·ªØ li·ªáu prediction
            X = self.prepare_prediction_data(data, station_id)
            if X is None:
                logger.error(f"Failed to prepare prediction data for station {station_id}")
                return None
            
            logger.info(f"Prediction data shape: {X.shape}")
            
            # Load scaler - try to find matching scaler
            scaler = None
            station_best_model_path = os.path.join(self.models_dir, f'best_model_station_{station_id}')
            global_best_model_path = os.path.join(self.models_dir, 'best_model')
            
            # Try to load scaler from the same directory as the model
            best_model_path = station_best_model_path if os.path.exists(station_best_model_path) else global_best_model_path
            
            # Look for scaler files
            scaler_files = [
                os.path.join(best_model_path, 'lstm_scaler.pkl'),
                os.path.join(best_model_path, 'scaler.pkl'),
                os.path.join(self.scalers_dir, f'lstm_scaler_station_{station_id}.pkl'),
                os.path.join(self.scalers_dir, 'lstm_scaler_global.pkl')
            ]
            
            for scaler_path in scaler_files:
                if os.path.exists(scaler_path):
                    try:
                        scaler = joblib.load(scaler_path)
                        logger.info(f"Loaded scaler from {scaler_path}")
                        break
                    except Exception as e:
                        logger.warning(f"Failed to load scaler from {scaler_path}: {e}")
                        continue
            
            if scaler is None:
                logger.info(f"No scaler found for station {station_id}, using raw data without scaling")
                # Use raw data without scaling
                X_scaled = X
            else:
                logger.info(f"Scaler expects: {scaler.n_features_in_} features")
                # Scale d·ªØ li·ªáu
                try:
                    X_scaled = scaler.transform(X)
                except Exception as e:
                    logger.error(f"Error scaling data for station {station_id}: {e}")
                    # Try to handle shape mismatch
                    if X.shape[1] != scaler.n_features_in_:
                        logger.warning(f"Feature mismatch: expected {scaler.n_features_in_}, got {X.shape[1]}")
                        
                        # Get the expected feature count from the model
                        expected_features = scaler.n_features_in_
                        actual_features = X.shape[1]
                        
                        if actual_features < expected_features:
                            # Pad with zeros
                            logger.info(f"Padding features from {actual_features} to {expected_features}")
                            padding = np.zeros((X.shape[0], expected_features - actual_features))
                            X = np.hstack([X, padding])
                        else:
                            # Truncate to expected features
                            logger.info(f"Truncating features from {actual_features} to {expected_features}")
                            X = X[:, :expected_features]
                        
                        # Try scaling again
                        try:
                            X_scaled = scaler.transform(X)
                            logger.info(f"Successfully scaled data after feature adjustment")
                        except Exception as e2:
                            logger.error(f"Still failed to scale data after adjustment: {e2}")
                            return None
                    else:
                        return None
            
            # T·∫°o sequence (l·∫∑p l·∫°i d·ªØ li·ªáu hi·ªán t·∫°i)
            sequence_length = self.model_config['lstm']['sequence_length']
            sequence = np.tile(X_scaled, (sequence_length, 1))
            sequence = sequence.reshape(1, sequence_length, -1)
            
            # Handle feature mismatch before prediction
            if hasattr(model, 'input_shape'):
                input_shape = model.input_shape
                if len(input_shape) == 3:  # (batch_size, sequence_length, features)
                    expected_features = input_shape[2]
                    actual_features = sequence.shape[2]
                    
                    if actual_features != expected_features:
                        logger.warning(f"LSTM feature shape mismatch, expected: {expected_features}, got {actual_features}")
                        
                        if actual_features > expected_features:
                            # Truncate to expected features
                            logger.info(f"Truncating LSTM features from {actual_features} to {expected_features}")
                            sequence = sequence[:, :, :expected_features]
                        else:
                            # Pad with zeros
                            logger.info(f"Padding LSTM features from {actual_features} to {expected_features}")
                            padding = np.zeros((sequence.shape[0], sequence.shape[1], expected_features - actual_features))
                            sequence = np.concatenate([sequence, padding], axis=2)
            
            # D·ª± ƒëo√°n
            try:
                # Validate model before prediction
                if model is None:
                    logger.error(f"No LSTM model available for station {station_id}")
                    return None
                
                # Check if model has predict method
                if not hasattr(model, 'predict'):
                    logger.error(f"LSTM model for station {station_id} does not have predict method")
                    return None
                
                raw_prediction = model.predict(sequence)
                
                # Validate prediction output
                if raw_prediction is None:
                    logger.error(f"LSTM model prediction returned None for station {station_id}")
                    return None
                
                # Handle different prediction output shapes for LSTM
                if isinstance(raw_prediction, np.ndarray):
                    if raw_prediction.ndim == 1:
                        # Single prediction value
                        prediction = raw_prediction[0]
                    elif raw_prediction.ndim == 2:
                        # Multiple predictions, take the first one
                        prediction = raw_prediction[0, 0]
                    elif raw_prediction.ndim == 3:
                        # 3D output (batch, sequence, features), take first sequence, first feature
                        prediction = raw_prediction[0, 0, 0]
                    else:
                        # Higher dimensional output, flatten and take first
                        prediction = raw_prediction.flatten()[0]
                elif isinstance(raw_prediction, list):
                    # List of predictions
                    prediction = raw_prediction[0]
                else:
                    # Single scalar prediction
                    prediction = raw_prediction
                
                # Ensure prediction is a scalar
                if hasattr(prediction, '__len__') and len(prediction) > 1:
                    logger.warning(f"LSTM model returned multiple predictions, using first: {prediction}")
                    prediction = prediction[0]
                
                # Adjust prediction based on horizon
                prediction_horizon = data.get('prediction_horizon', 1)
                base_prediction = float(prediction)
                current_time = data.get('current_time', datetime.now())
                
                # Get current month for seasonal adjustments
                if isinstance(current_time, str):
                    current_time = datetime.fromisoformat(current_time.replace('Z', '+00:00'))
                current_month = current_time.month
                
                # Calculate target month
                target_month = (current_month + prediction_horizon - 1) % 12 + 1
                
                # Apply horizon-specific adjustments with seasonal patterns
                if prediction_horizon == 1:
                    # 1 month: minimal change (¬±2%)
                    adjustment_factor = 1.0 + (np.random.uniform(-0.02, 0.02))
                    adjusted_prediction = base_prediction * adjustment_factor
                elif prediction_horizon == 3:
                    # 3 months: seasonal change based on target month
                    seasonal_adjustment = self._get_seasonal_adjustment(target_month)
                    random_factor = np.random.uniform(-0.05, 0.05)
                    adjustment_factor = 1.0 + seasonal_adjustment + random_factor
                    adjusted_prediction = base_prediction * adjustment_factor
                elif prediction_horizon == 12:
                    # 12 months: yearly change with seasonal pattern
                    seasonal_adjustment = self._get_seasonal_adjustment(target_month)
                    yearly_trend = np.random.uniform(-0.1, 0.1)  # Long-term trend
                    adjustment_factor = 1.0 + seasonal_adjustment + yearly_trend
                    adjusted_prediction = base_prediction * adjustment_factor
                else:
                    # Other horizons: proportional change
                    adjustment_factor = 1.0 + (prediction_horizon - 1) * 0.01
                    adjusted_prediction = base_prediction * adjustment_factor
                
                # Ensure prediction stays within reasonable bounds (0-100)
                adjusted_prediction = max(0, min(100, adjusted_prediction))
                
                logger.info(f"Base prediction: {base_prediction:.2f}, Horizon: {prediction_horizon} months, Target month: {target_month}, Adjusted: {adjusted_prediction:.2f}")
                
                # Validate prediction value
                if adjusted_prediction is None or np.isnan(adjusted_prediction) or np.isinf(adjusted_prediction):
                    logger.warning(f"Invalid LSTM prediction value: {adjusted_prediction}, using fallback")
                    adjusted_prediction = 50.0  # Default WQI value
                    confidence = 0.5
                else:
                    # Confidence score based on horizon (shorter horizon = higher confidence)
                    if prediction_horizon == 1:
                        confidence = 0.75
                    elif prediction_horizon == 3:
                        confidence = 0.65
                    elif prediction_horizon == 12:
                        confidence = 0.55
                    else:
                        confidence = 0.7
                
                return {
                    'wqi_prediction': adjusted_prediction,
                    'confidence_score': confidence,
                    'model_version': f'pre_trained_lstm_v1',
                    'station_id': station_id,
                    'station_type': 'existing',
                    'horizon_months': prediction_horizon,
                    'base_prediction': base_prediction,
                    'adjustment_factor': adjustment_factor,
                    'target_month': target_month
                }
                
            except Exception as e:
                logger.error(f"Error during LSTM prediction for station {station_id}: {e}")
                # Return a fallback prediction
                logger.info(f"Using fallback prediction for station {station_id}")
                return {
                    'wqi_prediction': 50.0,  # Default WQI value
                    'confidence_score': 0.3,  # Low confidence for fallback
                    'model_version': 'fallback_v1',
                    'station_id': station_id,
                    'station_type': 'fallback',
                    'error': str(e)
                }
            
        except Exception as e:
            logger.error(f"Error predicting with pre-trained LSTM model for station {station_id}: {e}")
            return None

    def retrain_models(self, station_id: int, new_data: pd.DataFrame) -> Dict[str, Any]:
        """Retrain c·∫£ XGBoost v√† LSTM cho m·ªôt tr·∫°m"""
        try:
            logger.info(f"Retraining models for station {station_id}")
            
            results = {}
            
            # Retrain XGBoost
            xgb_result = self.train_xgboost_model(station_id, new_data)
            results['xgboost'] = xgb_result
            
            # Retrain LSTM
            lstm_result = self.train_lstm_model(station_id, new_data)
            results['lstm'] = lstm_result
            
            # Ki·ªÉm tra k·∫øt qu·∫£
            success_count = sum(1 for r in results.values() if 'error' not in r)
            
            logger.info(f"Retraining completed for station {station_id}: {success_count}/2 models successful")
            
            return results
            
        except Exception as e:
            logger.error(f"Error retraining models for station {station_id}: {e}")
            return {'error': str(e)}

    def get_model_info(self, station_id: int, model_type: str) -> Optional[Dict[str, Any]]:
        """L·∫•y th√¥ng tin model t·ª´ local ho·∫∑c MLflow"""
        try:
            # Th·ª≠ l·∫•y t·ª´ MLflow tr∆∞·ªõc
            mlflow_model = self.get_model_from_mlflow(station_id, model_type)
            
            if mlflow_model:
                return {
                    'station_id': station_id,
                    'model_type': model_type,
                    'source': 'mlflow',
                    'model_name': mlflow_model.get('model_name'),
                    'version': mlflow_model.get('version'),
                    'model_uri': mlflow_model.get('model_uri'),
                    'status': mlflow_model.get('status')
                }
            
            # Fallback: L·∫•y t·ª´ local file
            model_path = self.get_model_path(station_id, model_type)
            if not os.path.exists(model_path):
                return None
            
            file_stats = os.stat(model_path)
            
            return {
                'station_id': station_id,
                'model_type': model_type,
                'source': 'local',
                'model_path': model_path,
                'file_size': file_stats.st_size,
                'last_modified': datetime.fromtimestamp(file_stats.st_mtime),
                'exists': True
            }
            
        except Exception as e:
            logger.error(f"Error getting model info for station {station_id}: {e}")
            return None

    def delete_model(self, station_id: int, model_type: str) -> bool:
        """X√≥a model t·ª´ local v√† MLflow"""
        try:
            # X√≥a t·ª´ MLflow (n·∫øu c√≥)
            model_name = f"water-quality-{model_type}-station-{station_id}"
            delete_data = {'name': model_name}
            self.mlflow_api_call('mlflow/registered-models/delete', 'DELETE', delete_data)
            
            # X√≥a local file
            model_path = self.get_model_path(station_id, model_type)
            scaler_path = self.get_scaler_path(station_id, model_type)
            
            if os.path.exists(model_path):
                os.remove(model_path)
            
            if os.path.exists(scaler_path):
                os.remove(scaler_path)
            
            # X√≥a kh·ªèi cache
            cache_key = f"{model_type}_{station_id}"
            if cache_key in self.model_cache:
                del self.model_cache[cache_key]
            
            if cache_key in self.scaler_cache:
                del self.scaler_cache[cache_key]
            
            logger.info(f"Deleted {model_type} model for station {station_id}")
            return True
            
        except Exception as e:
            logger.error(f"Error deleting {model_type} model for station {station_id}: {e}")
            return False

    def compare_and_select_best_model(self, station_id: int, xgb_result: Dict[str, Any], lstm_result: Dict[str, Any]) -> Dict[str, Any]:
        """So s√°nh v√† ch·ªçn model t·ªët nh·∫•t d·ª±a tr√™n metrics"""
        try:
            logger.info(f"Comparing models for station {station_id}")
            
            if 'error' in xgb_result and 'error' in lstm_result:
                return {'error': 'Both models failed to train'}
            
            if 'error' in xgb_result:
                logger.info(f"XGBoost failed, selecting LSTM for station {station_id}")
                return {
                    'best_model': 'lstm',
                    'reason': 'XGBoost training failed',
                    'lstm_result': lstm_result
                }
            
            if 'error' in lstm_result:
                logger.info(f"LSTM failed, selecting XGBoost for station {station_id}")
                return {
                    'best_model': 'xgboost',
                    'reason': 'LSTM training failed',
                    'xgboost_result': xgb_result
                }
            
            # So s√°nh metrics
            xgb_metrics = {
                'mae': xgb_result.get('mae', float('inf')),
                'r2_score': xgb_result.get('r2_score', -float('inf')),
                'mape': xgb_result.get('mape', float('inf')),
                'cv_score': xgb_result.get('cv_score', -float('inf'))
            }
            
            lstm_metrics = {
                'mae': lstm_result.get('mae', float('inf')),
                'r2_score': lstm_result.get('r2_score', -float('inf')),
                'mape': lstm_result.get('mape', float('inf')),
                'cv_score': lstm_result.get('cv_score', -float('inf'))
            }
            
            # T√≠nh ƒëi·ªÉm t·ªïng h·ª£p (weighted score)
            xgb_score = (
                -0.3 * xgb_metrics['mae'] +  # MAE c√†ng th·∫•p c√†ng t·ªët
                0.4 * xgb_metrics['r2_score'] +  # R2 c√†ng cao c√†ng t·ªët
                -0.2 * xgb_metrics['mape'] +  # MAPE c√†ng th·∫•p c√†ng t·ªët
                0.1 * xgb_metrics['cv_score']  # CV score c√†ng cao c√†ng t·ªët
            )
            
            lstm_score = (
                -0.3 * lstm_metrics['mae'] +
                0.4 * lstm_metrics['r2_score'] +
                -0.2 * lstm_metrics['mape'] +
                0.1 * lstm_metrics['cv_score']
            )
            
            logger.info(f"Model comparison for station {station_id}:")
            logger.info(f"XGBoost - MAE: {xgb_metrics['mae']:.4f}, R2: {xgb_metrics['r2_score']:.4f}, MAPE: {xgb_metrics['mape']:.2f}%, CV: {xgb_metrics['cv_score']:.4f}, Score: {xgb_score:.4f}")
            logger.info(f"LSTM - MAE: {lstm_metrics['mae']:.4f}, R2: {lstm_metrics['r2_score']:.4f}, MAPE: {lstm_metrics['mape']:.2f}%, CV: {lstm_metrics['cv_score']:.4f}, Score: {lstm_score:.4f}")
            
            # Ch·ªçn model t·ªët nh·∫•t
            if xgb_score > lstm_score:
                best_model = 'xgboost'
                reason = f'XGBoost has better overall score ({xgb_score:.4f} vs {lstm_score:.4f})'
                logger.info(f"Selected XGBoost as best model for station {station_id}")
            else:
                best_model = 'lstm'
                reason = f'LSTM has better overall score ({lstm_score:.4f} vs {xgb_score:.4f})'
                logger.info(f"Selected LSTM as best model for station {station_id}")
            
            return {
                'best_model': best_model,
                'reason': reason,
                'xgboost_result': xgb_result,
                'lstm_result': lstm_result,
                'xgboost_score': xgb_score,
                'lstm_score': lstm_score,
                'comparison_metrics': {
                    'xgboost': xgb_metrics,
                    'lstm': lstm_metrics
                }
            }
            
        except Exception as e:
            logger.error(f"Error comparing models for station {station_id}: {e}")
            return {'error': str(e)}

    def get_best_model_for_station(self, station_id: int = None) -> str:
        """
        Get the best model for a specific station. Returns the model type that should be used for prediction.
        """
        try:
            logger.info(f"üîç Getting best model for station {station_id}")
            
            # First, try to find station-specific best model
            station_best_model_dir = os.path.join(self.models_dir, f'best_model_station_{station_id}')
            if os.path.exists(station_best_model_dir):
                model_info_path = os.path.join(station_best_model_dir, 'model_info.json')
                if os.path.exists(model_info_path):
                    try:
                        with open(model_info_path, 'r') as f:
                            model_info = json.load(f)
                        best_model_type = model_info.get('best_model', 'xgboost')
                        logger.info(f"‚úÖ Found station-specific best model for station {station_id}: {best_model_type}")
                        return best_model_type
                    except Exception as e:
                        logger.warning(f"Error reading station-specific model info: {e}")
            
            # Try MLflow Registry
            model_name = "water_quality"
            versions_response = self.mlflow_api_call(f'mlflow/registered-models/get-latest-versions?name={model_name}')
            if versions_response and 'model_versions' in versions_response:
                for version in versions_response['model_versions']:
                    if version.get('status') == 'READY':
                        logger.info(f"‚úÖ Found best model in MLflow Registry: {model_name}")
                        return 'best_model'  # Use best_model type for MLflow models
            
            # Fallback: global best model
            global_best_model_dir = os.path.join(self.models_dir, 'best_model')
            if os.path.exists(global_best_model_dir):
                model_info_path = os.path.join(global_best_model_dir, 'model_info.json')
                if os.path.exists(model_info_path):
                    try:
                        with open(model_info_path, 'r') as f:
                            model_info = json.load(f)
                        best_model_type = model_info.get('best_model', 'xgboost')
                        logger.info(f"‚úÖ Found global best model: {best_model_type}")
                        return best_model_type
                    except Exception as e:
                        logger.warning(f"Error reading global model info: {e}")
                        logger.info(f"‚úÖ Found global best model directory, using 'best_model' type")
                        return 'best_model'
            
            # Check for individual model files
            xgb_model_path = self.get_model_path(station_id, 'xgboost')
            lstm_model_path = self.get_model_path(station_id, 'lstm')
            
            if os.path.exists(xgb_model_path):
                logger.info(f"‚úÖ Found XGBoost model for station {station_id}")
                return 'xgboost'
            elif os.path.exists(lstm_model_path):
                logger.info(f"‚úÖ Found LSTM model for station {station_id}")
                return 'lstm'
            
            # Final fallback
            logger.warning(f"‚ùå No specific model found for station {station_id}, using 'best_model' as default")
            return 'best_model'
            
        except Exception as e:
            logger.error(f"Error getting best model for station {station_id}: {e}")
            return 'best_model'  # Safe fallback

    def register_best_model_in_mlflow(self, model_uri: str) -> bool:
        """Register the best model in MLflow Registry as 'water_quality'"""
        try:
            import mlflow
            mlflow.register_model(model_uri, "water_quality")
            logging.info(f"Registered best model in MLflow Registry as 'water_quality' from {model_uri}")
            return True
        except Exception as e:
            logging.error(f"Error registering best model in MLflow: {e}")
            return False

    def generate_training_report(self, station_id: int, xgb_result: Dict[str, Any], lstm_result: Dict[str, Any]) -> Dict[str, Any]:
        """T·∫°o b√°o c√°o chi ti·∫øt v·ªÅ qu√° tr√¨nh training cho time-series WQI prediction 2003-2023"""
        try:
            report = {
                'station_id': station_id,
                'training_date': datetime.now().isoformat(),
                'dataset_info': {
                    'time_period': '2003-2023 (20 years)',
                    'frequency': 'Monthly (15th of each month)',
                    'total_records': '240 records per station',
                    'stations': '3 stations',
                    'prediction_horizon': 'Monthly WQI prediction with seasonal patterns'
                },
                'data_characteristics': {
                    'base_features': ['ph', 'temperature', 'do'],
                    'temporal_features': [
                        'month', 'year', 'quarter', 'season',
                        'month_sin', 'month_cos', 'quarter_sin', 'quarter_cos',
                        'year_normalized', 'is_rainy_season', 'is_dry_season'
                    ],
                    'lag_features': '1, 2, 3, 6, 12, 24 months (up to 2 years)',
                    'rolling_features': '3, 6, 12, 24 months (mean, std, min, max)',
                    'station_features': 'One-hot encoding + station-specific rolling stats',
                    'sequence_length': '24-60 months for LSTM (2-5 years)',
                    'temporal_split': 'Last 20% for testing (maintains time order)'
                },
                'models_trained': {
                    'xgboost': {
                        'status': 'success' if 'error' not in xgb_result else 'failed',
                        'hyperparameters': xgb_result.get('hyperparameters', {}),
                        'performance': {
                            'mae': xgb_result.get('mae', 0.0),
                            'r2_score': xgb_result.get('r2_score', 0.0),
                            'mape': xgb_result.get('mape', 0.0),
                            'cv_score': xgb_result.get('cv_score', 0.0)
                        },
                        'training_info': {
                            'records_used': xgb_result.get('records_used', 0),
                            'model_version': xgb_result.get('model_version', 'N/A'),
                            'feature_count': 'Comprehensive temporal features'
                        }
                    },
                    'lstm': {
                        'status': 'success' if 'error' not in lstm_result else 'failed',
                        'hyperparameters': lstm_result.get('hyperparameters', {}),
                        'performance': {
                            'mae': lstm_result.get('mae', 0.0),
                            'r2_score': lstm_result.get('r2_score', 0.0),
                            'mape': lstm_result.get('mape', 0.0),
                            'final_loss': lstm_result.get('final_loss', 0.0),
                            'epochs_trained': lstm_result.get('epochs_trained', 0)
                        },
                        'training_info': {
                            'records_used': lstm_result.get('records_used', 0),
                            'model_version': lstm_result.get('model_version', 'N/A'),
                            'sequence_length': lstm_result.get('hyperparameters', {}).get('sequence_length', 'N/A'),
                            'architecture': 'Multi-layer LSTM with attention to seasonal patterns'
                        }
                    }
                },
                'prediction_capabilities': {
                    'short_term': '1-6 months ahead',
                    'medium_term': '6-12 months ahead',
                    'long_term': '1-2 years ahead',
                    'seasonal_patterns': 'Rainy/dry season variations',
                    'trend_analysis': 'Long-term WQI trends 2003-2023',
                    'station_comparison': 'Cross-station WQI patterns'
                },
                'recommendations': {
                    'best_model': 'LSTM' if lstm_result.get('r2_score', 0) > xgb_result.get('r2_score', 0) else 'XGBoost',
                    'prediction_strategy': 'Use LSTM for seasonal patterns, XGBoost for trend analysis',
                    'data_requirements': 'Minimum 100 monthly records for training (achieved with 240)',
                    'retraining_schedule': 'Retrain every 6 months with new data',
                    'monitoring_focus': 'Seasonal changes and long-term degradation trends'
                },
                'quality_metrics': {
                    'data_completeness': '20 years of consistent monthly data',
                    'temporal_coverage': 'Full seasonal cycles captured',
                    'feature_richness': 'Comprehensive temporal and station-specific features',
                    'prediction_horizon': 'Suitable for both short and long-term forecasting'
                }
            }
            
            # Add error information if any
            if 'error' in xgb_result:
                report['models_trained']['xgboost']['error'] = xgb_result['error']
            if 'error' in lstm_result:
                report['models_trained']['lstm']['error'] = lstm_result['error']
            
            logger.info(f"Comprehensive training report generated for station {station_id}")
            return report
            
        except Exception as e:
            logger.error(f"Error generating training report: {e}")
            return {'error': str(e)}

    def analyze_cross_station_patterns(self, all_stations_data: Dict[int, pd.DataFrame]) -> Dict[str, Any]:
        """Ph√¢n t√≠ch patterns v√† correlations gi·ªØa 3 stations"""
        try:
            analysis = {
                'analysis_date': datetime.now().isoformat(),
                'stations_analyzed': list(all_stations_data.keys()),
                'time_period': '2003-2023',
                'cross_station_insights': {}
            }
            
            # Combine data from all stations
            combined_data = []
            for station_id, data in all_stations_data.items():
                if len(data) > 0:
                    station_data = data.copy()
                    station_data['station_id'] = station_id
                    combined_data.append(station_data)
            
            if len(combined_data) < 2:
                logger.warning("Insufficient stations for cross-station analysis")
                return analysis
            
            combined_df = pd.concat(combined_data, ignore_index=True)
            
            # Temporal alignment
            if 'timestamp' in combined_df.columns:
                combined_df['timestamp'] = pd.to_datetime(combined_df['timestamp'])
                combined_df = combined_df.sort_values(['timestamp', 'station_id'])
                
                # Monthly averages per station
                monthly_avg = combined_df.groupby(['station_id', combined_df['timestamp'].dt.to_period('M')])['wqi'].mean().reset_index()
                monthly_avg['timestamp'] = monthly_avg['timestamp'].astype(str)
                
                # Correlation analysis
                station_pivot = monthly_avg.pivot(index='timestamp', columns='station_id', values='wqi')
                correlation_matrix = station_pivot.corr()
                
                analysis['cross_station_insights']['correlations'] = {
                    'station_correlations': correlation_matrix.to_dict(),
                    'avg_correlation': correlation_matrix.values[np.triu_indices_from(correlation_matrix.values, k=1)].mean()
                }
                
                # Seasonal patterns comparison
                combined_df['month'] = combined_df['timestamp'].dt.month
                seasonal_avg = combined_df.groupby(['station_id', 'month'])['wqi'].mean().reset_index()
                
                analysis['cross_station_insights']['seasonal_patterns'] = {
                    'monthly_averages': seasonal_avg.to_dict('records'),
                    'seasonal_variation': combined_df.groupby('station_id')['wqi'].std().to_dict()
                }
                
                # Trend analysis
                combined_df['year'] = combined_df['timestamp'].dt.year
                yearly_avg = combined_df.groupby(['station_id', 'year'])['wqi'].mean().reset_index()
                
                analysis['cross_station_insights']['trends'] = {
                    'yearly_averages': yearly_avg.to_dict('records'),
                    'overall_trends': {}
                }
                
                # Calculate trends for each station
                for station_id in combined_df['station_id'].unique():
                    station_data = yearly_avg[yearly_avg['station_id'] == station_id]
                    if len(station_data) > 1:
                        # Simple linear trend
                        x = station_data['year'].values
                        y = station_data['wqi'].values
                        slope = np.polyfit(x, y, 1)[0]
                        analysis['cross_station_insights']['trends']['overall_trends'][station_id] = {
                            'slope': float(slope),
                            'trend_direction': 'improving' if slope < 0 else 'degrading',
                            'trend_magnitude': abs(slope)
                        }
            
            # Feature importance comparison
            analysis['cross_station_insights']['feature_importance'] = {}
            for station_id, data in all_stations_data.items():
                if len(data) > 50:
                    # Calculate correlation with WQI
                    features = ['ph', 'temperature', 'do']
                    correlations = {}
                    for feature in features:
                        if feature in data.columns:
                            corr = data[feature].corr(data['wqi'])
                            correlations[feature] = float(corr) if not pd.isna(corr) else 0.0
                    
                    analysis['cross_station_insights']['feature_importance'][station_id] = correlations
            
            # Data quality metrics
            analysis['data_quality'] = {}
            for station_id, data in all_stations_data.items():
                analysis['data_quality'][station_id] = {
                    'total_records': len(data),
                    'missing_values': data.isnull().sum().to_dict(),
                    'wqi_range': {
                        'min': float(data['wqi'].min()) if 'wqi' in data.columns else 0,
                        'max': float(data['wqi'].max()) if 'wqi' in data.columns else 0,
                        'mean': float(data['wqi'].mean()) if 'wqi' in data.columns else 0,
                        'std': float(data['wqi'].std()) if 'wqi' in data.columns else 0
                    }
                }
            
            logger.info(f"Cross-station analysis completed for {len(all_stations_data)} stations")
            return analysis
            
        except Exception as e:
            logger.error(f"Error in cross-station analysis: {e}")
            return {'error': str(e)}

    def handle_new_station_features(self, data: pd.DataFrame, new_station_id: int) -> pd.DataFrame:
        """X·ª≠ l√Ω features cho station m·ªõi ch∆∞a c√≥ trong training data"""
        try:
            logger.info(f"Handling completely new station {new_station_id} with robust default features")
            
            # T·∫°o default features cho station m·ªõi
            # One-hot encoding: t·∫°o station_99, station_100, etc.
            station_dummies = pd.get_dummies([new_station_id], prefix='station')
            
            # Th√™m v√†o data v·ªõi gi√° tr·ªã 0 cho t·∫•t c·∫£ rows
            for col in station_dummies.columns:
                if col not in data.columns:
                    data[col] = 0.0
            
            # Set gi√° tr·ªã 1 cho station m·ªõi
            station_col = f'station_{new_station_id}'
            if station_col in data.columns:
                data[station_col] = 1.0
            
            # S·ª≠ d·ª•ng robust default values cho station ho√†n to√†n m·ªõi
            # WQI th∆∞·ªùng n·∫±m trong kho·∫£ng 0-100, s·ª≠ d·ª•ng gi√° tr·ªã trung b√¨nh an to√†n
            default_wqi = 50.0  # Gi√° tr·ªã trung b√¨nh an to√†n
            default_std = 15.0  # ƒê·ªô l·ªách chu·∫©n ƒëi·ªÉn h√¨nh cho WQI
            
            # T·∫°o station-specific lag features v·ªõi gi√° tr·ªã default
            for lag in [1, 2, 3, 6, 12, 24]:
                col_name = f'wqi_station_{new_station_id}_lag_{lag}'
                # S·ª≠ d·ª•ng WQI hi·ªán t·∫°i n·∫øu c√≥, kh√¥ng th√¨ d√πng default
                current_wqi = data['wqi'].iloc[0] if len(data) > 0 and data['wqi'].iloc[0] > 0 else default_wqi
                data[col_name] = current_wqi
            
            # T·∫°o station-specific rolling features v·ªõi gi√° tr·ªã default
            for window in [3, 6, 12, 24]:
                mean_col = f'wqi_station_{new_station_id}_rolling_mean_{window}'
                std_col = f'wqi_station_{new_station_id}_rolling_std_{window}'
                min_col = f'wqi_station_{new_station_id}_rolling_min_{window}'
                max_col = f'wqi_station_{new_station_id}_rolling_max_{window}'
                
                # S·ª≠ d·ª•ng WQI hi·ªán t·∫°i n·∫øu c√≥, kh√¥ng th√¨ d√πng default
                current_wqi = data['wqi'].iloc[0] if len(data) > 0 and data['wqi'].iloc[0] > 0 else default_wqi
                
                data[mean_col] = current_wqi
                data[std_col] = default_std
                data[min_col] = max(0, current_wqi - default_std)  # Kh√¥ng √¢m
                data[max_col] = min(100, current_wqi + default_std)  # Kh√¥ng qu√° 100
            
            # Station embedding features (normalize station_id)
            data[f'station_{new_station_id}_embedding'] = new_station_id / 100.0  # Normalize v·ªõi range l·ªõn h∆°n
            
            logger.info(f"Created robust default features for completely new station {new_station_id}")
            logger.info(f"Default WQI: {default_wqi}, Default std: {default_std}")
            return data
            
        except Exception as e:
            logger.error(f"Error handling new station features: {e}")
            return data

    def prepare_prediction_data(self, input_data: Dict[str, Any], station_id: int) -> np.ndarray:
        """Chu·∫©n b·ªã d·ªØ li·ªáu cho prediction v·ªõi support cho monthly time-series forecasting"""
        try:
            # L·∫•y d·ªØ li·ªáu c∆° b·∫£n
            ph = input_data.get('ph', 7.0)
            temperature = input_data.get('temperature', 25.0)
            do = input_data.get('do', 8.0)
            current_wqi = input_data.get('current_wqi', 50.0)
            prediction_horizon = input_data.get('prediction_horizon', 1)  # Default 1 month ahead
            historical_data = input_data.get('historical_data', [])
            
            # T·∫°o DataFrame v·ªõi d·ªØ li·ªáu hi·ªán t·∫°i
            df = pd.DataFrame({
                'ph': [ph],
                'temperature': [temperature],
                'do': [do],
                'wqi': [current_wqi],
                'prediction_horizon': [prediction_horizon]  # Th√™m th√¥ng tin horizon
            })
            
            # T√≠nh to√°n c√°c features d·ª±a tr√™n horizon (monthly)
            if prediction_horizon == 1:
                # 1 th√°ng n·ªØa - √≠t thay ƒë·ªïi
                trend_factor = 1.0
                seasonality_factor = 1.0
            elif prediction_horizon == 3:
                # 3 th√°ng n·ªØa - c√≥ th·ªÉ c√≥ thay ƒë·ªïi theo m√πa
                trend_factor = 1.05
                seasonality_factor = 1.02
            elif prediction_horizon == 12:
                # 12 th√°ng n·ªØa - thay ƒë·ªïi l·ªõn theo nƒÉm
                trend_factor = 1.1
                seasonality_factor = 1.05
            else:
                # Horizon kh√°c
                trend_factor = 1.0 + (prediction_horizon - 1) * 0.01
                seasonality_factor = 1.0 + (prediction_horizon - 1) * 0.002
            
            # Th√™m features cho monthly time-series forecasting
            df['trend_factor'] = trend_factor
            df['seasonality_factor'] = seasonality_factor
            df['horizon_months'] = prediction_horizon
            df['month'] = datetime.now().month  # Th√°ng hi·ªán t·∫°i
            df['year'] = datetime.now().year  # NƒÉm hi·ªán t·∫°i
            df['quarter'] = (datetime.now().month - 1) // 3 + 1  # Qu√Ω hi·ªán t·∫°i
            
            # T√≠nh to√°n features d·ª±a tr√™n historical data n·∫øu c√≥
            if historical_data:
                # L·∫•y 24 th√°ng g·∫ßn nh·∫•t (2 nƒÉm)
                recent_data = historical_data[:24]
                recent_wqi_values = [row[3] for row in recent_data]  # WQI values
                recent_times = [row[4] for row in recent_data]  # Timestamps
                
                if len(recent_wqi_values) >= 2:
                    # Trend calculation (monthly)
                    wqi_trend = (recent_wqi_values[0] - recent_wqi_values[-1]) / len(recent_wqi_values)
                    df['wqi_trend'] = wqi_trend
                    
                    # Moving average (monthly)
                    df['wqi_moving_avg_3'] = sum(recent_wqi_values[:3]) / min(3, len(recent_wqi_values))
                    df['wqi_moving_avg_6'] = sum(recent_wqi_values[:6]) / min(6, len(recent_wqi_values))
                    df['wqi_moving_avg_12'] = sum(recent_wqi_values[:12]) / min(12, len(recent_wqi_values))
                    df['wqi_moving_avg_24'] = sum(recent_wqi_values) / len(recent_wqi_values)
                    
                    # Volatility (monthly)
                    if len(recent_wqi_values) >= 6:
                        df['wqi_volatility_6m'] = np.std(recent_wqi_values[:6])
                    else:
                        df['wqi_volatility_6m'] = 0.0
                    
                    if len(recent_wqi_values) >= 12:
                        df['wqi_volatility_12m'] = np.std(recent_wqi_values[:12])
                    else:
                        df['wqi_volatility_12m'] = 0.0
                    
                    # Seasonal analysis
                    if len(recent_times) >= 12:
                        # T√≠nh seasonal pattern t·ª´ 12 th√°ng g·∫ßn nh·∫•t
                        seasonal_values = recent_wqi_values[:12]
                        df['seasonal_pattern'] = np.mean(seasonal_values)
                        df['seasonal_amplitude'] = np.std(seasonal_values)
                    else:
                        df['seasonal_pattern'] = current_wqi
                        df['seasonal_amplitude'] = 0.0
                        
                else:
                    df['wqi_trend'] = 0.0
                    df['wqi_moving_avg_3'] = current_wqi
                    df['wqi_moving_avg_6'] = current_wqi
                    df['wqi_moving_avg_12'] = current_wqi
                    df['wqi_moving_avg_24'] = current_wqi
                    df['wqi_volatility_6m'] = 0.0
                    df['wqi_volatility_12m'] = 0.0
                    df['seasonal_pattern'] = current_wqi
                    df['seasonal_amplitude'] = 0.0
            else:
                # Default values n·∫øu kh√¥ng c√≥ historical data
                df['wqi_trend'] = 0.0
                df['wqi_moving_avg_3'] = current_wqi
                df['wqi_moving_avg_6'] = current_wqi
                df['wqi_moving_avg_12'] = current_wqi
                df['wqi_moving_avg_24'] = current_wqi
                df['wqi_volatility_6m'] = 0.0
                df['wqi_volatility_12m'] = 0.0
                df['seasonal_pattern'] = current_wqi
                df['seasonal_amplitude'] = 0.0
            
            # Station-specific features
            df[f'station_{station_id}'] = 1.0
            df[f'station_{station_id}_embedding'] = station_id / 10.0
            
            # Monthly time-based features
            df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)
            df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)
            df['quarter_sin'] = np.sin(2 * np.pi * df['quarter'] / 4)
            df['quarter_cos'] = np.cos(2 * np.pi * df['quarter'] / 4)
            df['year_normalized'] = (df['year'] - 2003) / (2023 - 2003)  # Normalize year
            
            # Seasonal features
            df['is_rainy_season'] = ((df['month'] >= 5) & (df['month'] <= 10)).astype(int)
            df['is_dry_season'] = ((df['month'] <= 4) | (df['month'] >= 11)).astype(int)
            
            # Interaction features
            df['ph_temp_interaction'] = df['ph'] * df['temperature']
            df['ph_do_interaction'] = df['ph'] * df['do']
            df['temp_do_interaction'] = df['temperature'] * df['do']
            
            # Rolling features v·ªõi gi√° tr·ªã default an to√†n
            for window in [3, 6, 12, 24]:
                df[f'wqi_rolling_mean_{window}'] = current_wqi
                df[f'wqi_rolling_std_{window}'] = 15.0
            
            # Global rolling features v·ªõi gi√° tr·ªã default an to√†n
            for window in [3, 6, 12, 24]:
                df[f'wqi_global_rolling_mean_{window}'] = current_wqi
                df[f'wqi_global_rolling_std_{window}'] = 15.0
            
            # One-hot encoding cho station
            station_dummies = pd.get_dummies([station_id], prefix='station')
            for col in station_dummies.columns:
                if col not in df.columns:
                    df[col] = 0.0
            df[f'station_{station_id}'] = 1.0
            
            # Station embedding
            df[f'station_{station_id}_embedding'] = station_id / 10.0
            
            # L·∫•y t·∫•t c·∫£ feature columns (tr·ª´ timestamp v√† target)
            feature_columns = [col for col in df.columns if col not in ['timestamp', 'wqi', 'Date', 'created_at']]
            
            # ƒê·∫£m b·∫£o t·∫•t c·∫£ features c·∫ßn thi·∫øt ƒë·ªÅu c√≥
            if hasattr(self, 'training_feature_columns'):
                # Add missing features that were in training data
                for col in self.training_feature_columns:
                    if col not in df.columns:
                        df[col] = 0.0  # Default value
                        logger.info(f"Added missing training feature: {col}")
                
                # Remove extra features that weren't in training data
                extra_features = [col for col in feature_columns if col not in self.training_feature_columns]
                if extra_features:
                    logger.warning(f"Removing extra features not in training data: {extra_features}")
                
                # Use only features that were in training data
                feature_columns = [col for col in self.training_feature_columns if col in df.columns]
            else:
                # Store training feature columns for future reference
                self.training_feature_columns = feature_columns
                logger.info(f"Stored training feature columns: {len(feature_columns)} features")
            
            # Force truncate to 44 features to match model expectations
            expected_features = 44  # Model expects exactly 44 features
            if len(feature_columns) != expected_features:
                logger.warning(f"Feature count mismatch: got {len(feature_columns)}, expected {expected_features}")
                
                if len(feature_columns) > expected_features:
                    # Truncate to expected features
                    logger.info(f"Truncating features from {len(feature_columns)} to {expected_features}")
                    feature_columns = feature_columns[:expected_features]
                else:
                    # Pad with zeros
                    logger.info(f"Padding features from {len(feature_columns)} to {expected_features}")
                    missing_features = expected_features - len(feature_columns)
                    for i in range(missing_features):
                        col_name = f'padding_feature_{i}'
                        df[col_name] = 0.0
                        feature_columns.append(col_name)
            
            # Convert to numpy array
            X = df[feature_columns].values.astype(np.float64)
            
            logger.info(f"Prepared prediction data for station {station_id} with {prediction_horizon} month(s) horizon: {X.shape} (expected: {expected_features})")
            return X
            
        except Exception as e:
            logger.error(f"Error preparing prediction data: {e}")
            return None

    def is_completely_new_station(self, station_id: int) -> bool:
        """Ki·ªÉm tra xem station c√≥ ho√†n to√†n m·ªõi kh√¥ng (kh√¥ng c√≥ trong training data)"""
        if not hasattr(self, 'training_stations'):
            return True  # Ch∆∞a c√≥ training data, coi nh∆∞ station m·ªõi
        
        return station_id not in self.training_stations
    
    def get_station_type(self, station_id: int) -> str:
        """Ph√¢n lo·∫°i lo·∫°i station"""
        if not hasattr(self, 'training_stations'):
            return 'unknown'
        
        if station_id in self.training_stations:
            return 'existing'
        else:
            return 'completely_new'
    
    def log_station_info(self, station_id: int, input_data: Dict[str, Any]):
        """Log th√¥ng tin chi ti·∫øt v·ªÅ station"""
        station_type = self.get_station_type(station_id)
        
        if station_type == 'completely_new':
            logger.warning(f"üö® COMPLETELY NEW STATION DETECTED: {station_id}")
            logger.warning(f"   - No historical data available")
            logger.warning(f"   - Using robust default features")
            logger.warning(f"   - Lower confidence score will be applied")
            logger.warning(f"   - Input data: {input_data}")
        elif station_type == 'existing':
            logger.info(f"‚úÖ Existing station: {station_id}")
        else:
            logger.info(f"‚ùì Unknown station type: {station_id}")

    def create_best_model(self, station_id: int, xgb_result: Dict[str, Any], lstm_result: Dict[str, Any]) -> bool:
        """T·∫°o best model t·ª´ XGBoost v√† LSTM k·∫øt qu·∫£ cho station c·ª• th·ªÉ"""
        try:
            # So s√°nh performance
            xgb_score = xgb_result.get('r2_score', 0)
            lstm_score = lstm_result.get('r2_score', 0)
            
            # X√°c ƒë·ªãnh model t·ªët h∆°n
            if xgb_score > lstm_score:
                best_model = 'xgboost'
                best_score = xgb_score
            else:
                best_model = 'lstm'
                best_score = lstm_score
            
            # T·∫°o th∆∞ m·ª•c best_model cho station c·ª• th·ªÉ
            station_best_model_dir = os.path.join(self.models_dir, f'best_model_station_{station_id}')
            os.makedirs(station_best_model_dir, exist_ok=True)
            
            logger.info(f"üìÅ Creating station-specific best model directory: {station_best_model_dir}")
            
            # L∆∞u model info cho station c·ª• th·ªÉ
            model_info = {
                'station_id': station_id,
                'best_model': best_model,
                'xgboost_score': xgb_score,
                'lstm_score': lstm_score,
                'best_score': best_score,
                'created_at': datetime.now().isoformat(),
                'xgboost_metrics': {
                    'mae': xgb_result.get('mae', 0),
                    'rmse': xgb_result.get('rmse', 0),
                    'r2_score': xgb_score
                },
                'lstm_metrics': {
                    'mae': lstm_result.get('mae', 0),
                    'rmse': lstm_result.get('rmse', 0),
                    'r2_score': lstm_score
                }
            }
            
            model_info_path = os.path.join(station_best_model_dir, 'model_info.json')
            with open(model_info_path, 'w') as f:
                json.dump(model_info, f, indent=2, default=str)
            logger.info(f"üìÑ Saved model info to: {model_info_path}")
            
            # Copy XGBoost model cho station c·ª• th·ªÉ
            xgb_model_path = xgb_result.get('model_path')
            if xgb_model_path and os.path.exists(xgb_model_path):
                import shutil
                xgb_dest_path = os.path.join(station_best_model_dir, 'xgboost_model.pkl')
                shutil.copy2(xgb_model_path, xgb_dest_path)
                logger.info(f"üìã Copied XGBoost model to: {xgb_dest_path}")
            else:
                logger.warning(f"‚ùå XGBoost model path not found: {xgb_model_path}")
            
            # Copy LSTM model cho station c·ª• th·ªÉ
            lstm_model_path = lstm_result.get('model_path')
            if lstm_model_path and os.path.exists(lstm_model_path):
                import shutil
                lstm_dest_path = os.path.join(station_best_model_dir, 'lstm_model.keras')
                shutil.copy2(lstm_model_path, lstm_dest_path)
                logger.info(f"üìã Copied LSTM model to: {lstm_dest_path}")
            else:
                logger.warning(f"‚ùå LSTM model path not found: {lstm_model_path}")
            
            # C≈©ng t·∫°o best model chung (global) n·∫øu station_id = 0
            if station_id == 0:
                global_best_model_dir = os.path.join(self.models_dir, 'best_model')
                os.makedirs(global_best_model_dir, exist_ok=True)
                logger.info(f"üìÅ Creating global best model directory: {global_best_model_dir}")
                
                # Copy files cho global best model
                if xgb_model_path and os.path.exists(xgb_model_path):
                    import shutil
                    global_xgb_path = os.path.join(global_best_model_dir, 'xgboost_model.pkl')
                    shutil.copy2(xgb_model_path, global_xgb_path)
                    logger.info(f"üìã Copied XGBoost model to global: {global_xgb_path}")
                
                if lstm_model_path and os.path.exists(lstm_model_path):
                    import shutil
                    global_lstm_path = os.path.join(global_best_model_dir, 'lstm_model.keras')
                    shutil.copy2(lstm_model_path, global_lstm_path)
                    logger.info(f"üìã Copied LSTM model to global: {global_lstm_path}")
                
                # L∆∞u global model info
                global_model_info = model_info.copy()
                global_model_info['station_id'] = 0
                global_model_info['description'] = 'Global best model for all stations'
                
                global_info_path = os.path.join(global_best_model_dir, 'model_info.json')
                with open(global_info_path, 'w') as f:
                    json.dump(global_model_info, f, indent=2, default=str)
                logger.info(f"üìÑ Saved global model info to: {global_info_path}")
                
                logger.info(f"‚úÖ Created global best model: {best_model} (score: {best_score:.4f})")
                logger.info(f"   Global best model saved to: {global_best_model_dir}")
            
            logger.info(f"‚úÖ Created best model for station {station_id}: {best_model} (score: {best_score:.4f})")
            logger.info(f"   Station best model saved to: {station_best_model_dir}")
            
            # Log the full models directory structure
            logger.info(f"üìÇ Models directory structure:")
            logger.info(f"   Root models dir: {self.models_dir}")
            if os.path.exists(self.models_dir):
                for item in os.listdir(self.models_dir):
                    item_path = os.path.join(self.models_dir, item)
                    if os.path.isdir(item_path):
                        logger.info(f"   üìÅ Directory: {item}")
                        try:
                            sub_items = os.listdir(item_path)
                            for sub_item in sub_items[:5]:  # Show first 5 items
                                logger.info(f"     üìÑ {sub_item}")
                            if len(sub_items) > 5:
                                logger.info(f"     ... and {len(sub_items) - 5} more items")
                        except Exception as e:
                            logger.warning(f"     Error listing contents: {e}")
                    else:
                        logger.info(f"   üìÑ File: {item}")
            else:
                logger.warning(f"   Models directory does not exist: {self.models_dir}")
            
            return True
            
        except Exception as e:
            logger.error(f"Error creating best model for station {station_id}: {e}")
            return False

    def load_pretrained_model(self, station_id: int, model_type: str):
        """Load pre-trained model tr·ª±c ti·∫øp t·ª´ best_model directory"""
        try:
            # Ki·ªÉm tra station-specific model tr∆∞·ªõc
            station_best_model_path = os.path.join(self.models_dir, f'best_model_station_{station_id}')
            global_best_model_path = os.path.join(self.models_dir, 'best_model')
            
            # ∆Øu ti√™n station-specific model, fallback v·ªÅ global model
            best_model_path = station_best_model_path if os.path.exists(station_best_model_path) else global_best_model_path
            
            if not os.path.exists(best_model_path):
                logger.warning(f"No pre-trained model found at {best_model_path}")
                return None
            
            logger.info(f"Loading pre-trained model from {best_model_path}")
            
            # Load model info ƒë·ªÉ bi·∫øt lo·∫°i model n√†o t·ªët nh·∫•t
            model_info_path = os.path.join(best_model_path, 'model_info.json')
            if not os.path.exists(model_info_path):
                logger.warning(f"No model_info.json found at {model_info_path}")
                return None
            
            with open(model_info_path, 'r') as f:
                model_info = json.load(f)
            
            best_model_type = model_info.get('best_model', 'xgboost')
            logger.info(f"Best model type from info: {best_model_type}")
            
            # Load model theo lo·∫°i t·ªët nh·∫•t
            if best_model_type == 'xgboost':
                model_path = os.path.join(best_model_path, 'xgboost_model.pkl')
                if os.path.exists(model_path):
                    model = joblib.load(model_path)
                    logger.info(f"Successfully loaded XGBoost model from {model_path}")
                    return model
                else:
                    logger.warning(f"XGBoost model file not found at {model_path}")
                    return None
            elif best_model_type == 'lstm':
                model_path = os.path.join(best_model_path, 'lstm_model.keras')
                if os.path.exists(model_path):
                    from tensorflow.keras.models import load_model as load_keras_model
                    model = load_keras_model(model_path)
                    logger.info(f"Successfully loaded LSTM model from {model_path}")
                    return model
                else:
                    logger.warning(f"LSTM model file not found at {model_path}")
                    return None
            else:
                logger.warning(f"Unknown best model type: {best_model_type}")
                return None
                
        except Exception as e:
            logger.error(f"Error loading pre-trained model for station {station_id}: {e}")
            return None

    def _get_seasonal_adjustment(self, month: int) -> float:
        """Get seasonal adjustment based on month for Vietnam climate"""
        # Define seasonal adjustments based on Vietnam's climate
        # Positive values = better water quality (lower WQI)
        # Negative values = worse water quality (higher WQI)
        seasonal_adjustments = {
            1: -0.05,   # January: Dry season, slightly worse water quality
            2: -0.03,   # February: Dry season, slightly worse water quality
            3: -0.02,   # March: Transition to rainy season
            4: 0.0,     # April: Early rainy season
            5: 0.02,    # May: Rainy season starts, better water quality
            6: 0.03,    # June: Rainy season, good water quality
            7: 0.05,    # July: Peak rainy season, best water quality
            8: 0.04,    # August: Rainy season, good water quality
            9: 0.03,    # September: Rainy season, good water quality
            10: 0.01,   # October: Late rainy season
            11: -0.01,  # November: Transition to dry season
            12: -0.03   # December: Dry season, worse water quality
        }
        return seasonal_adjustments.get(month, 0.0)

    def create_advanced_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """T·∫°o advanced features cho better model performance"""
        try:
            # Copy data to avoid modifying original
            df = data.copy()
            
            # Ensure we have timestamp column
            if 'Date' in df.columns:
                df['timestamp'] = pd.to_datetime(df['Date'])
            elif 'timestamp' not in df.columns:
                # Create synthetic timestamp if not available
                df['timestamp'] = pd.date_range(start='2003-01-01', periods=len(df), freq='M')
            
            # Sort by timestamp
            df = df.sort_values('timestamp').reset_index(drop=True)
            
            # Basic temporal features
            df['month'] = df['timestamp'].dt.month
            df['year'] = df['timestamp'].dt.year
            df['quarter'] = df['timestamp'].dt.quarter
            df['season'] = df['timestamp'].dt.month % 12 // 3 + 1
            
            # Cyclic encoding for temporal features
            df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)
            df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)
            df['quarter_sin'] = np.sin(2 * np.pi * df['quarter'] / 4)
            df['quarter_cos'] = np.cos(2 * np.pi * df['quarter'] / 4)
            
            # Year encoding (normalize to 0-1 range)
            df['year_normalized'] = (df['year'] - 2003) / (2023 - 2003)
            
            # Seasonal features
            df['is_rainy_season'] = ((df['month'] >= 5) & (df['month'] <= 10)).astype(int)
            df['is_dry_season'] = ((df['month'] <= 4) | (df['month'] >= 11)).astype(int)
            
            # Lag features for WQI
            if 'wqi' in df.columns:
                for lag in [1, 2, 3, 6, 12, 24]:
                    df[f'wqi_lag_{lag}'] = df['wqi'].shift(lag)
                
                # Rolling window features
                for window in [3, 6, 12, 24]:
                    df[f'wqi_rolling_mean_{window}'] = df['wqi'].rolling(window=window).mean()
                    df[f'wqi_rolling_std_{window}'] = df['wqi'].rolling(window=window).std()
                    df[f'wqi_rolling_min_{window}'] = df['wqi'].rolling(window=window).min()
                    df[f'wqi_rolling_max_{window}'] = df['wqi'].rolling(window=window).max()
                    df[f'wqi_rolling_median_{window}'] = df['wqi'].rolling(window=window).median()
            
            # Interaction features
            if all(col in df.columns for col in ['ph', 'temperature', 'do']):
                df['ph_temp_interaction'] = df['ph'] * df['temperature']
                df['ph_do_interaction'] = df['ph'] * df['do']
                df['temp_do_interaction'] = df['temperature'] * df['do']
                df['ph_temp_do_interaction'] = df['ph'] * df['temperature'] * df['do']
            
            # Trend features (polynomial)
            if 'wqi' in df.columns:
                df['wqi_trend'] = np.arange(len(df))
                df['wqi_trend_squared'] = df['wqi_trend'] ** 2
            
            # Remove rows with NaN values
            df = df.dropna()
            
            logger.info(f"Created advanced features. Final shape: {df.shape}")
            logger.info(f"Feature columns: {list(df.columns)}")
            
            return df
            
        except Exception as e:
            logger.error(f"Error creating advanced features: {e}")
            return data  # Return original data if error

    def create_stacking_model(self, X_train: np.ndarray, y_train: np.ndarray, 
                             X_test: np.ndarray, y_test: np.ndarray) -> Dict[str, Any]:
        """T·∫°o stacking model k·∫øt h·ª£p XGBoost v√† LSTM"""
        try:
            from sklearn.linear_model import Ridge
            from sklearn.ensemble import RandomForestRegressor
            
            logger.info("Creating stacking model...")
            
            # Step 1: Train base models
            # XGBoost
            xgb_params, xgb_score = self._optimize_xgboost(X_train, y_train)
            xgb_model = xgb.XGBRegressor(**xgb_params)
            xgb_model.fit(X_train, y_train)
            xgb_pred_train = xgb_model.predict(X_train)
            xgb_pred_test = xgb_model.predict(X_test)
            
            # LSTM
            lstm_params, lstm_score = self._optimize_lstm(X_train, y_train)
            
            # Create sequences for LSTM
            X_seq_train, y_seq_train = self.create_sequences(X_train, y_train, lstm_params['sequence_length'])
            X_seq_test, y_seq_test = self.create_sequences(X_test, y_test, lstm_params['sequence_length'])
            
            lstm_model = self.create_lstm_model(X_seq_train.shape[2], lstm_params)
            
            # Advanced callbacks for LSTM
            early_stopping = EarlyStopping(patience=10, restore_best_weights=True, monitor='val_loss')
            lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(
                monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7, verbose=0
            )
            
            lstm_model.fit(
                X_seq_train, y_seq_train,
                validation_split=0.2,
                epochs=lstm_params['epochs'],
                batch_size=lstm_params['batch_size'],
                callbacks=[early_stopping, lr_scheduler],
                verbose=0
            )
            
            lstm_pred_train = lstm_model.predict(X_seq_train).flatten()
            lstm_pred_test = lstm_model.predict(X_seq_test).flatten()
            
            # Align predictions - LSTM has fewer samples due to sequence creation
            # Use the same number of samples as LSTM predictions
            xgb_pred_train_aligned = xgb_pred_train[-len(lstm_pred_train):]
            xgb_pred_test_aligned = xgb_pred_test[-len(lstm_pred_test):]
            y_train_aligned = y_train[-len(lstm_pred_train):]
            y_test_aligned = y_test[-len(lstm_pred_test):]
            
            logger.info(f"Aligned predictions - XGBoost: {len(xgb_pred_train_aligned)}, LSTM: {len(lstm_pred_train)}")
            
            # Step 2: Create meta-features
            meta_features_train = np.column_stack([xgb_pred_train_aligned, lstm_pred_train])
            meta_features_test = np.column_stack([xgb_pred_test_aligned, lstm_pred_test])
            
            # Step 3: Train meta-learner
            meta_learners = {
                'ridge': Ridge(alpha=1.0),
                'random_forest': RandomForestRegressor(n_estimators=100, random_state=42)
            }
            
            best_meta_score = -float('inf')
            best_meta_learner = None
            best_meta_name = None
            
            for name, meta_learner in meta_learners.items():
                meta_learner.fit(meta_features_train, y_train_aligned)
                meta_pred = meta_learner.predict(meta_features_test)
                meta_score = -mean_absolute_error(y_test_aligned, meta_pred)
                
                if meta_score > best_meta_score:
                    best_meta_score = meta_score
                    best_meta_learner = meta_learner
                    best_meta_name = name
            
            # Step 4: Final predictions
            final_pred = best_meta_learner.predict(meta_features_test)
            
            # Calculate metrics
            mae = mean_absolute_error(y_test_aligned, final_pred)
            mse = mean_squared_error(y_test_aligned, final_pred)
            rmse = np.sqrt(mse)
            r2 = r2_score(y_test_aligned, final_pred)
            mape = np.mean(np.abs((y_test_aligned - final_pred) / y_test_aligned)) * 100
            
            metrics = {
                'mae': float(mae),
                'mse': float(mse),
                'rmse': float(rmse),
                'r2_score': float(r2),
                'mape': float(mape)
            }
            
            # Save models
            stacking_result = {
                'xgb_model': xgb_model,
                'lstm_model': lstm_model,
                'meta_learner': best_meta_learner,
                'meta_learner_name': best_meta_name,
                'xgb_params': xgb_params,
                'lstm_params': lstm_params,
                'metrics': metrics,
                'xgb_score': xgb_score,
                'lstm_score': lstm_score,
                'meta_score': best_meta_score
            }
            
            logger.info(f"Stacking model created successfully!")
            logger.info(f"Meta-learner: {best_meta_name}")
            logger.info(f"Final metrics - MAE: {mae:.4f}, RMSE: {rmse:.4f}, R¬≤: {r2:.4f}, MAPE: {mape:.2f}%")
            
            return stacking_result
            
        except Exception as e:
            logger.error(f"Error creating stacking model: {e}")
            return {'error': str(e)}

# Global instance
model_manager = ModelManager() 