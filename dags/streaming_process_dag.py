# /usr/local/airflow/dags/streaming_process_dag.py

from datetime import datetime, timezone
import logging

from airflow.decorators import dag
from airflow.providers.apache.kafka.sensors.kafka import AwaitMessageTriggerFunctionSensor
from airflow.operators.trigger_dagrun import TriggerDagRunOperator
import json
from include.iot_streaming.database_manager import db_manager

logger = logging.getLogger(__name__)

# ===== Callback khi c√≥ message: TRIGGER DAG B ngay t·∫°i ƒë√¢y =====
def on_kafka_event(event=None, **kwargs):
    """
    V·ªõi AwaitMessageTriggerFunctionSensor:
    - `event` l√† GI√Å TR·ªä do apply_function tr·∫£ v·ªÅ (payload chu·ªói).
    - Sensor s·∫Ω ti·∫øp t·ª•c DEFER ƒë·ªÉ nghe ti·∫øp; v√¨ v·∫≠y ta TRIGGER DAG B NGAY T·∫†I ƒê√ÇY.
    """
    dag = kwargs.get("dag")
    ts = kwargs.get("ts")  # Airflow logical time
    ti = kwargs.get("ti")

    payload = event  # ƒë√£ l√† string do extract_value tr·∫£ v·ªÅ
    logger.info("üì• Kafka payload (from apply_function): %s", payload)
    
    # Parse JSON payload
    try:
        data = json.loads(payload) if payload else {}
    except Exception:
        logger.warning("Payload is not valid JSON; skipping parse")
        return True

    # (tu·ª≥ ch·ªçn) l∆∞u ƒë·ªÉ debug
    if ti:
        ti.xcom_push(key="kafka_message", value=payload)
        ti.xcom_push(key="trigger_time", value=ts or datetime.now(timezone.utc).isoformat())

    # Handle both single object and array of objects
    if isinstance(data, list):
        # Array of objects - process each one
        logger.info(f"üìä Processing {len(data)} records from array")
        for i, item in enumerate(data):
            try:
                _process_single_record(item)
            except Exception as e:
                logger.error(f"‚ùå Error processing record {i}: {e}")
    else:
        # Single object
        try:
            _process_single_record(data)
        except Exception as e:
            logger.error(f"‚ùå Error processing single record: {e}")

    # T·∫°o task TriggerDagRunOperator runtime + execute ngay
    unique_suffix = datetime.now(timezone.utc).strftime("%Y%m%dT%H%M%S%f")
    trigger_task = TriggerDagRunOperator(
        task_id=f"trigger_ml_pipeline_inline_{unique_suffix}",
        trigger_dag_id="streaming_data_processor",   # <-- ƒë·ªïi n·∫øu DAG B t√™n kh√°c
        conf={
            "kafka_msg": payload,
            "trigger_time": ts or datetime.now(timezone.utc).isoformat(),
            "source_dag": "streaming_process_dag",
        },
        wait_for_completion=False,
        reset_dag_run=True,
    )

    trigger_task.dag = dag
    trigger_task.execute(context=kwargs)

    logger.info("‚úÖ Triggered external DAG 'streaming_data_processor' with conf.")
    return True  # callback xong; sensor quay l·∫°i DEFER ƒë·ªÉ nghe ti·∫øp

def _process_single_record(data):
    """Process a single record"""
    # Map fields for DB insert
    station_id = int(data.get("station_id")) if data.get("station_id") is not None else 0
    ph = float(data.get("ph")) if data.get("ph") is not None else None
    temperature = float(data.get("temperature")) if data.get("temperature") is not None else None
    do_val = float(data.get("do")) if data.get("do") is not None else None

    mt = data.get("measurement_time")
    if isinstance(mt, str):
        # Handle ISO strings including with timezone or trailing Z
        mt_str = mt[:-1] if mt.endswith("Z") else mt
        try:
            measurement_time = datetime.fromisoformat(mt_str)
        except ValueError:
            measurement_time = datetime.now(timezone.utc)
    elif isinstance(mt, (int, float)):
        measurement_time = datetime.fromtimestamp(mt, tz=timezone.utc)
    else:
        measurement_time = datetime.now(timezone.utc)

    raw_record = {
        "station_id": station_id,
        "measurement_time": measurement_time,
        "ph": ph if ph is not None else 7.0,
        "temperature": temperature if temperature is not None else 25.0,
        "do": do_val if do_val is not None else 8.0,
    }

    if db_manager.insert_raw_data(raw_record):
        logger.info("‚úÖ Raw data inserted for station %s at %s", station_id, measurement_time)
    else:
        logger.warning("‚ö†Ô∏è Failed to insert raw data for station %s", station_id)

# ===== Tham s·ªë DAG =====
default_args = {
    "owner": "airflow",
    "depends_on_past": False,
    "email_on_failure": False,
    "email_on_retry": False,
}

@dag(
    default_args=default_args,
    description="Continuous IoT streaming (Kafka -> trigger external DAG inline)",
    start_date=datetime(2024, 1, 1),
    schedule=None,
    catchup=False,
    tags=["iot_pipeline_continuous"],
    max_active_runs=1,
)
def streaming_process_dag():
    """
    Flow:
    - Sensor l·∫Øng nghe Kafka li√™n t·ª•c (DEFERRED).
    - M·ªói khi c√≥ message: iothandlers.kafka.extract_value tr·∫£ payload -> on_kafka_event ƒë∆∞·ª£c g·ªçi -> trigger DAG B.
    - Kh√¥ng c√≥ downstream tasks v√¨ sensor kh√¥ng success; n√≥ s·∫Ω quay l·∫°i nghe ti·∫øp.
    """
    wait_for_kafka = AwaitMessageTriggerFunctionSensor(
        task_id="wait_for_kafka",
        kafka_config_id="kafka_default",                # ƒë·∫£m b·∫£o Connection n√†y t·ªìn t·∫°i
        topics=["water-quality-data"],                  # ƒë·ªïi theo topic c·ªßa b·∫°n
        apply_function="include.iot_streaming.kafka_handlers.extract_value",  # <‚Äî STRING dotted-path (import ƒë∆∞·ª£c ·ªü Triggerer)
        event_triggered_function=on_kafka_event,           # <‚Äî CALLABLE
        poll_timeout=1,
        poll_interval=10,
    )

    # Kh√¥ng c·∫ßn return task kh√°c; callback t·ª± trigger DAG B

# Instantiate DAG
streaming_process_dag()
