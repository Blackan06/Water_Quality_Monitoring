from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import logging
import requests
import os
from openai import OpenAI
from airflow.models import Variable
from airflow.decorators import dag, task
from pendulum import datetime
from include.iot_streaming.database_manager import db_manager
from include.iot_streaming.pipeline_processor import PipelineProcessor
from include.iot_streaming.prediction_service import PredictionService

try:
    openai_key = os.getenv("OPENAI_API_KEY") or Variable.get("openai_api_key", default_var=None)
except Exception:
    openai_key = None
# Cáº¥u hÃ¬nh logging
logger = logging.getLogger(__name__)

# Default arguments cho DAG
default_args = {
    'owner': 'water_quality_team',
    'depends_on_past': False,
    'start_date': datetime(2025, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

# Äá»‹nh nghÄ©a DAG
 
# ============================================================================
# TASK FUNCTIONS - CHá»ˆ ORCHESTRATION, KHÃ”NG BUSINESS LOGIC
# ============================================================================

def initialize_database_connection(**context):
    """Khá»Ÿi táº¡o káº¿t ná»‘i database - chá»‰ orchestration"""
    logger.info("Initializing database connection...")
    
    try:
       
        status = db_manager.check_database_status()
        
        logger.info("âœ… Database connection initialized successfully")
        return status
        
    except Exception as e:
        logger.error(f"âŒ Error initializing database connection: {e}")
        return f"Error: {e}"

def process_streaming_data(**context):
    """Orchestrate streaming data processing - láº¥y raw data vÃ  lÆ°u vÃ o processed table"""
    logger.info("Starting streaming data processing orchestration")
    
    try:
      
      
        
        # Láº¥y raw data tá»« database
        pipeline_processor = PipelineProcessor()
        raw_data = pipeline_processor.get_unprocessed_raw_data()
        
        if not raw_data:
            logger.info("No unprocessed raw data found")
            context['task_instance'].xcom_push(key='unprocessed_count', value=0)
            context['task_instance'].xcom_push(key='stations_with_models', value=[])
            context['task_instance'].xcom_push(key='processed_count', value=0)
            return "No unprocessed raw data found"
        
        # Process raw data thÃ nh processed data vÃ  lÆ°u vÃ o báº£ng processed_water_quality_data
        processed_count = pipeline_processor.process_raw_data(raw_data)
        
        logger.info(f"âœ… Successfully processed {processed_count} raw records into processed_water_quality_data")
        
        # Láº¥y thÃ´ng tin sau khi process
        unprocessed_count = db_manager.get_unprocessed_raw_count()
        stations_with_models = db_manager.get_stations_with_models()
        
        # LÆ°u thÃ´ng tin cho cÃ¡c task tiáº¿p theo
        context['task_instance'].xcom_push(key='unprocessed_count', value=unprocessed_count)
        context['task_instance'].xcom_push(key='stations_with_models', value=stations_with_models)
        context['task_instance'].xcom_push(key='processed_count', value=processed_count)
        
        logger.info(f"ðŸ“Š Summary: Processed {processed_count} records, remaining {unprocessed_count} unprocessed, {len(stations_with_models)} stations with models")
        return f"âœ… Processed {processed_count} raw records into processed table, {len(stations_with_models)} stations available"
        
    except Exception as e:
        logger.error(f"âŒ Error processing streaming data: {e}")
        return f"Error: {e}"

def predict_existing_stations(**context):
    """Orchestrate predictions for existing stations - láº¥y records is_processed=FALSE, predict, update"""
    logger.info("Starting prediction orchestration")
    
    try:
     

        # Láº¥y stations cÃ³ unprocessed data
        conn = db_manager.get_connection()
        if not conn:
            logger.error("Cannot connect to database")
            return "Cannot connect to database"
        
        cur = conn.cursor()
        cur.execute("""
            SELECT DISTINCT station_id 
            FROM raw_sensor_data 
            WHERE is_processed = FALSE
        """)
        
        stations_with_data = [row[0] for row in cur.fetchall()]
        cur.close()
        conn.close()
        
        if not stations_with_data:
            logger.info("No stations with unprocessed data found")
            return "No stations with unprocessed data found"
        
        logger.info(f"Found {len(stations_with_data)} stations with unprocessed data")
        
        # Gá»i prediction service
        prediction_service = PredictionService()
        prediction_results = prediction_service.process_stations_predictions(stations_with_data)
        
        # Update is_processed = TRUE cho records Ä‘Ã£ predict
        if prediction_results:
            conn = db_manager.get_connection()
            if conn:
                cur = conn.cursor()
                for station_id in stations_with_data:
                    cur.execute("""
                        UPDATE raw_sensor_data 
                        SET is_processed = TRUE
                        WHERE station_id = %s AND is_processed = FALSE
                    """, (station_id,))
                
                conn.commit()
                cur.close()
                conn.close()
                logger.info(f"âœ… Updated is_processed = TRUE for {len(stations_with_data)} stations")
        
        # LÆ°u káº¿t quáº£
        if prediction_results is None:
            prediction_results = []
        
        context['task_instance'].xcom_push(key='prediction_results', value=prediction_results)
        
        successful_count = len([r for r in prediction_results if r and r.get('success')])
        total_count = len(prediction_results) if prediction_results else 0
        logger.info(f"Predictions completed: {successful_count}/{total_count} successful")
        
        return f"âœ… Predictions completed: {successful_count}/{total_count} successful, updated {len(stations_with_data)} stations"
        
    except Exception as e:
        logger.error(f"Error orchestrating predictions: {e}")
        return f"Error: {e}"

def update_database_metrics(**context):
    """Orchestrate database metrics update"""
    logger.info("Updating database metrics")
    
    try:
     
        
        # Gá»i service Ä‘á»ƒ cáº­p nháº­t metrics
        metrics_result = db_manager.update_metrics()
        
        logger.info("Database metrics updated successfully")
        return f"Metrics updated: {metrics_result}"
            
    except Exception as e:
        logger.error(f"Error updating database metrics: {e}")
        return f"Error: {e}"

def generate_alerts_and_notifications(**context):
    """Orchestrate alerts generation and push notifications"""
    logger.info("Generating alerts and sending push notifications")
    try:
        
        # Láº¥y káº¿t quáº£ predictions
        prediction_results = context['task_instance'].xcom_pull(
            task_ids='predict_existing_stations', key='prediction_results'
        )
        
        if not prediction_results:
            logger.info("No prediction results for alerts")
            context['task_instance'].xcom_push(key='alerts_result', value='No prediction results for alerts')
            context['task_instance'].xcom_push(key='notifications_sent', value=0)
            return "No prediction results for alerts"
        
        # Gá»i alert service
        prediction_service = PredictionService()
        alerts_result = prediction_service.generate_alerts(prediction_results)
        
        # Gá»­i push notifications cho tá»«ng station
        notifications_sent = 0
        
        for prediction in prediction_results:
            if not prediction or not prediction.get('success'):
                continue
                
            station_id = prediction.get('station_id')
            future_predictions = prediction.get('future_predictions', {})
            
            if not future_predictions:
                continue
            
            # Láº¥y dá»± Ä‘oÃ¡n cho thÃ¡ng Ä‘áº§u tiÃªn (1 thÃ¡ng)
            first_prediction = future_predictions.get('1month', {})
            if not first_prediction:
                continue
                
            wqi_prediction = first_prediction.get('wqi_prediction', 50.0)
            confidence_score = first_prediction.get('confidence_score', 0.5)
            
            # Láº¥y dá»¯ liá»‡u hiá»‡n táº¡i tá»« database Ä‘á»ƒ phÃ¢n tÃ­ch
           
            conn = db_manager.get_connection()
            current_data = None
            
            if conn:
                cur = conn.cursor()
              
                # Náº¿u khÃ´ng cÃ³, thá»­ tÃ¬m trong raw_sensor_data
                if not current_data:
                    cur.execute("""
                        SELECT ph, temperature, "do" 
                        FROM raw_sensor_data 
                        WHERE station_id = %s 
                        ORDER BY measurement_time DESC 
                        LIMIT 1
                    """, (station_id,))
                    current_data = cur.fetchone()
                
                cur.close()
                conn.close()
                
                if current_data:
                    ph, temperature, do = current_data
                    logger.info(f"ðŸ“Š Found current data for station {station_id}: pH={ph}, Temp={temperature}, DO={do}")
                else:
                    logger.warning(f"âš ï¸ No current data found for station {station_id} in both processed and raw tables")
                    # Sá»­ dá»¥ng giÃ¡ trá»‹ máº·c Ä‘á»‹nh náº¿u khÃ´ng cÃ³ dá»¯ liá»‡u
                    ph, temperature, do = 7.0, 25.0, 8.0
                
                # PhÃ¢n tÃ­ch cháº¥t lÆ°á»£ng nÆ°á»›c
                analysis = analyze_water_quality(wqi_prediction, ph, do, temperature)
                
                # XÃ¡c Ä‘á»‹nh status dá»±a trÃªn WQI
                status = "good" if wqi_prediction > 50 else "danger" 
                
                # Láº¥y tÃªn tráº¡m Ä‘á»ƒ Ä‘Æ°a vÃ o tiÃªu Ä‘á» thÃ´ng bÃ¡o
                station_name = None
                try:
                    conn2 = db_manager.get_connection()
                    if conn2:
                        cur2 = conn2.cursor()
                        cur2.execute("""
                            SELECT station_name FROM monitoring_stations WHERE station_id = %s
                        """, (station_id,))
                        row = cur2.fetchone()
                        if row:
                            station_name = row[0]
                        cur2.close()
                        conn2.close()
                except Exception:
                    station_name = None

                # Gá»­i thÃ´ng bÃ¡o
                account_id = 3  # Sá»­ dá»¥ng station_id lÃ m account_id
                message = analysis
                title = f"Káº¿t quáº£ WQI - {station_name}" if station_name else "Káº¿t quáº£ WQI"
                
                if push_notification(account_id, title, message, status):
                    notifications_sent += 1
                    logger.info(f"âœ… Notification sent for station {station_id}: WQI={wqi_prediction}, Status={status}")
                else:
                    logger.warning(f"âš ï¸ Failed to send notification for station {station_id}")
            else:
                logger.error(f"âŒ Cannot connect to database for station {station_id}")
        
        context['task_instance'].xcom_push(key='alerts_result', value=alerts_result)
        context['task_instance'].xcom_push(key='notifications_sent', value=notifications_sent)
        
        logger.info(f"âœ… Alerts generated and {notifications_sent} notifications sent successfully")
        return f"âœ… Alerts generated: {alerts_result}, Notifications sent: {notifications_sent}"
                        
    except Exception as e:
        logger.error(f"Error generating alerts and notifications: {e}")
        return f"Error: {e}"

def mark_records_as_processed(**context):
    """Orchestrate marking raw records as processed (already done in predict_existing_stations)"""
    logger.info("Checking processed records status")
    
    try:
        # Láº¥y thÃ´ng tin tá»« task trÆ°á»›c
        prediction_results = context['task_instance'].xcom_pull(
            task_ids='predict_existing_stations', key='prediction_results'
        )
        
        if not prediction_results:
            logger.info("No prediction results found")
            context['task_instance'].xcom_push(key='processed_count', value=0)
            return "No prediction results found"
        
        successful_count = len([r for r in prediction_results if r and r.get('success')])
        context['task_instance'].xcom_push(key='processed_count', value=successful_count)
        
        logger.info(f"Confirmed {successful_count} predictions were successful")
        return f"Confirmed {successful_count} predictions were successful"
        
    except Exception as e:
        logger.error(f"Error checking processed records: {e}")
        return f"Error: {e}"

def summarize_pipeline_execution(**context):
    """Orchestrate pipeline summary"""
    logger.info("Summarizing pipeline execution")
    
    try:
        
        # Láº¥y dá»¯ liá»‡u tá»« cÃ¡c task trÆ°á»›c
        prediction_results = context['task_instance'].xcom_pull(
            task_ids='predict_existing_stations', key='prediction_results'
        )
        alerts_result = context['task_instance'].xcom_pull(
            task_ids='generate_alerts_and_notifications', key='alerts_result'
        )
        processed_count = context['task_instance'].xcom_pull(
            task_ids='mark_records_as_processed', key='processed_count'
        )
        notifications_sent = context['task_instance'].xcom_pull(
            task_ids='generate_alerts_and_notifications', key='notifications_sent'
        )
        
        # Handle None values from XCom
        if prediction_results is None:
            prediction_results = []
        if alerts_result is None:
            alerts_result = ''
        if processed_count is None:
            processed_count = 0
        if notifications_sent is None:
            notifications_sent = 0
        
        # Gá»i summary service
        pipeline_processor = PipelineProcessor()
        summary = pipeline_processor.create_summary({
            'prediction_results': prediction_results,
            'alerts_result': alerts_result,
            'processed_count': processed_count,
            'notifications_sent': notifications_sent
        })
        
        context['task_instance'].xcom_push(key='pipeline_summary', value=summary)
        
        logger.info("Pipeline summary created successfully")
        return f"Pipeline summary: {summary}"
        
    except Exception as e:
        logger.error(f"Error summarizing pipeline: {e}")
        return f"Error: {e}"

def push_notification(account_id, title, message, status):
    """Gá»­i push notification Ä‘áº¿n API"""
    url = "https://datamanagerment.anhkiet.xyz/notifications/send-notification"
    payload = {
        "account_id": str(account_id),
        "title": title,
        "message": message,
        "status": status
    }
    try:
        response = requests.post(url, json=payload, timeout=10)
        if response.status_code == 200:
            logger.info("Push notification sent successfully.")
            return True
        else:
            logger.error(f"Push notification failed: {response.text}")
            return False
    except Exception as e:
        logger.error(f"Error sending push notification: {e}")
        return False

def analyze_water_quality(wqi, ph, do, temperature):
    """PhÃ¢n tÃ­ch cháº¥t lÆ°á»£ng nÆ°á»›c vÃ  Ä‘Æ°a ra Ä‘Ã¡nh giÃ¡"""
    try:
        
        # Khá»Ÿi táº¡o OpenAI client
        client = OpenAI(api_key=os.getenv('OPENAI_API_KEY', openai_key))
        
        prompt = (
            "ÄÃ¢y lÃ  nÆ°á»›c nuÃ´i cÃ¡. "
            f"WQI dá»± Ä‘oÃ¡n: {wqi}, pH={ph}, DO={do} mg/L, nhiá»‡t Ä‘á»™={temperature}Â°C. "
            "HÃ£y Ä‘Ã¡nh giÃ¡ khÃ¡ch quan vÃ  Ä‘á» xuáº¥t biá»‡n phÃ¡p, chá»‰ viáº¿t Ä‘Ãºng 4 cÃ¢u, "
            "má»—i cÃ¢u káº¿t thÃºc báº±ng dáº¥u cháº¥m, Ä‘áº§y Ä‘á»§ nghÄ©a, khÃ´ng bá» thiáº¿u chá»¯, khÃ´ng xuá»‘ng dÃ²ng."
        )
        
        resp = client.chat.completions.create(
            model="gpt-4o-mini",  # Sá»­ dá»¥ng model má»›i hÆ¡n
            messages=[{"role": "user", "content": prompt}],
            max_tokens=150,
            temperature=0.5
        )
        
        text = resp.choices[0].message.content.strip()
        # Gá»™p vÃ  Ä‘áº£m báº£o khÃ´ng cÃ³ newline
        text = " ".join(text.splitlines())
        return f"WQI thÃ¡ng sau: {wqi}. {text}"
        
    except Exception as e:
        logger.error(f"Error analyzing water quality: {e}")
        return f"WQI thÃ¡ng sau: {wqi}. KhÃ´ng thá»ƒ phÃ¢n tÃ­ch chi tiáº¿t do lá»—i há»‡ thá»‘ng."

@dag(
    'streaming_data_processor',
    default_args=default_args,
    description='Orchestration pipeline for streaming water quality data processing',
    catchup=False,
    tags=['water-quality', 'streaming', 'orchestration', 'postgresql']
)
def streaming_data_processor():

    # ============================================================================
    # TASK DEFINITIONS
    # ============================================================================

    # Task Ä‘á»ƒ khá»Ÿi táº¡o database connection
    initialize_db_task = PythonOperator(
        task_id='initialize_database_connection',
        python_callable=initialize_database_connection,
    )

    # Task Ä‘á»ƒ xá»­ lÃ½ streaming data (Ä‘Ã£ bá», chuyá»ƒn logic vÃ o predict_existing_stations)

    # Task Ä‘á»ƒ dá»± Ä‘oÃ¡n cho existing stations
    predict_existing_task = PythonOperator(
        task_id='predict_existing_stations',
        python_callable=predict_existing_stations,
    )

    # Task Ä‘á»ƒ cáº­p nháº­t database metrics
    update_metrics_task = PythonOperator(
        task_id='update_database_metrics',
        python_callable=update_database_metrics,
    )

    # Task Ä‘á»ƒ táº¡o alerts vÃ  notifications
    alerts_task = PythonOperator(
        task_id='generate_alerts_and_notifications',
        python_callable=generate_alerts_and_notifications,
    )

    # Task Ä‘á»ƒ mark records as processed
    mark_processed_task = PythonOperator(
        task_id='mark_records_as_processed',
        python_callable=mark_records_as_processed,
    )

    # Task Ä‘á»ƒ tÃ³m táº¯t pipeline execution
    summarize_task = PythonOperator(
        task_id='summarize_pipeline_execution',
        python_callable=summarize_pipeline_execution,
    )

    # ============================================================================
    # DAG DEPENDENCIES
    # ============================================================================

    # Äá»‹nh nghÄ©a dependencies - chá»‰ orchestration
    initialize_db_task >> predict_existing_task
    predict_existing_task >> update_metrics_task >> alerts_task >> mark_processed_task >> summarize_task 

streaming_data_processor()