#!/usr/bin/env python3
import os
import math
import logging
import numpy as np
from datetime import datetime

from pyspark.sql import SparkSession, functions as F
from pyspark.sql.window import Window
from pyspark.ml import Pipeline
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml.feature import (
    StringIndexer, OneHotEncoder, VectorAssembler, Imputer
)
from pyspark.ml.regression import RandomForestRegressor
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.stat import Correlation

import xgboost as xgb
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from sklearn.model_selection import TimeSeriesSplit

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def create_spark_session():
    """T·∫°o SparkSession v·ªõi nhi·ªÅu c·∫•u h√¨nh fallback."""
    builder = SparkSession.builder.appName("Comprehensive-Monthly-TimeSeries-WQI-Training")
    for drv_mem, ex_mem, max_res in [("2g","2g","1g"), ("1g","1g","512m"), ("512m","512m",None)]:
        b = builder.config("spark.driver.memory", drv_mem)\
                   .config("spark.executor.memory", ex_mem)\
                   .master("local[*]")
        if max_res:
            b = b.config("spark.driver.maxResultSize", max_res)
        try:
            spark = b.getOrCreate()
            logger.info(f"‚úÖ SparkSession v·ªõi driver={drv_mem}, executor={ex_mem}")
            return spark
        except Exception as e:
            logger.warning(f"‚ùå SparkSession th·∫•t b·∫°i (driver={drv_mem}, executor={ex_mem}): {e}")
    raise RuntimeError("üö´ Kh√¥ng th·ªÉ kh·ªüi t·∫°o SparkSession")


def load_data_from_postgres(spark):
    """Load d·ªØ li·ªáu WQI l·ªãch s·ª≠ t·ª´ PostgreSQL."""
    try:
        df = (
            spark.read.format("jdbc")
            .option("url", "jdbc:postgresql://194.238.16.14:5432/wqi_db")
            .option("dbtable", "public.historical_wqi_data")
            .option("user", "postgres")
            .option("password", "postgres1234")
            .option("driver", "org.postgresql.Driver")
            .load()
        )
        logger.info(f"‚úÖ ƒê√£ load {df.count()} d√≤ng t·ª´ Postgres")
        return df
    except Exception as e:
        logger.error(f"‚ùå Load t·ª´ Postgres th·∫•t b·∫°i: {e}")
        raise


def clean_time_series_monthly(df, for_ml=True):
    """
    L√†m s·∫°ch d·ªØ li·ªáu theo th√°ng cho t·ª´ng station:
    - √âp ki·ªÉu, b·ªè tr√πng (station_id, measurement_date)
    - R√†ng bu·ªôc mi·ªÅn gi√° tr·ªã v·∫≠t l√Ω c∆° b·∫£n
    - Resample v·ªÅ th√°ng (mean)
    - B·ªï sung ƒë·ªß m·ªëc th√°ng (l·ªãch li√™n t·ª•c) cho m·ªói station
    - Forward-fill (cho ML) ho·∫∑c Forward-fill + Back-fill (cho EDA)
    """
    logger.info("üßπ Clean time-series (monthly) ...")

    # 0) Chu·∫©n c·ªôt & b·ªè tr√πng
    df = (df
        .withColumn("measurement_date", F.to_timestamp("measurement_date"))
        .dropDuplicates(["station_id", "measurement_date"])
        .filter(F.col("measurement_date").isNotNull())
    )

    # 1) R√†ng bu·ªôc mi·ªÅn gi√° tr·ªã (tu·ª≥ domain, c√≥ th·ªÉ n·ªõi/si·∫øt)
    df = (df
        .filter((F.col("wqi").isNull()) | ((F.col("wqi") >= 0) & (F.col("wqi") <= 200)))
        .filter((F.col("ph").isNull())  | ((F.col("ph") >= 0) & (F.col("ph") <= 14)))
        .filter((F.col("temperature").isNull()) | ((F.col("temperature") > -5) & (F.col("temperature") < 80)))
        .filter((F.col("do").isNull())  | ((F.col("do") >= 0) & (F.col("do") <= 25)))
    )

    # 2) Resample v·ªÅ TH√ÅNG (n·∫øu ƒë√£ l√† th√°ng th√¨ v·∫´n an to√†n: groupBy s·∫Ω g·ªôp)
    dfm = (df
        .withColumn("month_date", F.date_trunc("month", F.col("measurement_date")))
        .groupBy("station_id", "month_date")
        .agg(
            F.avg("wqi").alias("wqi"),
            F.avg("ph").alias("ph"),
            F.avg("temperature").alias("temperature"),
            F.avg("do").alias("do")
        )
    )

    # 3) Sinh l·ªãch th√°ng li√™n t·ª•c cho t·ª´ng station
    minmax = (dfm.groupBy("station_id")
        .agg(F.min("month_date").alias("min_d"), F.max("month_date").alias("max_d"))
    )
    cal = (minmax
        .select(
            "station_id",
            F.sequence(F.col("min_d"), F.col("max_d"), F.expr("INTERVAL 1 MONTH")).alias("month_seq")
        )
        .withColumn("month_date", F.explode(F.col("month_seq")))
        .select("station_id", "month_date")
    )

    # 4) Join l·ªãch v·ªõi d·ªØ li·ªáu ‚Üí c√≥ th·ªÉ null ·ªü th√°ng thi·∫øu
    df_full = cal.join(dfm, ["station_id", "month_date"], "left")

    # 5) Forward-fill (OK cho ML)
    win_ff = (Window.partitionBy("station_id")
        .orderBy("month_date")
        .rowsBetween(Window.unboundedPreceding, 0)
    )
    for c in ["wqi", "ph", "temperature", "do"]:
        df_full = df_full.withColumn(c, F.last(F.col(c), ignorenulls=True).over(win_ff))

    if not for_ml:
        # Back-fill (ch·ªâ d√πng cho EDA, KH√îNG d√πng cho training)
        win_bf = (Window.partitionBy("station_id")
            .orderBy(F.col("month_date").desc())
            .rowsBetween(Window.unboundedPreceding, 0)
        )
        for c in ["wqi", "ph", "temperature", "do"]:
            df_full = df_full.withColumn(c, F.coalesce(F.col(c), F.last(F.col(c), ignorenulls=True).over(win_bf)))

    # 6) Tr·∫£ v·ªÅ ƒë√∫ng schema ng√†y th√°ng b·∫°n ƒëang d√πng
    df_clean = (df_full
        .withColumnRenamed("month_date", "measurement_date")
        .orderBy("station_id", "measurement_date")
    )

    logger.info("‚úÖ Done clean monthly time-series.")
    return df_clean


def add_label_for_horizon(df, H: int):
    """T·∫°o nh√£n d·ª± b√°o WQI ·ªü t∆∞∆°ng lai H th√°ng: label = WQI_{t+H}."""
    win = Window.partitionBy("station_id").orderBy("measurement_date")
    return df.withColumn("label", F.lead("wqi", H).over(win)).filter(F.col("label").isNotNull())


def add_time_folds(df, k: int = 3):
    """G√°n fold theo m·ªëc th·ªùi gian ƒë·ªÉ CrossValidator kh√¥ng tr·ªôn ng·∫´u nhi√™n."""
    # ch·ªâ c·∫ßn theo tr·ª•c th·ªùi gian to√†n c·ª•c l√† ƒë·ªß
    df = df.withColumn("foldCol", F.ntile(int(k)).over(Window.orderBy("measurement_date")))
    return df
    # NOTE: V·ªõi d·ªØ li·ªáu √≠t tr·∫°m (3 stations) th√¨ fold theo th·ªùi gian to√†n c·ª•c l√† OK
    # N·∫øu c√≥ nhi·ªÅu tr·∫°m v√† ph√¢n b·ªë kh√°c nhau, c√≥ th·ªÉ c√¢n nh·∫Øc fold theo (station_id, time)


def split_train_test_monthly(df, test_months=12):
    """
    Chia theo m·ªëc th·ªùi gian: l·∫•y test_months th√°ng cu·ªëi l√†m test, c√≤n l·∫°i l√† train.
    FIXED: S·ª≠ d·ª•ng add_months thay v√¨ F.expr
    """
    split_dt = df.select(F.add_months(F.max("measurement_date"), -int(test_months)).alias("dt")).first()["dt"]
    train = df.filter(F.col("measurement_date") <= F.lit(split_dt))
    test  = df.filter(F.col("measurement_date")  > F.lit(split_dt))
    logger.info(f"‚úÖ Split (train/test): train={train.count()}, test={test.count()}, split_dt={split_dt}")
    return train, test


def drop_cold_start_rows(df, required_lags=("wqi_lag_12", "ph_lag_12", "temp_lag_12", "do_lag_12")):
    """
    Lo·∫°i c√°c d√≤ng ch∆∞a ƒë·ªß qu√° kh·ª© cho c√°c lag l·ªõn nh·∫•t (tr√°nh Imputer b√π mean g√¢y nhi·ªÖu).
    G·ªçi sau b∆∞·ªõc feature_engineering.
    """
    cond = None
    for c in required_lags:
        expr = F.col(c).isNotNull()
        cond = expr if cond is None else (cond & expr)
    before = df.count()
    df2 = df.filter(cond)
    after = df2.count()
    logger.info(f"üßä Drop cold-start: {before-after} rows removed (remain={after})")
    return df2


def compute_correlations(df):
    """
    Tr·∫£ v·ªÅ:
      - corr_global: dict Pearson corr gi·ªØa WQI v√† DO/pH/Temp to√†n c·ª•c
      - corr_by_station: DataFrame corr theo t·ª´ng station
      - corr_matrix: numpy array 4x4 cho [wqi, do, ph, temperature]
    """
    # --- Global corr (WQI v·ªõi DO/pH/Temp)
    row = (df.select(
        F.corr("wqi","do").alias("corr_wqi_do"),
        F.corr("wqi","ph").alias("corr_wqi_ph"),
        F.corr("wqi","temperature").alias("corr_wqi_temp")
    ).first())
    corr_global = {
        "wqi~do": float(row["corr_wqi_do"]) if row["corr_wqi_do"] is not None else None,
        "wqi~ph": float(row["corr_wqi_ph"]) if row["corr_wqi_ph"] is not None else None,
        "wqi~temp": float(row["corr_wqi_temp"]) if row["corr_wqi_temp"] is not None else None,
    }

    # --- Corr theo tr·∫°m
    corr_by_station = (df.groupBy("station_id").agg(
        F.corr("wqi","do").alias("corr_wqi_do"),
        F.corr("wqi","ph").alias("corr_wqi_ph"),
        F.corr("wqi","temperature").alias("corr_wqi_temp")
    ))

    # --- Correlation matrix cho [wqi, do, ph, temperature]
    cols = ["wqi","do","ph","temperature"]
    vec_df = VectorAssembler(inputCols=cols, outputCol="vec")\
        .transform(df.select(*cols).na.drop())
    mat = Correlation.corr(vec_df, "vec", "pearson").head()[0].toArray()  # numpy matrix 4x4

    return corr_global, corr_by_station, mat


def analyze_seasonality(df):
    """
    T√≠nh seasonality theo th√°ng:
      - wqi_mean_by_month: trung b√¨nh WQI theo th√°ng (1..12)
      - seasonal_index: wqi_mean_by_month / overall_mean
      - wqi_mean_by_station_month: trung b√¨nh theo tr·∫°m & th√°ng
    """
    dfm = df.withColumn("month", F.month("measurement_date"))

    wqi_mean_by_month = (dfm.groupBy("month")
        .agg(F.avg("wqi").alias("wqi_mean"), F.stddev("wqi").alias("wqi_std"), F.count("*").alias("n")))

    overall_mean = df.agg(F.avg("wqi").alias("m")).first()["m"]
    seasonal_index = wqi_mean_by_month.withColumn(
        "seasonal_index", F.col("wqi_mean") / F.lit(overall_mean)
    )

    wqi_mean_by_station_month = (dfm.groupBy("station_id","month")
        .agg(F.avg("wqi").alias("wqi_mean")))

    return wqi_mean_by_month.orderBy("month"), seasonal_index.orderBy("month"), wqi_mean_by_station_month


def add_wqi_moving_averages(df):
    """
    Th√™m c√°c c·ªôt wqi_ma_3, wqi_ma_6, wqi_ma_12 (moving average theo tr·∫°m).
    C·ª≠a s·ªï: t·ª´ qu√° kh·ª© t·ªõi hi·ªán t·∫°i (an to√†n cho forecast t∆∞∆°ng lai).
    """
    win = (Window.partitionBy("station_id")
           .orderBy("measurement_date"))

    for w in [3,6,12]:
        wwin = win.rowsBetween(-w+1, 0)  # include current t
        df = df.withColumn(f"wqi_ma_{w}", F.avg("wqi").over(wwin))
    return df


def comprehensive_feature_engineering(df):
    """T·∫°o feature: time, cyclical, lag, rolling, interaction, trend (FIXED DATA LEAKAGE)."""
    logger.info("üîß Feature engineering...")
    win = Window.partitionBy("station_id").orderBy("measurement_date")

    # 1) Time features
    df = df.withColumn("year", F.year("measurement_date")) \
           .withColumn("month", F.month("measurement_date")) \
           .withColumn("day_of_year", F.dayofyear("measurement_date")) \
           .withColumn("quarter", F.quarter("measurement_date"))

    # 2) Cyclical encoding
    df = df.withColumn(
                "month_sin",
                F.sin(F.col("month") * F.lit(2 * math.pi) / F.lit(12))
           ).withColumn(
                "month_cos",
                F.cos(F.col("month") * F.lit(2 * math.pi) / F.lit(12))
           ).withColumn(
                "day_sin",
                F.sin(F.col("day_of_year") * F.lit(2 * math.pi) / F.lit(365.25))
           ).withColumn(
                "day_cos",
                F.cos(F.col("day_of_year") * F.lit(2 * math.pi) / F.lit(365.25))
           )

    # 3) Lag features (FIXED: ch·ªâ d√πng qu√° kh·ª©)
    for lag in [1, 2, 3, 6, 12]:
        df = df.withColumn(f"wqi_lag_{lag}",  F.lag("wqi", lag).over(win)) \
               .withColumn(f"ph_lag_{lag}",   F.lag("ph", lag).over(win)) \
               .withColumn(f"temp_lag_{lag}", F.lag("temperature", lag).over(win)) \
               .withColumn(f"do_lag_{lag}",   F.lag("do", lag).over(win))

    # 4) Rolling window stats (FIXED: ch·ªâ d√πng qu√° kh·ª©)
    for w in [3, 6, 12]:
        wwin = win.rowsBetween(-w + 1, 0)  # ch·ªâ qu√° kh·ª© ƒë·∫øn hi·ªán t·∫°i
        df = df.withColumn(f"wqi_ma_{w}",  F.avg("wqi").over(wwin)) \
               .withColumn(f"wqi_std_{w}", F.stddev("wqi").over(wwin)) \
               .withColumn(f"ph_ma_{w}",   F.avg("ph").over(wwin)) \
               .withColumn(f"temp_ma_{w}", F.avg("temperature").over(wwin)) \
               .withColumn(f"do_ma_{w}",   F.avg("do").over(wwin))

    # 5) Rate-of-change (safe - tr√°nh chia cho 0)
    for lag in [1, 3, 6]:
        df = df.withColumn(
            f"wqi_roc_{lag}",
            F.when(
                (F.col(f"wqi_lag_{lag}").isNotNull()) & (F.col(f"wqi_lag_{lag}") != 0),
                (F.col("wqi") - F.col(f"wqi_lag_{lag}")) / F.col(f"wqi_lag_{lag}")
            ).otherwise(F.lit(0.0))
        )

    # 6) Interaction features
    df = df.withColumn("ph_temp_i", F.col("ph") * F.col("temperature")) \
           .withColumn("ph_do_i",   F.col("ph") * F.col("do")) \
           .withColumn("temp_do_i", F.col("temperature") * F.col("do"))

    # 7) Station/global trends (FIXED: ch·ªâ d√πng qu√° kh·ª©)
    # Station statistics: expanding window (ch·ªâ qu√° kh·ª© ƒë·∫øn hi·ªán t·∫°i)
    wstat = Window.partitionBy("station_id").orderBy("measurement_date")\
                  .rowsBetween(Window.unboundedPreceding, 0)  # ch·ªâ qu√° kh·ª© ƒë·∫øn hi·ªán t·∫°i t
    df = df.withColumn("station_avg_wqi", F.avg("wqi").over(wstat)) \
           .withColumn("station_std_wqi", F.stddev("wqi").over(wstat))

    # Global trends: rolling window (ch·ªâ qu√° kh·ª©)
    gwin = Window.orderBy("measurement_date").rowsBetween(-12, 0)
    df = df.withColumn("global_wqi_ma12", F.avg("wqi").over(gwin)) \
           .withColumn("global_trend", F.col("wqi") - F.col("global_wqi_ma12"))

    return df


def create_comprehensive_feature_pipeline():
    """X√¢y pipeline: station encoding, Imputer, VectorAssembler."""
    logger.info("üîß Build feature pipeline with Imputer...")
    idx = StringIndexer(inputCol="station_id", outputCol="station_idx", handleInvalid="keep")
    enc = OneHotEncoder(inputCol="station_idx", outputCol="station_ohe")

    feature_cols = [
        "ph","temperature","do","wqi",  # NOTE: wqi hi·ªán t·∫°i - h·ª£p l·ªá cho forecast n·∫øu c√≥ WQI t·∫°i th·ªùi ƒëi·ªÉm d·ª± b√°o
        "year","month","day_of_year","quarter",
        "month_sin","month_cos","day_sin","day_cos",
        *[f"wqi_lag_{l}" for l in [1,2,3,6,12]],
        *[f"ph_lag_{l}"  for l in [1,2,3,6,12]],
        *[f"temp_lag_{l}"for l in [1,2,3,6,12]],
        *[f"do_lag_{l}"  for l in [1,2,3,6,12]],
        *[f"{m}_{w}" for m in ["wqi_ma","wqi_std","ph_ma","temp_ma","do_ma"] for w in [3,6,12]],
        *[f"wqi_roc_{l}" for l in [1,3,6]],
        "ph_temp_i","ph_do_i","temp_do_i",
        "station_avg_wqi","station_std_wqi","global_wqi_ma12","global_trend"
    ]

    # PRODUCTION NOTE: N·∫øu kh√¥ng c√≥ WQI hi·ªán t·∫°i l√∫c d·ª± b√°o, b·ªè "wqi" kh·ªèi feature_cols
    # Gi·ªØ l·∫°i: wqi_lag_*, rolling features, pH/DO/Temp l√† ƒë·ªß

    imputer = Imputer(
        inputCols=feature_cols,
        outputCols=[f"{c}_imp" for c in feature_cols]
    ).setStrategy("mean")

    assembler = VectorAssembler(
        inputCols=[f"{c}_imp" for c in feature_cols] + ["station_ohe"],
        outputCol="features",
        handleInvalid="skip"
    )

    return Pipeline(stages=[idx, enc, imputer, assembler])


def train_with_optimized_tuning(train, test, spark, label_col="label"):
    """Train XGBoost (TS CV) + Spark RF (CV theo th·ªùi gian) cho FORECAST (d√πng label)."""
    pipeline = create_comprehensive_feature_pipeline()
    model_pipe = pipeline.fit(train)

    train_tf = model_pipe.transform(train)
    test_tf  = model_pipe.transform(test)

    def to_np(dfv, label_col=label_col):
        # S·∫Øp x·∫øp tuy·ªát ƒë·ªëi theo th·ªùi gian (v√† theo station ƒë·ªÉ ·ªïn ƒë·ªãnh)
        rows = (dfv
                .select("station_id", "measurement_date", "features", label_col)
                .orderBy("measurement_date", "station_id")
                .collect())
        X = np.array([r["features"].toArray() for r in rows])
        y = np.array([r[label_col] for r in rows])
        X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)
        return X, y

    X_train, y_train = to_np(train_tf, label_col)
    X_test,  y_test  = to_np(test_tf,  label_col)

    logger.info(f"üìä Training on {X_train.shape[1]} features (label={label_col})")

    # XGBoost + StandardScaler trong pipeline (tr√°nh CV leakage)
    from sklearn.pipeline import Pipeline as SKPipeline
    tscv = TimeSeriesSplit(n_splits=3)

    best_xgb, best_score, best_params = None, -np.inf, None
    grid = {
        'n_estimators':[100,200],'max_depth':[4,6],
        'learning_rate':[0.1,0.2],'subsample':[0.8],
        'reg_alpha':[0,0.1],'reg_lambda':[0,0.1]
    }

    for ne in grid['n_estimators']:
      for md in grid['max_depth']:
        for lr in grid['learning_rate']:
          for ss in grid['subsample']:
            for ra in grid['reg_alpha']:
              for rl in grid['reg_lambda']:
                pipe = SKPipeline([
                    ("scaler", StandardScaler()),
                    ("xgb", xgb.XGBRegressor(
                        n_estimators=ne, max_depth=md, learning_rate=lr,
                        subsample=ss, reg_alpha=ra, reg_lambda=rl,
                        random_state=42, n_jobs=-1))
                ])
                scores = []
                for ti, vi in tscv.split(X_train):
                    pipe.fit(X_train[ti], y_train[ti])
                    scores.append(r2_score(y_train[vi], pipe.predict(X_train[vi])))
                sc = float(np.mean(scores))
                if sc > best_score:
                    best_score, best_params, best_xgb = sc, dict(ne=ne,md=md,lr=lr,ss=ss,ra=ra,rl=rl), pipe
    best_xgb.fit(X_train, y_train)
    logger.info(f"üéØ Best XGB CV R2={best_score:.4f}, params={best_params}")

    # Spark RF v·ªõi label_col v√† fold theo th·ªùi gian
    # NOTE: Pipeline ƒë∆∞·ª£c fit 2 l·∫ßn (cho XGB v√† RF) - c√≥ th·ªÉ t·ªëi ∆∞u hi·ªáu nƒÉng n·∫øu c·∫ßn
    rf = RandomForestRegressor(labelCol=label_col, featuresCol="features", seed=42)
    paramGrid = ParamGridBuilder()\
        .addGrid(rf.maxDepth, [10,15])\
        .addGrid(rf.numTrees, [100,200])\
        .addGrid(rf.minInstancesPerNode, [1,2])\
        .build()
    cv = CrossValidator(
        estimator=Pipeline(stages=[pipeline, rf]),
        evaluator=RegressionEvaluator(labelCol=label_col, metricName="r2"),
        estimatorParamMaps=paramGrid,
        numFolds=3
    ).setFoldCol("foldCol")   # d√πng c·ªôt fold theo th·ªùi gian
    cvModel = cv.fit(train)   # train ƒë√£ c√≥ foldCol

    best_rf_pipeline = cvModel.bestModel
    y_xgb = best_xgb.predict(X_test)
    y_rf  = np.array(best_rf_pipeline.transform(test).select("prediction").collect()).flatten()
    y_en  = 0.6*y_xgb + 0.4*y_rf

    def calc(y, yh):
        return {'r2': r2_score(y,yh),
                'mae': mean_absolute_error(y,yh),
                'rmse': math.sqrt(mean_squared_error(y,yh))}
    m_xgb, m_rf, m_en = calc(y_test,y_xgb), calc(y_test,y_rf), calc(y_test,y_en)

    logger.info(f"üìä Results (forecast): XGB={m_xgb} RF={m_rf} ENS={m_en}")
    return {'xgb': best_xgb, 'rf_pipeline': best_rf_pipeline,
            'scaler': best_xgb.named_steps['scaler']}, \
           {'xgb': m_xgb, 'rf': m_rf, 'ensemble': m_en}


def export_corr_matrix(df, out_dir="/app/models/eda"):
    """
    T√≠nh correlation matrix v√† xu·∫•t CSV + heatmap PNG.
    """
    import os, numpy as np, pandas as pd, matplotlib.pyplot as plt
    os.makedirs(out_dir, exist_ok=True)

    _, _, mat = compute_correlations(df)
    cols = ["wqi","do","ph","temperature"]
    cm = pd.DataFrame(mat, index=cols, columns=cols)
    cm.to_csv(os.path.join(out_dir, "corr_matrix.csv"))

    # Heatmap ƒë∆°n gi·∫£n
    plt.figure()
    plt.imshow(cm, interpolation="nearest")
    plt.xticks(range(len(cols)), cols, rotation=45, ha="right")
    plt.yticks(range(len(cols)), cols)
    plt.colorbar()
    plt.title("Correlation Matrix (Pearson)")
    plt.tight_layout()
    plt.savefig(os.path.join(out_dir, "corr_matrix.png"))


def save_comprehensive_models(models, metrics, out_dir="/app/models"):
    """L∆∞u XGB + Spark-RF + scaler + metrics v√†o files (MLflow s·∫Ω ƒë∆∞·ª£c l∆∞u b·ªüi DAG)."""
    os.makedirs(out_dir, exist_ok=True)
    import pickle, json
    
    # 1) L∆∞u XGBoost model
    with open(f"{out_dir}/xgb.pkl","wb") as f:
        pickle.dump(models['xgb'], f)
    
    # 2) L∆∞u scaler
    with open(f"{out_dir}/scaler.pkl","wb") as f:
        pickle.dump(models['scaler'], f)
    
    # 3) L∆∞u Spark-RF pipeline
    rf_pipe = models['rf_pipeline']
    rf_pipe.write().overwrite().save(f"{out_dir}/rf_pipeline")
    
    # 4) L∆∞u metrics JSON
    with open(f"{out_dir}/metrics.json","w") as f:
        json.dump(metrics, f, indent=2)
    
    # 5) T√¨m best model
    best_model = max(metrics.items(), key=lambda x: x[1]['r2'])
    logger.info(f"üèÜ Best model: {best_model[0]} (R¬≤: {best_model[1]['r2']:.4f})")
    
    logger.info("‚úÖ ƒê√£ l∆∞u XGB, Spark-RF pipeline, scaler v√† metrics v√†o files.")
    logger.info("üìù MLflow s·∫Ω ƒë∆∞·ª£c l∆∞u b·ªüi DAG sau khi training ho√†n t·∫•t.")


def main():
    spark = create_spark_session()
    try:
        df_raw = load_data_from_postgres(spark)

        # Clean cho ML (KH√îNG back-fill ƒë·ªÉ tr√°nh leakage)
        df_clean = clean_time_series_monthly(df_raw, for_ml=True)

        # (TU·ª≤ CH·ªåN) EDA Analysis - ch·∫°y khi c·∫ßn b√°o c√°o chi ti·∫øt
        # df_clean_eda = clean_time_series_monthly(df_raw, for_ml=False)  # c√≥ back-fill cho EDA
        # corr_global, corr_by_station, corr_mat = compute_correlations(df_clean_eda)
        # wqi_by_month, seasonal_idx, wqi_by_station_month = analyze_seasonality(df_clean_eda)
        # export_corr_matrix(df_clean_eda)

        # Feature engineering (ƒë√£ fix windows qu√° kh·ª©)
        df_feat = comprehensive_feature_engineering(df_clean)
        df_feat = drop_cold_start_rows(df_feat, required_lags=("wqi_lag_12","ph_lag_12","temp_lag_12","do_lag_12"))

        for H in [1, 3, 12]:
            logger.info(f"üöÄ Forecast horizon H={H} months")
            dfH = add_label_for_horizon(df_feat, H)

            # split 12 th√°ng cu·ªëi l√†m test
            train, test = split_train_test_monthly(dfH, test_months=12)

            # g√°n fold th·ªùi gian cho Spark CV
            train_f = add_time_folds(train, k=3)

            models, metrics = train_with_optimized_tuning(train_f, test, spark, label_col="label")
            save_comprehensive_models(models, metrics, out_dir=f"/app/models/H{H}")
            
            logger.info(f"‚úÖ Completed training for H={H} months")
        
        logger.info("üéâ All forecast models training completed!")
        logger.info("üìÅ Trained models saved in /app/models/H1, H3, H12")
        
    finally:
        spark.stop()


if __name__ == "__main__":
    main()
